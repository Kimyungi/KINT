{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Humor Crawler : natepan (10대이야기, 20대이야기, 톡커들의 선택 명예의 전당 (일별)), todayhumor (베오베, 베스트게시물), ilbe (일베-일간베스트), dcinside (야구갤러리), ppomppu (자유게시판)\n",
    "\n",
    "- 오유 베오베 : 1\n",
    "- 오유 베게 : 2\n",
    "- 일베-일간베스트 : 3\n",
    "- dcinside (야구갤러리) : 4\n",
    "- 뽐뿌 자유게시판 : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.compat import urlparse, urljoin\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import sqlite3\n",
    "from requests import Session\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    try:\n",
    "        session = Session()\n",
    "        resp = session.request(method, url,\n",
    "                               params = params if method.upper() == 'GET' else '',\n",
    "                               data = params if method.upper() == 'POST' else '',\n",
    "                               headers = headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(60) # Server Error이기 때문에 delay를 두고 실행하기 위해서 사용한다.\n",
    "            # 보통은 5분에 한 번꼴로 random하게 되도록 설정한다.\n",
    "            download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS head;\n",
    "#     CREATE TABLE head(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         head TEXT NOT NULL,\n",
    "#         wdate TEXT NOT NULL,\n",
    "#         cdate TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL,\n",
    "#         page INTEGER NOT NULL\n",
    "#     );\n",
    "    \n",
    "#     DROP TABLE IF EXISTS history;\n",
    "#     CREATE TABLE history(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         seen TEXT NOT NULL\n",
    "#     )\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오늘의 유머 베스트오브베스트\n",
    "url = 'http://www.todayhumor.co.kr/board/list.php?table=bestofbest&page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('tbody > tr:nth-of-type(33) a')\n",
    "                  if _.has_attr('href') and re.match(r'(^l.+)',_['href']).group()]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "              \n",
    "        head = [_.text.strip() for _ in dom.select('td.subject > a')]\n",
    "        date = [_.text.strip() for _ in dom.select('tbody > tr > td.date')]\n",
    "        if len(head) == len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,1)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 오늘의 유머 베스트게시물\n",
    "url = 'http://www.todayhumor.co.kr/board/list.php?table=humorbest&page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('tbody > tr:nth-of-type(33) a')\n",
    "                  if _.has_attr('href') and re.match(r'(^l.+)',_['href']).group()]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "              \n",
    "        head = [_.text.strip() for _ in dom.select('td.subject > a')]\n",
    "        date = [_.text.strip() for _ in dom.select('tbody > tr > td.date')]\n",
    "        if len(head)==len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,2)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n"
     ]
    }
   ],
   "source": [
    "# 일베-일간베스트\n",
    "url = 'https://www.ilbe.com/list/ilbe'\n",
    "params = {\n",
    "    'page':'',\n",
    "    'listStyle':'list',\n",
    "}\n",
    "\n",
    "page = list()\n",
    "cur.execute('SELECT seen FROM history')\n",
    "page.append(int(cur.fetchall()[-1][0])+1)\n",
    "\n",
    "seen = list()\n",
    "\n",
    "\n",
    "count = page[0]\n",
    "\n",
    "while page:\n",
    "    try:\n",
    "        params['page'] = page.pop(0)\n",
    "        count += 1\n",
    "        seen.append(params['page'])\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [params['page']])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(url, params=params, method='GET')\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "        \n",
    "        if '{}'.format(count) in [_.text.strip() for _ in dom.select('.paginate > a') if _.has_attr('class') == False]:\n",
    "            page.append(count)\n",
    "        if count % 10 == 1:\n",
    "            page.append(count)\n",
    "            \n",
    "        head = [_.text.strip() for _ in dom.select('.board-body .subject') if _.has_attr('style') == False]\n",
    "        date = [_.text.strip() for _ in dom.select('li > .date')][7:]\n",
    "                \n",
    "        if len(head)==len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,3)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], params['page']])\n",
    "                conn.commit()\n",
    "            \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "HTTPSConnectionPool(host='gall.dcinside.com', port=443): Max retries exceeded with url: /board/lists/?id=baseball_new9&page=4811 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8e7c106160>: Failed to establish a new connection: [Errno 113] No route to host'))\n",
      "HTTPSConnectionPool(host='gall.dcinside.com', port=443): Max retries exceeded with url: /board/lists/?id=baseball_new9&page=4812 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8e7c106820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n",
      "HTTPSConnectionPool(host='gall.dcinside.com', port=443): Max retries exceeded with url: /board/lists/?id=baseball_new9&page=4813 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8e7c1067c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n",
      "HTTPSConnectionPool(host='gall.dcinside.com', port=443): Max retries exceeded with url: /board/lists/?id=baseball_new9&page=21392 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8e7c106250>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n",
      "HTTPSConnectionPool(host='gall.dcinside.com', port=443): Max retries exceeded with url: /board/lists/?id=baseball_new9&page=4814 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8e7c106400>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n"
     ]
    }
   ],
   "source": [
    "# 디시인사이드 - 국내야구갤러리\n",
    "url = 'https://gall.dcinside.com/board/lists/?id=baseball_new9&page=1'\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen) VALUES(?)\n",
    "        ''', [seed])\n",
    "        \n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')\n",
    "                  if _.has_attr('href') and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()]:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "        wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "        if len(head)==len(wdate):\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            for _ in range(0,len(head)):\n",
    "                cur.execute('''\n",
    "                    INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,4)\n",
    "                    ''', [head[_], wdate[_], cdate, page])\n",
    "                conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뽐뿌 - 자유게시판\n",
    "url = 'http://www.ppomppu.co.kr/zboard/zboard.php?id=freeboard&page=1&divpage=1321'\n",
    "headers = {'user-agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0'}\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('#page_list a')\n",
    "              if _.has_attr('href') and re.match(r'(?:https?:/)?/?\\w+(?:[./]\\w+)+', _['href'])]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('tr[class^=list] a > font') if _.find('img') == None or _.find('img').has_attr('style') == False]\n",
    "        date = [_['title'] for _ in dom.select('tr[class^=list] td.eng') if _.has_attr('title') and _['title'][:2] != '13']\n",
    "            \n",
    "        if len(head) == len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,5)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이트판 10대 이야기\n",
    "url = 'https://pann.nate.com/talk/c20038?page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.paginate > a.paging')\n",
    "              if _.has_attr('href') and re.match(r'(?:https?:/)?/?\\w+(?:[./]\\w+)+', _['href'])]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('tbody td.subject > a')]\n",
    "        date = [_.text.strip() for _ in dom.select('tbody > tr > td:nth-of-type(4)')]\n",
    "            \n",
    "        if len(head) == len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,6)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이트판 20대 이야기\n",
    "url = 'https://pann.nate.com/talk/c20002?page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.paginate > a.paging')\n",
    "              if _.has_attr('href') and re.match(r'(?:https?:/)?/?\\w+(?:[./]\\w+)+', _['href'])]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('tbody td.subject > a')]\n",
    "        date = [_.text.strip() for _ in dom.select('tbody > tr > td:nth-of-type(4)')]\n",
    "            \n",
    "        if len(head) == len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,7)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이트판 톡커들의 선택\n",
    "url = 'https://pann.nate.com/talk/ranking/d?stdt=20200730&page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        if re.search(r'page=(\\d+)',urlparse(seed)[4]).group(1) == '1':\n",
    "            # page 넘기기\n",
    "            stdt = re.search(r\",'(\\d+)',(\\d+)\",str(dom.select_one('button.last'))).group(1)\n",
    "            page = re.search(r\",'(\\d+)',(\\d+)\",str(dom.select_one('button.last'))).group(2)\n",
    "            newUrls = urljoin(url, 'd?stdt={}&page={}'.format(stdt,page))\n",
    "\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "\n",
    "            # 이전 날짜로 가기\n",
    "            stdt = re.search(r\",'(\\d+)',(\\d+)\",str(dom.select_one('button.prev'))).group(1)\n",
    "            page = re.search(r\",'(\\d+)',(\\d+)\",str(dom.select_one('button.prev'))).group(2)\n",
    "            newUrls = urljoin(url, 'd?stdt={}&page={}'.format(stdt,page))\n",
    "\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('ul.post_wrap > li dt > a')]\n",
    "        date = dom.select_one('span.tdate').text.strip()\n",
    "            \n",
    "        if head:\n",
    "            for _ in range(0,len(head)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,8)',\n",
    "                            [head[_], date, str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
