{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Humor Crawler : natepan (10대이야기, 20대이야기, 톡커들의 선택 명예의 전당 (일별)), todayhumor (베오베, 베스트게시물), ilbe (일베-일간베스트), dcinside (야구갤러리), ppomppu (자유게시판)\n",
    "\n",
    "- 오유 베오베 : 1\n",
    "- 오유 베게 : 2\n",
    "- 일베-일간베스트 : 3\n",
    "- dcinside (야구갤러리) : 4\n",
    "- 뽐뿌 자유게시판 : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.compat import urlparse, urljoin\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import sqlite3\n",
    "from requests import Session\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    try:\n",
    "        session = Session()\n",
    "        resp = session.request(method, url,\n",
    "                               params = params if method.upper() == 'GET' else '',\n",
    "                               data = params if method.upper() == 'POST' else '',\n",
    "                               headers = headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(60) # Server Error이기 때문에 delay를 두고 실행하기 위해서 사용한다.\n",
    "            # 보통은 5분에 한 번꼴로 random하게 되도록 설정한다.\n",
    "            download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS head;\n",
    "#     CREATE TABLE head(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         head TEXT NOT NULL,\n",
    "#         wdate TEXT NOT NULL,\n",
    "#         cdate TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL,\n",
    "#         page INTEGER NOT NULL\n",
    "#     );\n",
    "    \n",
    "#     DROP TABLE IF EXISTS history;\n",
    "#     CREATE TABLE history(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         seen TEXT NOT NULL\n",
    "#     )\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오늘의 유머 베스트오브베스트\n",
    "url = 'http://www.todayhumor.co.kr/board/list.php?table=bestofbest&page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('tbody > tr:nth-of-type(33) a')\n",
    "                  if _.has_attr('href') and re.match(r'(^l.+)',_['href']).group()]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "                \n",
    "        if dom.select_one('td.subject > a'):\n",
    "            head = [_.text.strip() for _ in dom.select('td.subject > a')]\n",
    "            date = [_.text.strip() for _ in dom.select('tbody > tr > td.date')]\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,1)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 오늘의 유머 베스트게시물\n",
    "url = 'http://www.todayhumor.co.kr/board/list.php?table=humorbest&page=1'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('tbody > tr:nth-of-type(33) a')\n",
    "                  if _.has_attr('href') and re.match(r'(^l.+)',_['href']).group()]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "                \n",
    "        if dom.select_one('td.subject > a'):\n",
    "            head = [_.text.strip() for _ in dom.select('td.subject > a')]\n",
    "            date = [_.text.strip() for _ in dom.select('tbody > tr > td.date')]\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,2)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일베-일간베스트\n",
    "url = 'https://www.ilbe.com/list/ilbe'\n",
    "params = {\n",
    "    'page':'',\n",
    "    'listStyle':'list',\n",
    "}\n",
    "\n",
    "page = list()\n",
    "cur.execute('SELECT seen FROM history')\n",
    "page.append(int(cur.fetchall()[-1][0])+1)\n",
    "\n",
    "seen = list()\n",
    "\n",
    "\n",
    "count = page[0]\n",
    "\n",
    "while page:\n",
    "    try:\n",
    "        params['page'] = page.pop(0)\n",
    "        count += 1\n",
    "        seen.append(params['page'])\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [params['page']])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(url, params=params, method='GET')\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "        \n",
    "        if '{}'.format(count) in [_.text.strip() for _ in dom.select('.paginate > a') if _.has_attr('class') == False]:\n",
    "            page.append(count)\n",
    "        if count % 10 == 1:\n",
    "            page.append(count)\n",
    "            \n",
    "        head = [_.text.strip() for _ in dom.select('.board-body .subject') if _.has_attr('style') == False]\n",
    "        date = [_.text.strip() for _ in dom.select('li > .date')][7:]\n",
    "                \n",
    "        if len(head)==len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,3)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], params['page']])\n",
    "                conn.commit()\n",
    "            \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디시인사이드 - 국내야구갤러리\n",
    "url = 'https://gall.dcinside.com/board/lists/?id=baseball_new9&page=1'\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen) VALUES(?)\n",
    "        ''', [seed])\n",
    "        \n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')\n",
    "                  if _.has_attr('href') and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()]:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "        \n",
    "        if dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "            wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            for _ in range(0,len(head)):\n",
    "                cur.execute('''\n",
    "                    INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,4)\n",
    "                    ''', [head[_], wdate[_], cdate, page])\n",
    "                conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뽐뿌 - 자유게시판\n",
    "url = 'http://www.ppomppu.co.kr/zboard/zboard.php?id=freeboard&page=1&divpage=1321'\n",
    "headers = {'user-agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0'}\n",
    "urls = list()\n",
    "seen = list()\n",
    "\n",
    "urls.append(url)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "        seen.append(seed)\n",
    "        cur.execute('INSERT INTO history(seen) VALUES(?)', [seed])\n",
    "        conn.commit()\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('#page_list a')\n",
    "              if _.has_attr('href') and re.match(r'(?:https?:/)?/?\\w+(?:[./]\\w+)+', _['href'])]:\n",
    "            newUrls = urljoin(seed,_)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "        \n",
    "        head = [_.text.strip() for _ in dom.select('tr[class^=list] a > font') if _.find('img') == None or _.find('img').has_attr('style') == False]\n",
    "        date = [_['title'] for _ in dom.select('tr[class^=list] td.eng') if _.has_attr('title') and _['title'][:2] != '13']\n",
    "            \n",
    "        if len(head) == len(date):\n",
    "            for _ in range(0,len(date)):\n",
    "                cur.execute('INSERT INTO head(head, wdate, cdate,page, ref) VALUES(?,?,?,?,5)',\n",
    "                            [head[_], date[_], str(datetime.datetime.now()).split('.')[0], re.search(r'=(\\d+)',urlparse(seed)[4]).group(1)])\n",
    "                conn.commit()\n",
    "        \n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
