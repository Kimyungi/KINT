{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from requests import request\n",
    "from requests.compat import urljoin, urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from requests.compat import urlparse, urljoin\n",
    "import sqlite3\n",
    "from requests import Session\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canfetch(url, agent='*', path='/'):\n",
    "    robot = RobotFileParser(urljoin(url, '/robots.txt'))\n",
    "    robot.read()\n",
    "    return robot.can_fetch(agent, urlparse(url)[2])\n",
    "    \n",
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    try:\n",
    "        session = Session()\n",
    "        resp = session.request(method, url,\n",
    "                               params = params if method.upper() == 'GET' else '',\n",
    "                               data = params if method.upper() == 'POST' else '',\n",
    "                               headers = headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(60) # Server Error이기 때문에 delay를 두고 실행하기 위해서 사용한다.\n",
    "            # 보통은 5분에 한 번꼴로 random하게 되도록 설정한다.\n",
    "            download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('mom.db')\n",
    "# cur = conn.cursor()\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS head;\n",
    "#     CREATE TABLE head(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         head TEXT NOT NULL,\n",
    "#         cdate TEXT NOT NULL,\n",
    "#         wdate TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL,\n",
    "#         page INTEGER NOT NULL\n",
    "#     );\n",
    "# ''')\n",
    "\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS history;\n",
    "#     CREATE TABLE history(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         seen TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL\n",
    "#     );\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 시작할 때 \n",
    "# 맘스홀릭베이비 ref = 104\n",
    "url = 'https://cafe.naver.com/ArticleList.nhn?search.clubid=10094499&search.menuid=179&search.boardtype=L&search.totalCount=151&search.page=1'\n",
    "seen = list()\n",
    "\n",
    "urls = list()\n",
    "urls.append(url)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 104)\n",
    "        ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.prev-next > a')\n",
    "                  if _.has_attr('href')and re.match(r'/ArticleList',_['href'])]:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "\n",
    "        if dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_article > div.board-list > div > a.article') != None:\n",
    "            head = [\"\".join(re.findall(r'[^\\n]+',_.text.strip())) for _ in dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_article > div.board-list > div > a.article')]\n",
    "            wdate = [_.text.strip() for _ in dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,104)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맘카페\n",
    "# 이어서 시작할 때\n",
    "conn = sqlite3.connect('mom.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT seen FROM history')\n",
    "seen =cur.fetchall()\n",
    "seen_ = list()\n",
    "for _ in seen:\n",
    "    seen_.append(''.join(_))\n",
    "url = seen[-1]\n",
    "url = ''.join(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맘카페\n",
    "# 이어서 시작할 때\n",
    "urls = list()\n",
    "urls.append(url)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen_.append(seed)\n",
    "        cur.execute('''\n",
    "                    INSERT INTO history(seen, ref) VALUES(?, 104)\n",
    "                ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.prev-next > a')\n",
    "                  if _.has_attr('href')and re.match(r'/ArticleList',_['href'])]:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen_:\n",
    "                urls.append(newUrls)\n",
    "                            \n",
    "\n",
    "        if dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_article > div.board-list > div > a.article') != None:\n",
    "            head = [\"\".join(re.findall(r'[^\\n]+',_.text.strip())) for _ in dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_article > div.board-list > div > a.article')]\n",
    "            wdate = [_.text.strip() for _ in dom.select('#main-area > div:nth-child(6) > table > tbody > tr > td.td_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,104)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
