{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from requests import request\n",
    "from requests.compat import urljoin, urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from requests.compat import urlparse, urljoin\n",
    "import sqlite3\n",
    "from requests import Session\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canfetch(url, agent='*', path='/'):\n",
    "    robot = RobotFileParser(urljoin(url, '/robots.txt'))\n",
    "    robot.read()\n",
    "    return robot.can_fetch(agent, urlparse(url)[2])\n",
    "    \n",
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    try:\n",
    "        session = Session()\n",
    "        resp = session.request(method, url,\n",
    "                               params = params if method.upper() == 'GET' else '',\n",
    "                               data = params if method.upper() == 'POST' else '',\n",
    "                               headers = headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(60) # Server Error이기 때문에 delay를 두고 실행하기 위해서 사용한다.\n",
    "            # 보통은 5분에 한 번꼴로 random하게 되도록 설정한다.\n",
    "            download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inbang = 'https://gall.dcinside.com/board/lists/?id=ib_new1&page=1'\n",
    "url_m = 'https://gall.dcinside.com/board/lists/?id=m_entertainer1'\n",
    "url_w = 'https://gall.dcinside.com/board/lists?id=w_entertainer'\n",
    "url_instiz = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('Entertainment.db')\n",
    "# cur = conn.cursor()\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS head;\n",
    "#     CREATE TABLE head(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         head TEXT NOT NULL,\n",
    "#         cdate TEXT NOT NULL,\n",
    "#         wdate TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL,\n",
    "#         page INTEGER NOT NULL\n",
    "#     );\n",
    "# ''')\n",
    "\n",
    "# cur.executescript('''\n",
    "#     DROP TABLE IF EXISTS history;\n",
    "#     CREATE TABLE history(\n",
    "#         pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         seen TEXT NOT NULL,\n",
    "#         ref INTEGER NOT NULL\n",
    "#     );\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인방갤\n",
    "# 처음 시작할 때 \n",
    "url_inbang = 'https://gall.dcinside.com/board/lists/?id=ib_new1&page=1'\n",
    "seen = list()\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "urls = list()\n",
    "\n",
    "\n",
    "urls.append(url_inbang)\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 100)\n",
    "        ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')[:-1]  #끝 미포함\n",
    "                  if _.has_attr('href')and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()\n",
    "                 and _.text.strip() != '처음' and  _.text.strip() != '이전']:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "\n",
    "        if dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "            wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,100)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Entertainment.db')\n",
    "cur = conn.cursor()\n",
    "# 인방갤\n",
    "# 이어서 시작할 때\n",
    "cur.execute('SELECT seen FROM history')\n",
    "seen = cur.fetchall()\n",
    "seen_ = list()\n",
    "for _ in seen:\n",
    "    seen_.append(''.join(_))\n",
    "url_inbang = seen[-1]\n",
    "url_inbang = ''.join(url_inbang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "urls = list()\n",
    "\n",
    "\n",
    "urls.append(url_inbang)\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen_.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 100)\n",
    "        ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')[:-1]  #끝 미포함\n",
    "                  if _.has_attr('href')and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()\n",
    "                 and _.text.strip() != '처음' and  _.text.strip() != '이전']:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen_:\n",
    "                 urls.append(newUrls)\n",
    "        \n",
    "        \n",
    "        if dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "            wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,100)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#남자 연예인 갤러리\n",
    "url_man = 'https://gall.dcinside.com/board/lists/?id=m_entertainer1&page=1'\n",
    "seen = list()\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "urls = list()\n",
    "\n",
    "\n",
    "urls.append(url_man)\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 101)\n",
    "        ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')[:-1]  #끝 미포함\n",
    "                  if _.has_attr('href')and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()\n",
    "                 and _.text.strip() != '처음' and  _.text.strip() != '이전']:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "\n",
    "        if dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "            wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,101)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여자 연예인 갤러리\n",
    "url_woman = 'https://gall.dcinside.com/board/lists?id=w_entertainer&page=1'\n",
    "seen = list()\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "urls = list()\n",
    "\n",
    "\n",
    "urls.append(url_woman)\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "while urls:\n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(seed, headers=headers)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 102)\n",
    "        ''', [seed])\n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('div.bottom_paging_box > a')[:-1]  #끝 미포함\n",
    "                  if _.has_attr('href')and re.match(r'(?:https?:/)?/\\w+(?:[./]\\w+)+',_['href']).group()\n",
    "                 and _.text.strip() != '처음' and  _.text.strip() != '이전']:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                 urls.append(newUrls)\n",
    "\n",
    "        if dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('tr.ub-content.us-post > td:nth-of-type(2) > a:nth-of-type(1)') ]\n",
    "            wdate = [_['title'] for _ in dom.select('tr.ub-content.us-post > td.gall_date')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('page=(\\d+)', urlparse(seed).query).group(1)\n",
    "            if len(head)==len(wdate):\n",
    "                for _ in range(0,len(head)):\n",
    "                    cur.execute('''\n",
    "                        INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,102)\n",
    "                        ''', [head[_], wdate[_], cdate, page])\n",
    "                    conn.commit()\n",
    "\n",
    "        if count%100 == 0:\n",
    "                print(count)\n",
    "     \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Entertainment.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스티즈\n",
    "\n",
    "url = 'https://www.instiz.net/name_enter?page=1&category=2'\n",
    "\n",
    "urls = list()\n",
    "seen = list()\n",
    "urls.append(url)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while urls:\n",
    "    \n",
    "    try:\n",
    "        count += 1\n",
    "        seed = urls.pop(0)\n",
    "\n",
    "        resp = download(url)\n",
    "        dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "        seen.append(seed)\n",
    "        cur.execute('''\n",
    "            INSERT INTO history(seen, ref) VALUES(?, 103)\n",
    "        ''', [seed])\n",
    "        \n",
    "\n",
    "        for _ in [_['href'] for _ in dom.select('#indextable a')\n",
    "                  if _.has_attr('href') and re.match(r'(../)?\\w+', _['href']).group()]:\n",
    "            newUrls = urljoin(seed, _)\n",
    "            if newUrls not in urls and newUrls not in seen:\n",
    "                urls.append(newUrls)\n",
    "                \n",
    "        if dom.select('#subject > a') != None:\n",
    "            head = [_.text.strip() for _ in dom.select('#subject > a')]\n",
    "            wdate = [_.text.strip() for _ in dom.select('td.listno.regdate')]\n",
    "            cdate = str(datetime.datetime.now()).split('.')[0]\n",
    "            page = re.search('p?a?g?e?=?(\\d+)', urlparse(seed).query).group(1)\n",
    "\n",
    "            for _ in range(0,len(head)):\n",
    "                cur.execute('''\n",
    "                    INSERT INTO head(head, cdate, wdate, page, ref) VALUES(?,?,?,?,103)\n",
    "                    ''', [head[_], wdate[_], cdate, page])\n",
    "                conn.commit()\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    except Exception as e:\n",
    "            print('Error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
