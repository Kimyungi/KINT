{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from string import punctuation\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "# Stopwords 처리\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation))) # punctuation 제거\n",
    "pattern2 = re.compile(r'[^가-힣 ]') # 특수문자, 모음, 숫자, 영어 제거\n",
    "pattern3 = re.compile(r'\\s{2,}') # white space 1개로 바꾸기.\n",
    "\n",
    "class Extracter:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "        self.word_extractor = WordExtractor(min_frequency=math.floor(len(self.df)*0.0001))\n",
    "        \n",
    "    def cleaning(self):\n",
    "        self.df['head'] = self.df['head'].map(lambda x:pattern3.sub(' ',\n",
    "                                                        pattern2.sub('',\n",
    "                                                         pattern1.sub('', x))))\n",
    "        return self.df\n",
    "\n",
    "    def extract_nouns(self):\n",
    "        tempary = np.linspace(0,1,11)\n",
    "        nouns = [self.noun_extractor.train_extract(_['head'], min_noun_frequency=math.floor(len(self.df)*0.0001)) for _ in [self.df.iloc[math.ceil(len(self.df)*tempary[_]):math.ceil(len(self.df)*tempary[_+1])] for _ in range(len(tempary)-1)]]\n",
    "        words = {k:v for i in range(len(nouns)) for k,v in nouns[i].items() if len(k) > 1}\n",
    "        return words\n",
    "\n",
    "    def search_dict(self,nouns):\n",
    "        # 사전 검색 결과 없는 단어 추출\n",
    "        conn = sqlite3.connect('kr_korean.db')\n",
    "        cur = conn.cursor()\n",
    "        data = pd.read_sql('SELECT word FROM kr', conn)\n",
    "        data = ' '.join(data['word'])\n",
    "        return pd.DataFrame([_ for _ in nouns if _[0] not in data])\n",
    "    \n",
    "    # 의미 추출을 위한 training data set 생성\n",
    "    def extract_sent(self, words):\n",
    "        sent = defaultdict(lambda:0)\n",
    "        for w in new_words[0]:\n",
    "            temp = [s for s in df['head'] if w in s]\n",
    "            sent[w] = '  '.join(temp)\n",
    "        return sent\n",
    "            \n",
    "    def extract_statistic_value(self, corpus):\n",
    "#         sent = [s for w in new_words[0] for s in self.df['head'] if w in s]\n",
    "#         sent = pd.DataFrame(sent)\n",
    "#         sent.drop_duplicates(keep='first', inplace=True)\n",
    "        self.word_extractor.train(corpus)\n",
    "        return self.word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 141099 from 125252 sents. mem=0.485 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=532046, mem=0.680 Gb\n",
      "[Noun Extractor] batch prediction was completed for 37314 words\n",
      "[Noun Extractor] checked compounds. discovered 21766 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 419 -> 418\n",
      "[Noun Extractor] postprocessing ignore_features : 418 -> 392\n",
      "[Noun Extractor] postprocessing ignore_NJ : 392 -> 389\n",
      "[Noun Extractor] 389 nouns (21766 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.747 Gb                    \n",
      "[Noun Extractor] 30.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 150363 from 125252 sents. mem=0.802 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=530478, mem=0.886 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39760 words\n",
      "[Noun Extractor] checked compounds. discovered 26243 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 408 -> 406\n",
      "[Noun Extractor] postprocessing ignore_features : 406 -> 384\n",
      "[Noun Extractor] postprocessing ignore_NJ : 384 -> 380\n",
      "[Noun Extractor] 380 nouns (26243 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.886 Gb                    \n",
      "[Noun Extractor] 30.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 151840 from 125253 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=523703, mem=0.925 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41063 words\n",
      "[Noun Extractor] checked compounds. discovered 26083 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 409 -> 405\n",
      "[Noun Extractor] postprocessing ignore_features : 405 -> 382\n",
      "[Noun Extractor] postprocessing ignore_NJ : 382 -> 381\n",
      "[Noun Extractor] 381 nouns (26083 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.924 Gb                    \n",
      "[Noun Extractor] 32.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 150899 from 125251 sents. mem=0.933 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=507895, mem=0.964 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41424 words\n",
      "[Noun Extractor] checked compounds. discovered 28540 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 392 -> 390\n",
      "[Noun Extractor] postprocessing ignore_features : 390 -> 363\n",
      "[Noun Extractor] postprocessing ignore_NJ : 363 -> 361\n",
      "[Noun Extractor] 361 nouns (28540 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.945 Gb                    \n",
      "[Noun Extractor] 31.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 158170 from 125252 sents. mem=0.953 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=512452, mem=0.996 Gb\n",
      "[Noun Extractor] batch prediction was completed for 44871 words\n",
      "[Noun Extractor] checked compounds. discovered 28585 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 377 -> 375\n",
      "[Noun Extractor] postprocessing ignore_features : 375 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 349\n",
      "[Noun Extractor] 349 nouns (28585 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.973 Gb                    \n",
      "[Noun Extractor] 30.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 163557 from 125253 sents. mem=0.990 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=517283, mem=1.022 Gb\n",
      "[Noun Extractor] batch prediction was completed for 46858 words\n",
      "[Noun Extractor] checked compounds. discovered 35739 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 372 -> 370\n",
      "[Noun Extractor] postprocessing ignore_features : 370 -> 342\n",
      "[Noun Extractor] postprocessing ignore_NJ : 342 -> 340\n",
      "[Noun Extractor] 340 nouns (35739 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.997 Gb                    \n",
      "[Noun Extractor] 31.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 125298 from 125252 sents. mem=1.033 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=344537, mem=0.967 Gb\n",
      "[Noun Extractor] batch prediction was completed for 31475 words\n",
      "[Noun Extractor] checked compounds. discovered 14248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 198\n",
      "[Noun Extractor] postprocessing ignore_features : 198 -> 184\n",
      "[Noun Extractor] postprocessing ignore_NJ : 184 -> 184\n",
      "[Noun Extractor] 184 nouns (14248 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.950 Gb                    \n",
      "[Noun Extractor] 18.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 110018 from 125251 sents. mem=0.950 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=300224, mem=0.933 Gb\n",
      "[Noun Extractor] batch prediction was completed for 25566 words\n",
      "[Noun Extractor] checked compounds. discovered 9529 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (9529 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.932 Gb                    \n",
      "[Noun Extractor] 17.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 99699 from 125252 sents. mem=0.932 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=273114, mem=0.918 Gb\n",
      "[Noun Extractor] batch prediction was completed for 22510 words\n",
      "[Noun Extractor] checked compounds. discovered 7573 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 129 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 123\n",
      "[Noun Extractor] 123 nouns (7573 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.902 Gb                    \n",
      "[Noun Extractor] 14.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 112195 from 125252 sents. mem=0.902 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=313183, mem=0.907 Gb\n",
      "[Noun Extractor] batch prediction was completed for 26687 words\n",
      "[Noun Extractor] checked compounds. discovered 9637 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 156 -> 156\n",
      "[Noun Extractor] postprocessing ignore_features : 156 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 147\n",
      "[Noun Extractor] 147 nouns (9637 compounds) with min frequency=125\n",
      "[Noun Extractor] flushing was done. mem=0.862 Gb                    \n",
      "[Noun Extractor] 16.50 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head',conn)\n",
    "df.drop_duplicates(keep='first')\n",
    "ext = Extracter(df)\n",
    "df = ext.cleaning()\n",
    "\n",
    "new_words = ext.search_dict(sorted(ext.extract_nouns().items(),key=lambda _:_[1], reverse=True))\n",
    "sent = ext.extract_sent(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training was done. used memory 0.872 Gb\n",
      "\r",
      "all cohesion probabilities was computed. # words = 0\n",
      "\r",
      "all branching entropies was computed # words = 0\n",
      "\r",
      "all accessor variety was computed # words = 0\n"
     ]
    }
   ],
   "source": [
    "static = defaultdict(lambda:0)\n",
    "for k,v in sent.items():\n",
    "    temp = ext.extract_statistic_value(v)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
