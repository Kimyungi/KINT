{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from string import punctuation\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "# Stopwords 처리\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation))) # punctuation 제거\n",
    "pattern2 = re.compile(r'[^가-힣 ]') # 특수문자, 모음, 숫자, 영어 제거\n",
    "pattern3 = re.compile(r'\\s{2,}') # white space 1개로 바꾸기.\n",
    "\n",
    "class Extracter:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "        self.word_extractor = WordExtractor(min_frequency=math.floor(len(self.df)*0.0001))\n",
    "        \n",
    "    def cleaning(self):\n",
    "        self.df['head'] = self.df['head'].map(lambda x:pattern3.sub(' ',\n",
    "                                                        pattern2.sub('',\n",
    "                                                         pattern1.sub('', x))))\n",
    "        return self.df\n",
    "\n",
    "    def extract_nouns(self):\n",
    "        tempary = np.linspace(0,1,11)\n",
    "        nouns = [self.noun_extractor.train_extract(_['head'], min_noun_frequency=math.floor(len(self.df)*0.0001)) for _ in [self.df.iloc[math.ceil(len(self.df)*tempary[_]):math.ceil(len(self.df)*tempary[_+1])] for _ in range(len(tempary)-1)]]\n",
    "        words = {k:v for i in range(len(nouns)) for k,v in nouns[i].items() if len(k) > 1}\n",
    "        return words\n",
    "\n",
    "    def search_dict(self,nouns):\n",
    "        # 사전 검색 결과 없는 단어 추출\n",
    "        conn = sqlite3.connect('kr_korean.db')\n",
    "        cur = conn.cursor()\n",
    "        data = pd.read_sql('SELECT word FROM kr', conn)\n",
    "        data = ' '.join(data['word'])\n",
    "        return pd.DataFrame([_ for _ in nouns if _[0] not in data])\n",
    "    \n",
    "    # 의미 추출을 위한 training data set 생성\n",
    "    def extract_sent(self, words):\n",
    "        sent = defaultdict(lambda:0)\n",
    "        for w in new_words[0]:\n",
    "            temp = [s for s in df['head'] if w in s]\n",
    "            sent[w] = '  '.join(temp)\n",
    "        return sent\n",
    "            \n",
    "    def extract_statistic_value(self, sent):\n",
    "        statistic = defaultdict(lambda:0)\n",
    "        for k,v in sent.items():\n",
    "            self.word_extractor.train([v])\n",
    "            try:\n",
    "                statistic[k] = self.word_extractor.extract()[k]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return statistic\n",
    "    \n",
    "    def extract_r_rat(self, sent, statistic):\n",
    "        conn = sqlite3.connect('kr_korean.db')\n",
    "        cur = conn.cursor()\n",
    "        post_pos = pd.read_sql('SELECT word FROM kr WHERE part=\"조사\"', conn)\n",
    "        post_pos['word'] = post_pos['word'].map(lambda x: pattern3.sub(' ',\n",
    "                                                            pattern2.sub('',\n",
    "                                                             pattern1.sub('', x))))\n",
    "        post_pos.drop_duplicates(keep='first', inplace=True)\n",
    "        post_pos = ''.join(post_pos['word'])\n",
    "        r_rat = defaultdict(lambda:0)\n",
    "        for k in statistic.keys():\n",
    "            try:\n",
    "                self.noun_extractor.train_extract([sent[k]])\n",
    "                count = pprat = wsrat = 0\n",
    "                for _ in self.noun_extractor.lrgraph.get_r(k, topk=-1):\n",
    "                    if _[0] in post_pos:\n",
    "                        if _[0] != '':\n",
    "                            pprat += _[1]\n",
    "                        elif _[0] == '':\n",
    "                            wsrat = _[1]\n",
    "                for _ in self.noun_extractor.lrgraph.get_r(k, topk=-1):\n",
    "                    count += _[1]\n",
    "\n",
    "                r_rat[k] = {'rpprat':pprat/count, 'rwsrat':wsrat/count}\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return r_rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 153629 from 127907 sents. mem=0.836 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556166, mem=0.842 Gb\n",
      "[Noun Extractor] batch prediction was completed for 40185 words\n",
      "[Noun Extractor] checked compounds. discovered 27173 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 426 -> 423\n",
      "[Noun Extractor] postprocessing ignore_features : 423 -> 397\n",
      "[Noun Extractor] postprocessing ignore_NJ : 397 -> 395\n",
      "[Noun Extractor] 395 nouns (27173 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=0.879 Gb                    \n",
      "[Noun Extractor] 31.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 156261 from 127907 sents. mem=0.933 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=547864, mem=1.025 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41507 words\n",
      "[Noun Extractor] checked compounds. discovered 28598 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 429 -> 426\n",
      "[Noun Extractor] postprocessing ignore_features : 426 -> 405\n",
      "[Noun Extractor] postprocessing ignore_NJ : 405 -> 404\n",
      "[Noun Extractor] 404 nouns (28598 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.025 Gb                    \n",
      "[Noun Extractor] 32.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 152909 from 127906 sents. mem=1.025 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=522178, mem=1.053 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41573 words\n",
      "[Noun Extractor] checked compounds. discovered 26220 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 406 -> 401\n",
      "[Noun Extractor] postprocessing ignore_features : 401 -> 376\n",
      "[Noun Extractor] postprocessing ignore_NJ : 376 -> 375\n",
      "[Noun Extractor] 375 nouns (26220 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.053 Gb                    \n",
      "[Noun Extractor] 31.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157855 from 127907 sents. mem=1.053 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=521544, mem=1.075 Gb\n",
      "[Noun Extractor] batch prediction was completed for 43681 words\n",
      "[Noun Extractor] checked compounds. discovered 28034 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 381 -> 378\n",
      "[Noun Extractor] postprocessing ignore_features : 378 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (28034 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.067 Gb                    \n",
      "[Noun Extractor] 30.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166295 from 127906 sents. mem=1.067 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=529708, mem=1.114 Gb\n",
      "[Noun Extractor] batch prediction was completed for 47892 words\n",
      "[Noun Extractor] checked compounds. discovered 31704 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 394 -> 394\n",
      "[Noun Extractor] postprocessing ignore_features : 394 -> 366\n",
      "[Noun Extractor] postprocessing ignore_NJ : 366 -> 364\n",
      "[Noun Extractor] 364 nouns (31704 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.095 Gb                    \n",
      "[Noun Extractor] 31.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 162683 from 127907 sents. mem=1.095 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=513031, mem=1.111 Gb\n",
      "[Noun Extractor] batch prediction was completed for 45035 words\n",
      "[Noun Extractor] checked compounds. discovered 29556 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 359 -> 358\n",
      "[Noun Extractor] postprocessing ignore_features : 358 -> 333\n",
      "[Noun Extractor] postprocessing ignore_NJ : 333 -> 331\n",
      "[Noun Extractor] 331 nouns (29556 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.102 Gb                    \n",
      "[Noun Extractor] 28.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157585 from 127907 sents. mem=1.102 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=538505, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39854 words\n",
      "[Noun Extractor] checked compounds. discovered 23050 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 405 -> 405\n",
      "[Noun Extractor] postprocessing ignore_features : 405 -> 381\n",
      "[Noun Extractor] postprocessing ignore_NJ : 381 -> 379\n",
      "[Noun Extractor] 379 nouns (23050 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.088 Gb                    \n",
      "[Noun Extractor] 29.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 148501 from 127906 sents. mem=1.088 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=514069, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39879 words\n",
      "[Noun Extractor] checked compounds. discovered 24916 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 408 -> 407\n",
      "[Noun Extractor] postprocessing ignore_features : 407 -> 385\n",
      "[Noun Extractor] postprocessing ignore_NJ : 385 -> 383\n",
      "[Noun Extractor] 383 nouns (24916 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.085 Gb                    \n",
      "[Noun Extractor] 35.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 159979 from 127907 sents. mem=1.085 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=510274, mem=1.116 Gb\n",
      "[Noun Extractor] batch prediction was completed for 44551 words\n",
      "[Noun Extractor] checked compounds. discovered 28238 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 381 -> 380\n",
      "[Noun Extractor] postprocessing ignore_features : 380 -> 354\n",
      "[Noun Extractor] postprocessing ignore_NJ : 354 -> 351\n",
      "[Noun Extractor] 351 nouns (28238 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.113 Gb                    \n",
      "[Noun Extractor] 28.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166948 from 127906 sents. mem=1.113 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=590217, mem=1.121 Gb\n",
      "[Noun Extractor] batch prediction was completed for 42936 words\n",
      "[Noun Extractor] checked compounds. discovered 25619 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 473 -> 471\n",
      "[Noun Extractor] postprocessing ignore_features : 471 -> 449\n",
      "[Noun Extractor] postprocessing ignore_NJ : 449 -> 446\n",
      "[Noun Extractor] 446 nouns (25619 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.118 Gb                    \n",
      "[Noun Extractor] 34.37 % eojeols are covered\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 26\n",
      "all branching entropies was computed # words = 4361\n",
      "all accessor variety was computed # words = 4361\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 43\n",
      "all branching entropies was computed # words = 6287\n",
      "all accessor variety was computed # words = 6287\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 101\n",
      "all branching entropies was computed # words = 16842\n",
      "all accessor variety was computed # words = 16842\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 107\n",
      "all branching entropies was computed # words = 17665\n",
      "all accessor variety was computed # words = 17665\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 21929\n",
      "all accessor variety was computed # words = 21929\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 170\n",
      "all branching entropies was computed # words = 25059\n",
      "all accessor variety was computed # words = 25059\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 179\n",
      "all branching entropies was computed # words = 26184\n",
      "all accessor variety was computed # words = 26184\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 205\n",
      "all branching entropies was computed # words = 28439\n",
      "all accessor variety was computed # words = 28439\n",
      "training was done. used memory 1.118 Gb1.118 Gb\n",
      "all cohesion probabilities was computed. # words = 205\n",
      "all branching entropies was computed # words = 28968\n",
      "all accessor variety was computed # words = 28968\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 236\n",
      "all branching entropies was computed # words = 35403\n",
      "all accessor variety was computed # words = 35403\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 240\n",
      "all branching entropies was computed # words = 36815\n",
      "all accessor variety was computed # words = 36815\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 256\n",
      "all branching entropies was computed # words = 37947\n",
      "all accessor variety was computed # words = 37947\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 281\n",
      "all branching entropies was computed # words = 39668\n",
      "all accessor variety was computed # words = 39668\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 286\n",
      "all branching entropies was computed # words = 39842\n",
      "all accessor variety was computed # words = 39842\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 291\n",
      "all branching entropies was computed # words = 40059\n",
      "all accessor variety was computed # words = 40059\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 302\n",
      "all branching entropies was computed # words = 40512\n",
      "all accessor variety was computed # words = 40512\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 305\n",
      "all branching entropies was computed # words = 40641\n",
      "all accessor variety was computed # words = 40641\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 321\n",
      "all branching entropies was computed # words = 41089\n",
      "all accessor variety was computed # words = 41089\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 345\n",
      "all branching entropies was computed # words = 42257\n",
      "all accessor variety was computed # words = 42257\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 351\n",
      "all branching entropies was computed # words = 43220\n",
      "all accessor variety was computed # words = 43220\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 375\n",
      "all branching entropies was computed # words = 44673\n",
      "all accessor variety was computed # words = 44673\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 448\n",
      "all branching entropies was computed # words = 46783\n",
      "all accessor variety was computed # words = 46783\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 452\n",
      "all branching entropies was computed # words = 46984\n",
      "all accessor variety was computed # words = 46984\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 468\n",
      "all branching entropies was computed # words = 48800\n",
      "all accessor variety was computed # words = 48800\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 478\n",
      "all branching entropies was computed # words = 50144\n",
      "all accessor variety was computed # words = 50144\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 509\n",
      "all branching entropies was computed # words = 51664\n",
      "all accessor variety was computed # words = 51664\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 533\n",
      "all branching entropies was computed # words = 52223\n",
      "all accessor variety was computed # words = 52223\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 536\n",
      "all branching entropies was computed # words = 52279\n",
      "all accessor variety was computed # words = 52279\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 554\n",
      "all branching entropies was computed # words = 52907\n",
      "all accessor variety was computed # words = 52907\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 561\n",
      "all branching entropies was computed # words = 53032\n",
      "all accessor variety was computed # words = 53032\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 566\n",
      "all branching entropies was computed # words = 53096\n",
      "all accessor variety was computed # words = 53096\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 690\n",
      "all branching entropies was computed # words = 58854\n",
      "all accessor variety was computed # words = 58854\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 693\n",
      "all branching entropies was computed # words = 59024\n",
      "all accessor variety was computed # words = 59024\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 696\n",
      "all branching entropies was computed # words = 59097\n",
      "all accessor variety was computed # words = 59097\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 698\n",
      "all branching entropies was computed # words = 59349\n",
      "all accessor variety was computed # words = 59349\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 700\n",
      "all branching entropies was computed # words = 59452\n",
      "all accessor variety was computed # words = 59452\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 711\n",
      "all branching entropies was computed # words = 59686\n",
      "all accessor variety was computed # words = 59686\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 713\n",
      "all branching entropies was computed # words = 59746\n",
      "all accessor variety was computed # words = 59746\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 713\n",
      "all branching entropies was computed # words = 59851\n",
      "all accessor variety was computed # words = 59851\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 717\n",
      "all branching entropies was computed # words = 59975\n",
      "all accessor variety was computed # words = 59975\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 720\n",
      "all branching entropies was computed # words = 60057\n",
      "all accessor variety was computed # words = 60057\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 728\n",
      "all branching entropies was computed # words = 60707\n",
      "all accessor variety was computed # words = 60707\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 731\n",
      "all branching entropies was computed # words = 60935\n",
      "all accessor variety was computed # words = 60935\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 732\n",
      "all branching entropies was computed # words = 61603\n",
      "all accessor variety was computed # words = 61603\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 62052\n",
      "all accessor variety was computed # words = 62052\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 744\n",
      "all branching entropies was computed # words = 62144\n",
      "all accessor variety was computed # words = 62144\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 749\n",
      "all branching entropies was computed # words = 62195\n",
      "all accessor variety was computed # words = 62195\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 756\n",
      "all branching entropies was computed # words = 62272\n",
      "all accessor variety was computed # words = 62272\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 759\n",
      "all branching entropies was computed # words = 62331\n",
      "all accessor variety was computed # words = 62331\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 775\n",
      "all branching entropies was computed # words = 63141\n",
      "all accessor variety was computed # words = 63141\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 778\n",
      "all branching entropies was computed # words = 63173\n",
      "all accessor variety was computed # words = 63173\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 782\n",
      "all branching entropies was computed # words = 63390\n",
      "all accessor variety was computed # words = 63390\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 787\n",
      "all branching entropies was computed # words = 63484\n",
      "all accessor variety was computed # words = 63484\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 790\n",
      "all branching entropies was computed # words = 63609\n",
      "all accessor variety was computed # words = 63609\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 799\n",
      "all branching entropies was computed # words = 63993\n",
      "all accessor variety was computed # words = 63993\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 801\n",
      "all branching entropies was computed # words = 64280\n",
      "all accessor variety was computed # words = 64280\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 802\n",
      "all branching entropies was computed # words = 64334\n",
      "all accessor variety was computed # words = 64334\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 802\n",
      "all branching entropies was computed # words = 64383\n",
      "all accessor variety was computed # words = 64383\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 806\n",
      "all branching entropies was computed # words = 64540\n",
      "all accessor variety was computed # words = 64540\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 811\n",
      "all branching entropies was computed # words = 64632\n",
      "all accessor variety was computed # words = 64632\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 817\n",
      "all branching entropies was computed # words = 64861\n",
      "all accessor variety was computed # words = 64861\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 820\n",
      "all branching entropies was computed # words = 64916\n",
      "all accessor variety was computed # words = 64916\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 823\n",
      "all branching entropies was computed # words = 65142\n",
      "all accessor variety was computed # words = 65142\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 824\n",
      "all branching entropies was computed # words = 65379\n",
      "all accessor variety was computed # words = 65379\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 829\n",
      "all branching entropies was computed # words = 65481\n",
      "all accessor variety was computed # words = 65481\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 832\n",
      "all branching entropies was computed # words = 65658\n",
      "all accessor variety was computed # words = 65658\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 843\n",
      "all branching entropies was computed # words = 66220\n",
      "all accessor variety was computed # words = 66220\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 848\n",
      "all branching entropies was computed # words = 66286\n",
      "all accessor variety was computed # words = 66286\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 852\n",
      "all branching entropies was computed # words = 67232\n",
      "all accessor variety was computed # words = 67232\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 852\n",
      "all branching entropies was computed # words = 67589\n",
      "all accessor variety was computed # words = 67589\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 855\n",
      "all branching entropies was computed # words = 67648\n",
      "all accessor variety was computed # words = 67648\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 858\n",
      "all branching entropies was computed # words = 67660\n",
      "all accessor variety was computed # words = 67660\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 862\n",
      "all branching entropies was computed # words = 67810\n",
      "all accessor variety was computed # words = 67810\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 865\n",
      "all branching entropies was computed # words = 67903\n",
      "all accessor variety was computed # words = 67903\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 868\n",
      "all branching entropies was computed # words = 67973\n",
      "all accessor variety was computed # words = 67973\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 868\n",
      "all branching entropies was computed # words = 68002\n",
      "all accessor variety was computed # words = 68002\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 876\n",
      "all branching entropies was computed # words = 68234\n",
      "all accessor variety was computed # words = 68234\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 884\n",
      "all branching entropies was computed # words = 68797\n",
      "all accessor variety was computed # words = 68797\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 887\n",
      "all branching entropies was computed # words = 68816\n",
      "all accessor variety was computed # words = 68816\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 888\n",
      "all branching entropies was computed # words = 68980\n",
      "all accessor variety was computed # words = 68980\n",
      "'스벅'\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 894\n",
      "all branching entropies was computed # words = 69591\n",
      "all accessor variety was computed # words = 69591\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 899\n",
      "all branching entropies was computed # words = 69635\n",
      "all accessor variety was computed # words = 69635\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 908\n",
      "all branching entropies was computed # words = 70095\n",
      "all accessor variety was computed # words = 70095\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 912\n",
      "all branching entropies was computed # words = 70218\n",
      "all accessor variety was computed # words = 70218\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 70253\n",
      "all accessor variety was computed # words = 70253\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 919\n",
      "all branching entropies was computed # words = 70391\n",
      "all accessor variety was computed # words = 70391\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 922\n",
      "all branching entropies was computed # words = 70421\n",
      "all accessor variety was computed # words = 70421\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 925\n",
      "all branching entropies was computed # words = 70456\n",
      "all accessor variety was computed # words = 70456\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 928\n",
      "all branching entropies was computed # words = 70524\n",
      "all accessor variety was computed # words = 70524\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 929\n",
      "all branching entropies was computed # words = 70567\n",
      "all accessor variety was computed # words = 70567\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 937\n",
      "all branching entropies was computed # words = 71018\n",
      "all accessor variety was computed # words = 71018\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 941\n",
      "all branching entropies was computed # words = 71049\n",
      "all accessor variety was computed # words = 71049\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 944\n",
      "all branching entropies was computed # words = 71308\n",
      "all accessor variety was computed # words = 71308\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 949\n",
      "all branching entropies was computed # words = 71448\n",
      "all accessor variety was computed # words = 71448\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 950\n",
      "all branching entropies was computed # words = 71667\n",
      "all accessor variety was computed # words = 71667\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 957\n",
      "all branching entropies was computed # words = 71999\n",
      "all accessor variety was computed # words = 71999\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 960\n",
      "all branching entropies was computed # words = 72068\n",
      "all accessor variety was computed # words = 72068\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 963\n",
      "all branching entropies was computed # words = 72158\n",
      "all accessor variety was computed # words = 72158\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 966\n",
      "all branching entropies was computed # words = 72247\n",
      "all accessor variety was computed # words = 72247\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 971\n",
      "all branching entropies was computed # words = 72271\n",
      "all accessor variety was computed # words = 72271\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 973\n",
      "all branching entropies was computed # words = 72333\n",
      "all accessor variety was computed # words = 72333\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 989\n",
      "all branching entropies was computed # words = 72626\n",
      "all accessor variety was computed # words = 72626\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 992\n",
      "all branching entropies was computed # words = 72648\n",
      "all accessor variety was computed # words = 72648\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 995\n",
      "all branching entropies was computed # words = 72786\n",
      "all accessor variety was computed # words = 72786\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 998\n",
      "all branching entropies was computed # words = 72890\n",
      "all accessor variety was computed # words = 72890\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1002\n",
      "all branching entropies was computed # words = 72992\n",
      "all accessor variety was computed # words = 72992\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1010\n",
      "all branching entropies was computed # words = 73276\n",
      "all accessor variety was computed # words = 73276\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1025\n",
      "all branching entropies was computed # words = 73602\n",
      "all accessor variety was computed # words = 73602\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1028\n",
      "all branching entropies was computed # words = 73878\n",
      "all accessor variety was computed # words = 73878\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1031\n",
      "all branching entropies was computed # words = 73904\n",
      "all accessor variety was computed # words = 73904\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1036\n",
      "all branching entropies was computed # words = 74313\n",
      "all accessor variety was computed # words = 74313\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1039\n",
      "all branching entropies was computed # words = 74476\n",
      "all accessor variety was computed # words = 74476\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1044\n",
      "all branching entropies was computed # words = 74513\n",
      "all accessor variety was computed # words = 74513\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1044\n",
      "all branching entropies was computed # words = 74520\n",
      "all accessor variety was computed # words = 74520\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1047\n",
      "all branching entropies was computed # words = 74625\n",
      "all accessor variety was computed # words = 74625\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1047\n",
      "all branching entropies was computed # words = 74659\n",
      "all accessor variety was computed # words = 74659\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1050\n",
      "all branching entropies was computed # words = 74790\n",
      "all accessor variety was computed # words = 74790\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1055\n",
      "all branching entropies was computed # words = 74831\n",
      "all accessor variety was computed # words = 74831\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1058\n",
      "all branching entropies was computed # words = 75221\n",
      "all accessor variety was computed # words = 75221\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1059\n",
      "all branching entropies was computed # words = 75260\n",
      "all accessor variety was computed # words = 75260\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1061\n",
      "all branching entropies was computed # words = 75417\n",
      "all accessor variety was computed # words = 75417\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1083\n",
      "all branching entropies was computed # words = 76752\n",
      "all accessor variety was computed # words = 76752\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1086\n",
      "all branching entropies was computed # words = 76776\n",
      "all accessor variety was computed # words = 76776\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1089\n",
      "all branching entropies was computed # words = 76792\n",
      "all accessor variety was computed # words = 76792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1089\n",
      "all branching entropies was computed # words = 76805\n",
      "all accessor variety was computed # words = 76805\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1092\n",
      "all branching entropies was computed # words = 76860\n",
      "all accessor variety was computed # words = 76860\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1094\n",
      "all branching entropies was computed # words = 76881\n",
      "all accessor variety was computed # words = 76881\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1097\n",
      "all branching entropies was computed # words = 77212\n",
      "all accessor variety was computed # words = 77212\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1106\n",
      "all branching entropies was computed # words = 77262\n",
      "all accessor variety was computed # words = 77262\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1114\n",
      "all branching entropies was computed # words = 77558\n",
      "all accessor variety was computed # words = 77558\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1117\n",
      "all branching entropies was computed # words = 77579\n",
      "all accessor variety was computed # words = 77579\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1120\n",
      "all branching entropies was computed # words = 77679\n",
      "all accessor variety was computed # words = 77679\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1125\n",
      "all branching entropies was computed # words = 77708\n",
      "all accessor variety was computed # words = 77708\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1129\n",
      "all branching entropies was computed # words = 77755\n",
      "all accessor variety was computed # words = 77755\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1143\n",
      "all branching entropies was computed # words = 78286\n",
      "all accessor variety was computed # words = 78286\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1146\n",
      "all branching entropies was computed # words = 78306\n",
      "all accessor variety was computed # words = 78306\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1149\n",
      "all branching entropies was computed # words = 78396\n",
      "all accessor variety was computed # words = 78396\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1152\n",
      "all branching entropies was computed # words = 78409\n",
      "all accessor variety was computed # words = 78409\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1153\n",
      "all branching entropies was computed # words = 78485\n",
      "all accessor variety was computed # words = 78485\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1154\n",
      "all branching entropies was computed # words = 78522\n",
      "all accessor variety was computed # words = 78522\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1157\n",
      "all branching entropies was computed # words = 78557\n",
      "all accessor variety was computed # words = 78557\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1158\n",
      "all branching entropies was computed # words = 78605\n",
      "all accessor variety was computed # words = 78605\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1162\n",
      "all branching entropies was computed # words = 78936\n",
      "all accessor variety was computed # words = 78936\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1165\n",
      "all branching entropies was computed # words = 78990\n",
      "all accessor variety was computed # words = 78990\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1168\n",
      "all branching entropies was computed # words = 79033\n",
      "all accessor variety was computed # words = 79033\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1173\n",
      "all branching entropies was computed # words = 79299\n",
      "all accessor variety was computed # words = 79299\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1175\n",
      "all branching entropies was computed # words = 79486\n",
      "all accessor variety was computed # words = 79486\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1183\n",
      "all branching entropies was computed # words = 79853\n",
      "all accessor variety was computed # words = 79853\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1186\n",
      "all branching entropies was computed # words = 79919\n",
      "all accessor variety was computed # words = 79919\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1188\n",
      "all branching entropies was computed # words = 79983\n",
      "all accessor variety was computed # words = 79983\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1191\n",
      "all branching entropies was computed # words = 80022\n",
      "all accessor variety was computed # words = 80022\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1202\n",
      "all branching entropies was computed # words = 80114\n",
      "all accessor variety was computed # words = 80114\n",
      "'가카'\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1205\n",
      "all branching entropies was computed # words = 80181\n",
      "all accessor variety was computed # words = 80181\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1208\n",
      "all branching entropies was computed # words = 80274\n",
      "all accessor variety was computed # words = 80274\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1208\n",
      "all branching entropies was computed # words = 80363\n",
      "all accessor variety was computed # words = 80363\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1211\n",
      "all branching entropies was computed # words = 80387\n",
      "all accessor variety was computed # words = 80387\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1214\n",
      "all branching entropies was computed # words = 80455\n",
      "all accessor variety was computed # words = 80455\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1217\n",
      "all branching entropies was computed # words = 80700\n",
      "all accessor variety was computed # words = 80700\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1220\n",
      "all branching entropies was computed # words = 80861\n",
      "all accessor variety was computed # words = 80861\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1223\n",
      "all branching entropies was computed # words = 80877\n",
      "all accessor variety was computed # words = 80877\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1225\n",
      "all branching entropies was computed # words = 81248\n",
      "all accessor variety was computed # words = 81248\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1230\n",
      "all branching entropies was computed # words = 81272\n",
      "all accessor variety was computed # words = 81272\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1231\n",
      "all branching entropies was computed # words = 81329\n",
      "all accessor variety was computed # words = 81329\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1234\n",
      "all branching entropies was computed # words = 81366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 81366\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1237\n",
      "all branching entropies was computed # words = 81385\n",
      "all accessor variety was computed # words = 81385\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1241\n",
      "all branching entropies was computed # words = 81524\n",
      "all accessor variety was computed # words = 81524\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1245\n",
      "all branching entropies was computed # words = 81549\n",
      "all accessor variety was computed # words = 81549\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1246\n",
      "all branching entropies was computed # words = 81591\n",
      "all accessor variety was computed # words = 81591\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1251\n",
      "all branching entropies was computed # words = 81629\n",
      "all accessor variety was computed # words = 81629\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1255\n",
      "all branching entropies was computed # words = 81714\n",
      "all accessor variety was computed # words = 81714\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1258\n",
      "all branching entropies was computed # words = 81746\n",
      "all accessor variety was computed # words = 81746\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1258\n",
      "all branching entropies was computed # words = 81750\n",
      "all accessor variety was computed # words = 81750\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1261\n",
      "all branching entropies was computed # words = 81999\n",
      "all accessor variety was computed # words = 81999\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1264\n",
      "all branching entropies was computed # words = 82093\n",
      "all accessor variety was computed # words = 82093\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1269\n",
      "all branching entropies was computed # words = 82195\n",
      "all accessor variety was computed # words = 82195\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1272\n",
      "all branching entropies was computed # words = 82221\n",
      "all accessor variety was computed # words = 82221\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 82234\n",
      "all accessor variety was computed # words = 82234\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1282\n",
      "all branching entropies was computed # words = 82442\n",
      "all accessor variety was computed # words = 82442\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1285\n",
      "all branching entropies was computed # words = 82472\n",
      "all accessor variety was computed # words = 82472\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1286\n",
      "all branching entropies was computed # words = 82584\n",
      "all accessor variety was computed # words = 82584\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1293\n",
      "all branching entropies was computed # words = 82911\n",
      "all accessor variety was computed # words = 82911\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1298\n",
      "all branching entropies was computed # words = 83019\n",
      "all accessor variety was computed # words = 83019\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1304\n",
      "all branching entropies was computed # words = 83192\n",
      "all accessor variety was computed # words = 83192\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1306\n",
      "all branching entropies was computed # words = 83232\n",
      "all accessor variety was computed # words = 83232\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1306\n",
      "all branching entropies was computed # words = 83237\n",
      "all accessor variety was computed # words = 83237\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1307\n",
      "all branching entropies was computed # words = 83304\n",
      "all accessor variety was computed # words = 83304\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1310\n",
      "all branching entropies was computed # words = 83323\n",
      "all accessor variety was computed # words = 83323\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1319\n",
      "all branching entropies was computed # words = 84029\n",
      "all accessor variety was computed # words = 84029\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1322\n",
      "all branching entropies was computed # words = 84050\n",
      "all accessor variety was computed # words = 84050\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1326\n",
      "all branching entropies was computed # words = 84127\n",
      "all accessor variety was computed # words = 84127\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1334\n",
      "all branching entropies was computed # words = 84145\n",
      "all accessor variety was computed # words = 84145\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1334\n",
      "all branching entropies was computed # words = 84214\n",
      "all accessor variety was computed # words = 84214\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1337\n",
      "all branching entropies was computed # words = 84241\n",
      "all accessor variety was computed # words = 84241\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1340\n",
      "all branching entropies was computed # words = 84257\n",
      "all accessor variety was computed # words = 84257\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1341\n",
      "all branching entropies was computed # words = 84301\n",
      "all accessor variety was computed # words = 84301\n",
      "'어케'\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1344\n",
      "all branching entropies was computed # words = 84343\n",
      "all accessor variety was computed # words = 84343\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1347\n",
      "all branching entropies was computed # words = 84364\n",
      "all accessor variety was computed # words = 84364\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1352\n",
      "all branching entropies was computed # words = 84392\n",
      "all accessor variety was computed # words = 84392\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1352\n",
      "all branching entropies was computed # words = 84393\n",
      "all accessor variety was computed # words = 84393\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1357\n",
      "all branching entropies was computed # words = 84426\n",
      "all accessor variety was computed # words = 84426\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1364\n",
      "all branching entropies was computed # words = 84541\n",
      "all accessor variety was computed # words = 84541\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1367\n",
      "all branching entropies was computed # words = 84610\n",
      "all accessor variety was computed # words = 84610\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1372\n",
      "all branching entropies was computed # words = 84630\n",
      "all accessor variety was computed # words = 84630\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 84646\n",
      "all accessor variety was computed # words = 84646\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1379\n",
      "all branching entropies was computed # words = 84697\n",
      "all accessor variety was computed # words = 84697\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1384\n",
      "all branching entropies was computed # words = 84811\n",
      "all accessor variety was computed # words = 84811\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1384\n",
      "all branching entropies was computed # words = 84822\n",
      "all accessor variety was computed # words = 84822\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1387\n",
      "all branching entropies was computed # words = 84844\n",
      "all accessor variety was computed # words = 84844\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1392\n",
      "all branching entropies was computed # words = 84911\n",
      "all accessor variety was computed # words = 84911\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1397\n",
      "all branching entropies was computed # words = 84951\n",
      "all accessor variety was computed # words = 84951\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1398\n",
      "all branching entropies was computed # words = 85017\n",
      "all accessor variety was computed # words = 85017\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1400\n",
      "all branching entropies was computed # words = 85035\n",
      "all accessor variety was computed # words = 85035\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1404\n",
      "all branching entropies was computed # words = 85046\n",
      "all accessor variety was computed # words = 85046\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1406\n",
      "all branching entropies was computed # words = 85105\n",
      "all accessor variety was computed # words = 85105\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1407\n",
      "all branching entropies was computed # words = 85196\n",
      "all accessor variety was computed # words = 85196\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1412\n",
      "all branching entropies was computed # words = 85216\n",
      "all accessor variety was computed # words = 85216\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1413\n",
      "all branching entropies was computed # words = 85247\n",
      "all accessor variety was computed # words = 85247\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1418\n",
      "all branching entropies was computed # words = 85314\n",
      "all accessor variety was computed # words = 85314\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1421\n",
      "all branching entropies was computed # words = 85361\n",
      "all accessor variety was computed # words = 85361\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1422\n",
      "all branching entropies was computed # words = 85404\n",
      "all accessor variety was computed # words = 85404\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1425\n",
      "all branching entropies was computed # words = 85433\n",
      "all accessor variety was computed # words = 85433\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1429\n",
      "all branching entropies was computed # words = 85451\n",
      "all accessor variety was computed # words = 85451\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1430\n",
      "all branching entropies was computed # words = 85528\n",
      "all accessor variety was computed # words = 85528\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1433\n",
      "all branching entropies was computed # words = 85636\n",
      "all accessor variety was computed # words = 85636\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1438\n",
      "all branching entropies was computed # words = 85670\n",
      "all accessor variety was computed # words = 85670\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1442\n",
      "all branching entropies was computed # words = 85679\n",
      "all accessor variety was computed # words = 85679\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1447\n",
      "all branching entropies was computed # words = 85688\n",
      "all accessor variety was computed # words = 85688\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1452\n",
      "all branching entropies was computed # words = 85944\n",
      "all accessor variety was computed # words = 85944\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1455\n",
      "all branching entropies was computed # words = 85964\n",
      "all accessor variety was computed # words = 85964\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1460\n",
      "all branching entropies was computed # words = 86038\n",
      "all accessor variety was computed # words = 86038\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1464\n",
      "all branching entropies was computed # words = 86145\n",
      "all accessor variety was computed # words = 86145\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1467\n",
      "all branching entropies was computed # words = 86150\n",
      "all accessor variety was computed # words = 86150\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1470\n",
      "all branching entropies was computed # words = 86154\n",
      "all accessor variety was computed # words = 86154\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1473\n",
      "all branching entropies was computed # words = 86212\n",
      "all accessor variety was computed # words = 86212\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1476\n",
      "all branching entropies was computed # words = 86290\n",
      "all accessor variety was computed # words = 86290\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1479\n",
      "all branching entropies was computed # words = 86317\n",
      "all accessor variety was computed # words = 86317\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1482\n",
      "all branching entropies was computed # words = 86340\n",
      "all accessor variety was computed # words = 86340\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1485\n",
      "all branching entropies was computed # words = 86382\n",
      "all accessor variety was computed # words = 86382\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1490\n",
      "all branching entropies was computed # words = 86399\n",
      "all accessor variety was computed # words = 86399\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1495\n",
      "all branching entropies was computed # words = 86521\n",
      "all accessor variety was computed # words = 86521\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1496\n",
      "all branching entropies was computed # words = 86555\n",
      "all accessor variety was computed # words = 86555\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1496\n",
      "all branching entropies was computed # words = 86752\n",
      "all accessor variety was computed # words = 86752\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1497\n",
      "all branching entropies was computed # words = 86808\n",
      "all accessor variety was computed # words = 86808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1500\n",
      "all branching entropies was computed # words = 86844\n",
      "all accessor variety was computed # words = 86844\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1506\n",
      "all branching entropies was computed # words = 87008\n",
      "all accessor variety was computed # words = 87008\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1508\n",
      "all branching entropies was computed # words = 87175\n",
      "all accessor variety was computed # words = 87175\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1510\n",
      "all branching entropies was computed # words = 87208\n",
      "all accessor variety was computed # words = 87208\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1513\n",
      "all branching entropies was computed # words = 87259\n",
      "all accessor variety was computed # words = 87259\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1517\n",
      "all branching entropies was computed # words = 87325\n",
      "all accessor variety was computed # words = 87325\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1522\n",
      "all branching entropies was computed # words = 87621\n",
      "all accessor variety was computed # words = 87621\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1528\n",
      "all branching entropies was computed # words = 87853\n",
      "all accessor variety was computed # words = 87853\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1533\n",
      "all branching entropies was computed # words = 87978\n",
      "all accessor variety was computed # words = 87978\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1536\n",
      "all branching entropies was computed # words = 88001\n",
      "all accessor variety was computed # words = 88001\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1541\n",
      "all branching entropies was computed # words = 88110\n",
      "all accessor variety was computed # words = 88110\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1543\n",
      "all branching entropies was computed # words = 88232\n",
      "all accessor variety was computed # words = 88232\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1546\n",
      "all branching entropies was computed # words = 88274\n",
      "all accessor variety was computed # words = 88274\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1552\n",
      "all branching entropies was computed # words = 88592\n",
      "all accessor variety was computed # words = 88592\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1552\n",
      "all branching entropies was computed # words = 88606\n",
      "all accessor variety was computed # words = 88606\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1561\n",
      "all branching entropies was computed # words = 88631\n",
      "all accessor variety was computed # words = 88631\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1569\n",
      "all branching entropies was computed # words = 88749\n",
      "all accessor variety was computed # words = 88749\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1572\n",
      "all branching entropies was computed # words = 88784\n",
      "all accessor variety was computed # words = 88784\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1575\n",
      "all branching entropies was computed # words = 88817\n",
      "all accessor variety was computed # words = 88817\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1576\n",
      "all branching entropies was computed # words = 88835\n",
      "all accessor variety was computed # words = 88835\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1577\n",
      "all branching entropies was computed # words = 88854\n",
      "all accessor variety was computed # words = 88854\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1579\n",
      "all branching entropies was computed # words = 88876\n",
      "all accessor variety was computed # words = 88876\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1584\n",
      "all branching entropies was computed # words = 88906\n",
      "all accessor variety was computed # words = 88906\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1587\n",
      "all branching entropies was computed # words = 88940\n",
      "all accessor variety was computed # words = 88940\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1589\n",
      "all branching entropies was computed # words = 88961\n",
      "all accessor variety was computed # words = 88961\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1597\n",
      "all branching entropies was computed # words = 88980\n",
      "all accessor variety was computed # words = 88980\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1602\n",
      "all branching entropies was computed # words = 89041\n",
      "all accessor variety was computed # words = 89041\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1603\n",
      "all branching entropies was computed # words = 89113\n",
      "all accessor variety was computed # words = 89113\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1604\n",
      "all branching entropies was computed # words = 89366\n",
      "all accessor variety was computed # words = 89366\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1607\n",
      "all branching entropies was computed # words = 89384\n",
      "all accessor variety was computed # words = 89384\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1612\n",
      "all branching entropies was computed # words = 89443\n",
      "all accessor variety was computed # words = 89443\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1612\n",
      "all branching entropies was computed # words = 89452\n",
      "all accessor variety was computed # words = 89452\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1615\n",
      "all branching entropies was computed # words = 89493\n",
      "all accessor variety was computed # words = 89493\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1621\n",
      "all branching entropies was computed # words = 89704\n",
      "all accessor variety was computed # words = 89704\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1626\n",
      "all branching entropies was computed # words = 89776\n",
      "all accessor variety was computed # words = 89776\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1626\n",
      "all branching entropies was computed # words = 90159\n",
      "all accessor variety was computed # words = 90159\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1628\n",
      "all branching entropies was computed # words = 90183\n",
      "all accessor variety was computed # words = 90183\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1631\n",
      "all branching entropies was computed # words = 90194\n",
      "all accessor variety was computed # words = 90194\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1634\n",
      "all branching entropies was computed # words = 90257\n",
      "all accessor variety was computed # words = 90257\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1637\n",
      "all branching entropies was computed # words = 90363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 90363\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1637\n",
      "all branching entropies was computed # words = 90414\n",
      "all accessor variety was computed # words = 90414\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1644\n",
      "all branching entropies was computed # words = 90480\n",
      "all accessor variety was computed # words = 90480\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1645\n",
      "all branching entropies was computed # words = 90490\n",
      "all accessor variety was computed # words = 90490\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1647\n",
      "all branching entropies was computed # words = 90525\n",
      "all accessor variety was computed # words = 90525\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1649\n",
      "all branching entropies was computed # words = 90714\n",
      "all accessor variety was computed # words = 90714\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1652\n",
      "all branching entropies was computed # words = 90731\n",
      "all accessor variety was computed # words = 90731\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1652\n",
      "all branching entropies was computed # words = 90744\n",
      "all accessor variety was computed # words = 90744\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1655\n",
      "all branching entropies was computed # words = 90809\n",
      "all accessor variety was computed # words = 90809\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1657\n",
      "all branching entropies was computed # words = 90852\n",
      "all accessor variety was computed # words = 90852\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1661\n",
      "all branching entropies was computed # words = 90922\n",
      "all accessor variety was computed # words = 90922\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1665\n",
      "all branching entropies was computed # words = 91116\n",
      "all accessor variety was computed # words = 91116\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1673\n",
      "all branching entropies was computed # words = 91420\n",
      "all accessor variety was computed # words = 91420\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1677\n",
      "all branching entropies was computed # words = 91438\n",
      "all accessor variety was computed # words = 91438\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1679\n",
      "all branching entropies was computed # words = 91544\n",
      "all accessor variety was computed # words = 91544\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1683\n",
      "all branching entropies was computed # words = 91568\n",
      "all accessor variety was computed # words = 91568\n",
      "training was done. used memory 1.128 Gb1.128 Gb\n",
      "all cohesion probabilities was computed. # words = 1684\n",
      "all branching entropies was computed # words = 91604\n",
      "all accessor variety was computed # words = 91604\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1694\n",
      "all branching entropies was computed # words = 92005\n",
      "all accessor variety was computed # words = 92005\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1698\n",
      "all branching entropies was computed # words = 92170\n",
      "all accessor variety was computed # words = 92170\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1701\n",
      "all branching entropies was computed # words = 92219\n",
      "all accessor variety was computed # words = 92219\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1707\n",
      "all branching entropies was computed # words = 92406\n",
      "all accessor variety was computed # words = 92406\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1711\n",
      "all branching entropies was computed # words = 92441\n",
      "all accessor variety was computed # words = 92441\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1714\n",
      "all branching entropies was computed # words = 92478\n",
      "all accessor variety was computed # words = 92478\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1717\n",
      "all branching entropies was computed # words = 92489\n",
      "all accessor variety was computed # words = 92489\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1720\n",
      "all branching entropies was computed # words = 92507\n",
      "all accessor variety was computed # words = 92507\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1723\n",
      "all branching entropies was computed # words = 92574\n",
      "all accessor variety was computed # words = 92574\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1724\n",
      "all branching entropies was computed # words = 92731\n",
      "all accessor variety was computed # words = 92731\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1731\n",
      "all branching entropies was computed # words = 92781\n",
      "all accessor variety was computed # words = 92781\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1734\n",
      "all branching entropies was computed # words = 92821\n",
      "all accessor variety was computed # words = 92821\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1737\n",
      "all branching entropies was computed # words = 92838\n",
      "all accessor variety was computed # words = 92838\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1741\n",
      "all branching entropies was computed # words = 92881\n",
      "all accessor variety was computed # words = 92881\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1744\n",
      "all branching entropies was computed # words = 92902\n",
      "all accessor variety was computed # words = 92902\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1748\n",
      "all branching entropies was computed # words = 92911\n",
      "all accessor variety was computed # words = 92911\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1751\n",
      "all branching entropies was computed # words = 92964\n",
      "all accessor variety was computed # words = 92964\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1751\n",
      "all branching entropies was computed # words = 93087\n",
      "all accessor variety was computed # words = 93087\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1752\n",
      "all branching entropies was computed # words = 93148\n",
      "all accessor variety was computed # words = 93148\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1756\n",
      "all branching entropies was computed # words = 93306\n",
      "all accessor variety was computed # words = 93306\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1757\n",
      "all branching entropies was computed # words = 93339\n",
      "all accessor variety was computed # words = 93339\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1760\n",
      "all branching entropies was computed # words = 93415\n",
      "all accessor variety was computed # words = 93415\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1761\n",
      "all branching entropies was computed # words = 93432\n",
      "all accessor variety was computed # words = 93432\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 93457\n",
      "all accessor variety was computed # words = 93457\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1766\n",
      "all branching entropies was computed # words = 93573\n",
      "all accessor variety was computed # words = 93573\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1769\n",
      "all branching entropies was computed # words = 93594\n",
      "all accessor variety was computed # words = 93594\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1769\n",
      "all branching entropies was computed # words = 93631\n",
      "all accessor variety was computed # words = 93631\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1773\n",
      "all branching entropies was computed # words = 93688\n",
      "all accessor variety was computed # words = 93688\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1776\n",
      "all branching entropies was computed # words = 93742\n",
      "all accessor variety was computed # words = 93742\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1777\n",
      "all branching entropies was computed # words = 93824\n",
      "all accessor variety was computed # words = 93824\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1782\n",
      "all branching entropies was computed # words = 93862\n",
      "all accessor variety was computed # words = 93862\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1791\n",
      "all branching entropies was computed # words = 94097\n",
      "all accessor variety was computed # words = 94097\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1793\n",
      "all branching entropies was computed # words = 94123\n",
      "all accessor variety was computed # words = 94123\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1794\n",
      "all branching entropies was computed # words = 94131\n",
      "all accessor variety was computed # words = 94131\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1794\n",
      "all branching entropies was computed # words = 94245\n",
      "all accessor variety was computed # words = 94245\n",
      "training was done. used memory 1.138 Gb1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 1797\n",
      "all branching entropies was computed # words = 94285\n",
      "all accessor variety was computed # words = 94285\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1918\n",
      "all branching entropies was computed # words = 100388\n",
      "all accessor variety was computed # words = 100388\n",
      "training was done. used memory 1.188 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1921\n",
      "all branching entropies was computed # words = 100407\n",
      "all accessor variety was computed # words = 100407\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1924\n",
      "all branching entropies was computed # words = 100428\n",
      "all accessor variety was computed # words = 100428\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1929\n",
      "all branching entropies was computed # words = 100515\n",
      "all accessor variety was computed # words = 100515\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1932\n",
      "all branching entropies was computed # words = 100530\n",
      "all accessor variety was computed # words = 100530\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1935\n",
      "all branching entropies was computed # words = 100554\n",
      "all accessor variety was computed # words = 100554\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1938\n",
      "all branching entropies was computed # words = 100567\n",
      "all accessor variety was computed # words = 100567\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1941\n",
      "all branching entropies was computed # words = 100587\n",
      "all accessor variety was computed # words = 100587\n",
      "training was done. used memory 1.207 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1943\n",
      "all branching entropies was computed # words = 101006\n",
      "all accessor variety was computed # words = 101006\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1943\n",
      "all branching entropies was computed # words = 101012\n",
      "all accessor variety was computed # words = 101012\n",
      "training was done. used memory 1.188 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1946\n",
      "all branching entropies was computed # words = 101041\n",
      "all accessor variety was computed # words = 101041\n",
      "training was done. used memory 1.188 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1949\n",
      "all branching entropies was computed # words = 101078\n",
      "all accessor variety was computed # words = 101078\n",
      "training was done. used memory 1.207 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1954\n",
      "all branching entropies was computed # words = 101170\n",
      "all accessor variety was computed # words = 101170\n",
      "training was done. used memory 1.188 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 101191\n",
      "all accessor variety was computed # words = 101191\n",
      "training was done. used memory 1.207 Gb1.188 Gb\n",
      "all cohesion probabilities was computed. # words = 1961\n",
      "all branching entropies was computed # words = 101228\n",
      "all accessor variety was computed # words = 101228\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1961\n",
      "all branching entropies was computed # words = 101236\n",
      "all accessor variety was computed # words = 101236\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1977\n",
      "all branching entropies was computed # words = 101715\n",
      "all accessor variety was computed # words = 101715\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1980\n",
      "all branching entropies was computed # words = 101726\n",
      "all accessor variety was computed # words = 101726\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 101802\n",
      "all accessor variety was computed # words = 101802\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1987\n",
      "all branching entropies was computed # words = 101868\n",
      "all accessor variety was computed # words = 101868\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 101888\n",
      "all accessor variety was computed # words = 101888\n",
      "training was done. used memory 1.207 Gb1.168 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 101937\n",
      "all accessor variety was computed # words = 101937\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 1997\n",
      "all branching entropies was computed # words = 101987\n",
      "all accessor variety was computed # words = 101987\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 1998\n",
      "all branching entropies was computed # words = 102035\n",
      "all accessor variety was computed # words = 102035\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2006\n",
      "all branching entropies was computed # words = 102260\n",
      "all accessor variety was computed # words = 102260\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2011\n",
      "all branching entropies was computed # words = 102312\n",
      "all accessor variety was computed # words = 102312\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2015\n",
      "all branching entropies was computed # words = 102353\n",
      "all accessor variety was computed # words = 102353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2021\n",
      "all branching entropies was computed # words = 102531\n",
      "all accessor variety was computed # words = 102531\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2028\n",
      "all branching entropies was computed # words = 102671\n",
      "all accessor variety was computed # words = 102671\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2029\n",
      "all branching entropies was computed # words = 102690\n",
      "all accessor variety was computed # words = 102690\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2034\n",
      "all branching entropies was computed # words = 102782\n",
      "all accessor variety was computed # words = 102782\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2036\n",
      "all branching entropies was computed # words = 102798\n",
      "all accessor variety was computed # words = 102798\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2039\n",
      "all branching entropies was computed # words = 102846\n",
      "all accessor variety was computed # words = 102846\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2041\n",
      "all branching entropies was computed # words = 102851\n",
      "all accessor variety was computed # words = 102851\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2049\n",
      "all branching entropies was computed # words = 102901\n",
      "all accessor variety was computed # words = 102901\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2051\n",
      "all branching entropies was computed # words = 102913\n",
      "all accessor variety was computed # words = 102913\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2054\n",
      "all branching entropies was computed # words = 102925\n",
      "all accessor variety was computed # words = 102925\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2056\n",
      "all branching entropies was computed # words = 103050\n",
      "all accessor variety was computed # words = 103050\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2056\n",
      "all branching entropies was computed # words = 103050\n",
      "all accessor variety was computed # words = 103050\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 103098\n",
      "all accessor variety was computed # words = 103098\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 103101\n",
      "all accessor variety was computed # words = 103101\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2061\n",
      "all branching entropies was computed # words = 103116\n",
      "all accessor variety was computed # words = 103116\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2062\n",
      "all branching entropies was computed # words = 103124\n",
      "all accessor variety was computed # words = 103124\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2065\n",
      "all branching entropies was computed # words = 103214\n",
      "all accessor variety was computed # words = 103214\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2070\n",
      "all branching entropies was computed # words = 103243\n",
      "all accessor variety was computed # words = 103243\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2075\n",
      "all branching entropies was computed # words = 103710\n",
      "all accessor variety was computed # words = 103710\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2079\n",
      "all branching entropies was computed # words = 103812\n",
      "all accessor variety was computed # words = 103812\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2081\n",
      "all branching entropies was computed # words = 103984\n",
      "all accessor variety was computed # words = 103984\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2088\n",
      "all branching entropies was computed # words = 104111\n",
      "all accessor variety was computed # words = 104111\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2091\n",
      "all branching entropies was computed # words = 104136\n",
      "all accessor variety was computed # words = 104136\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2096\n",
      "all branching entropies was computed # words = 104173\n",
      "all accessor variety was computed # words = 104173\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2101\n",
      "all branching entropies was computed # words = 104209\n",
      "all accessor variety was computed # words = 104209\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2103\n",
      "all branching entropies was computed # words = 104228\n",
      "all accessor variety was computed # words = 104228\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2106\n",
      "all branching entropies was computed # words = 104346\n",
      "all accessor variety was computed # words = 104346\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2112\n",
      "all branching entropies was computed # words = 104532\n",
      "all accessor variety was computed # words = 104532\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2115\n",
      "all branching entropies was computed # words = 104616\n",
      "all accessor variety was computed # words = 104616\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2122\n",
      "all branching entropies was computed # words = 104660\n",
      "all accessor variety was computed # words = 104660\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2127\n",
      "all branching entropies was computed # words = 104705\n",
      "all accessor variety was computed # words = 104705\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 104784\n",
      "all accessor variety was computed # words = 104784\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2132\n",
      "all branching entropies was computed # words = 104808\n",
      "all accessor variety was computed # words = 104808\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 104926\n",
      "all accessor variety was computed # words = 104926\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2137\n",
      "all branching entropies was computed # words = 104979\n",
      "all accessor variety was computed # words = 104979\n",
      "'나옴'\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2138\n",
      "all branching entropies was computed # words = 105042\n",
      "all accessor variety was computed # words = 105042\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2141\n",
      "all branching entropies was computed # words = 105082\n",
      "all accessor variety was computed # words = 105082\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 105118\n",
      "all accessor variety was computed # words = 105118\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2147\n",
      "all branching entropies was computed # words = 105153\n",
      "all accessor variety was computed # words = 105153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 105341\n",
      "all accessor variety was computed # words = 105341\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2153\n",
      "all branching entropies was computed # words = 105379\n",
      "all accessor variety was computed # words = 105379\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2156\n",
      "all branching entropies was computed # words = 105417\n",
      "all accessor variety was computed # words = 105417\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2159\n",
      "all branching entropies was computed # words = 105443\n",
      "all accessor variety was computed # words = 105443\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 105624\n",
      "all accessor variety was computed # words = 105624\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2165\n",
      "all branching entropies was computed # words = 105639\n",
      "all accessor variety was computed # words = 105639\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 105668\n",
      "all accessor variety was computed # words = 105668\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2169\n",
      "all branching entropies was computed # words = 105819\n",
      "all accessor variety was computed # words = 105819\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2172\n",
      "all branching entropies was computed # words = 105845\n",
      "all accessor variety was computed # words = 105845\n",
      "'이수역'\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 105933\n",
      "all accessor variety was computed # words = 105933\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2178\n",
      "all branching entropies was computed # words = 105956\n",
      "all accessor variety was computed # words = 105956\n",
      "'나연'\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 105989\n",
      "all accessor variety was computed # words = 105989\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2183\n",
      "all branching entropies was computed # words = 106050\n",
      "all accessor variety was computed # words = 106050\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 106084\n",
      "all accessor variety was computed # words = 106084\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2190\n",
      "all branching entropies was computed # words = 106214\n",
      "all accessor variety was computed # words = 106214\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2190\n",
      "all branching entropies was computed # words = 106214\n",
      "all accessor variety was computed # words = 106214\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2193\n",
      "all branching entropies was computed # words = 106274\n",
      "all accessor variety was computed # words = 106274\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2196\n",
      "all branching entropies was computed # words = 106298\n",
      "all accessor variety was computed # words = 106298\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 106311\n",
      "all accessor variety was computed # words = 106311\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 106333\n",
      "all accessor variety was computed # words = 106333\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2204\n",
      "all branching entropies was computed # words = 106355\n",
      "all accessor variety was computed # words = 106355\n",
      "training was done. used memory 1.207 Gb1.207 Gb\n",
      "all cohesion probabilities was computed. # words = 2206\n",
      "all branching entropies was computed # words = 106373\n",
      "all accessor variety was computed # words = 106373\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11043 from 1 sents. mem=1.207 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=26528, mem=1.143 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3251 words\n",
      "[Noun Extractor] checked compounds. discovered 679 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1747 -> 1618\n",
      "[Noun Extractor] postprocessing ignore_features : 1618 -> 1582\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1582 -> 1576\n",
      "[Noun Extractor] 1576 nouns (679 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.124 Gb                    \n",
      "[Noun Extractor] 63.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8526 from 1 sents. mem=1.122 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27760, mem=1.118 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2446 words\n",
      "[Noun Extractor] checked compounds. discovered 468 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1213 -> 1193\n",
      "[Noun Extractor] postprocessing ignore_features : 1193 -> 1170\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1170 -> 1165\n",
      "[Noun Extractor] 1165 nouns (468 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.116 Gb                    \n",
      "[Noun Extractor] 66.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 25683 from 1 sents. mem=1.115 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=65483, mem=1.115 Gb\n",
      "[Noun Extractor] batch prediction was completed for 12803 words\n",
      "[Noun Extractor] checked compounds. discovered 2935 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 6229 -> 5996\n",
      "[Noun Extractor] postprocessing ignore_features : 5996 -> 5931\n",
      "[Noun Extractor] postprocessing ignore_NJ : 5931 -> 5929\n",
      "[Noun Extractor] 5929 nouns (2935 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.115 Gb                    \n",
      "[Noun Extractor] 72.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7122 from 1 sents. mem=1.115 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14256, mem=1.106 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2049 words\n",
      "[Noun Extractor] checked compounds. discovered 245 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 864 -> 809\n",
      "[Noun Extractor] postprocessing ignore_features : 809 -> 781\n",
      "[Noun Extractor] postprocessing ignore_NJ : 781 -> 781\n",
      "[Noun Extractor] 781 nouns (245 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.096 Gb                    \n",
      "[Noun Extractor] 51.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 19574 from 1 sents. mem=1.096 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=52729, mem=1.095 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5669 words\n",
      "[Noun Extractor] checked compounds. discovered 1210 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2901 -> 2694\n",
      "[Noun Extractor] postprocessing ignore_features : 2694 -> 2632\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2632 -> 2624\n",
      "[Noun Extractor] 2624 nouns (1210 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.095 Gb                    \n",
      "[Noun Extractor] 62.81 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 25911 from 1 sents. mem=1.095 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=51956, mem=1.095 Gb\n",
      "[Noun Extractor] batch prediction was completed for 9025 words\n",
      "[Noun Extractor] checked compounds. discovered 2780 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4849 -> 4624\n",
      "[Noun Extractor] postprocessing ignore_features : 4624 -> 4574\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4574 -> 4568\n",
      "[Noun Extractor] 4568 nouns (2780 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 61.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7731 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=17637, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2266 words\n",
      "[Noun Extractor] checked compounds. discovered 239 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 897 -> 840\n",
      "[Noun Extractor] postprocessing ignore_features : 840 -> 805\n",
      "[Noun Extractor] postprocessing ignore_NJ : 805 -> 804\n",
      "[Noun Extractor] 804 nouns (239 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14399 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=40154, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4718 words\n",
      "[Noun Extractor] checked compounds. discovered 862 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2439 -> 2312\n",
      "[Noun Extractor] postprocessing ignore_features : 2312 -> 2261\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2261 -> 2257\n",
      "[Noun Extractor] 2257 nouns (862 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 65.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5786 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13089, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1671 words\n",
      "[Noun Extractor] checked compounds. discovered 99 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 563 -> 554\n",
      "[Noun Extractor] postprocessing ignore_features : 554 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (99 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 30256 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=73836, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 11032 words\n",
      "[Noun Extractor] checked compounds. discovered 2555 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 5447 -> 5060\n",
      "[Noun Extractor] postprocessing ignore_features : 5060 -> 4993\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4993 -> 4989\n",
      "[Noun Extractor] 4989 nouns (2555 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 55.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14154 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=30111, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4363 words\n",
      "[Noun Extractor] checked compounds. discovered 444 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1721 -> 1636\n",
      "[Noun Extractor] postprocessing ignore_features : 1636 -> 1592\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1592 -> 1591\n",
      "[Noun Extractor] 1591 nouns (444 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10660 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=24648, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3125 words\n",
      "[Noun Extractor] checked compounds. discovered 519 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1596 -> 1468\n",
      "[Noun Extractor] postprocessing ignore_features : 1468 -> 1435\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1435 -> 1434\n",
      "[Noun Extractor] 1434 nouns (519 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 59.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11433 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=24179, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3679 words\n",
      "[Noun Extractor] checked compounds. discovered 494 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1571 -> 1449\n",
      "[Noun Extractor] postprocessing ignore_features : 1449 -> 1406\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1406 -> 1397\n",
      "[Noun Extractor] 1397 nouns (494 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 58.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2772 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5284, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 865 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 305\n",
      "[Noun Extractor] postprocessing ignore_features : 305 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3183 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6956, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 361 -> 360\n",
      "[Noun Extractor] postprocessing ignore_features : 360 -> 346\n",
      "[Noun Extractor] postprocessing ignore_NJ : 346 -> 346\n",
      "[Noun Extractor] 346 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5060 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11223, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1450 words\n",
      "[Noun Extractor] checked compounds. discovered 149 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 597 -> 555\n",
      "[Noun Extractor] postprocessing ignore_features : 555 -> 535\n",
      "[Noun Extractor] postprocessing ignore_NJ : 535 -> 535\n",
      "[Noun Extractor] 535 nouns (149 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2348 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4341, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 674 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 221\n",
      "[Noun Extractor] 221 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5208 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10282, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1793 words\n",
      "[Noun Extractor] checked compounds. discovered 177 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 710 -> 678\n",
      "[Noun Extractor] postprocessing ignore_features : 678 -> 653\n",
      "[Noun Extractor] postprocessing ignore_NJ : 653 -> 650\n",
      "[Noun Extractor] 650 nouns (177 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11385 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=28985, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3654 words\n",
      "[Noun Extractor] checked compounds. discovered 814 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1882 -> 1625\n",
      "[Noun Extractor] postprocessing ignore_features : 1625 -> 1587\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1587 -> 1581\n",
      "[Noun Extractor] 1581 nouns (814 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 63.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9713 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21610, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2853 words\n",
      "[Noun Extractor] checked compounds. discovered 644 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1583 -> 1403\n",
      "[Noun Extractor] postprocessing ignore_features : 1403 -> 1367\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1367 -> 1360\n",
      "[Noun Extractor] 1360 nouns (644 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 61.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 12610 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=26804, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3854 words\n",
      "[Noun Extractor] checked compounds. discovered 520 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1677 -> 1518\n",
      "[Noun Extractor] postprocessing ignore_features : 1518 -> 1482\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1482 -> 1478\n",
      "[Noun Extractor] 1478 nouns (520 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 21192 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=51210, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6491 words\n",
      "[Noun Extractor] checked compounds. discovered 1655 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3399 -> 3189\n",
      "[Noun Extractor] postprocessing ignore_features : 3189 -> 3130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3130 -> 3115\n",
      "[Noun Extractor] 3115 nouns (1655 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3480 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6911, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1059 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 368 -> 363\n",
      "[Noun Extractor] postprocessing ignore_features : 363 -> 354\n",
      "[Noun Extractor] postprocessing ignore_NJ : 354 -> 353\n",
      "[Noun Extractor] 353 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 17440 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=41464, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4836 words\n",
      "[Noun Extractor] checked compounds. discovered 794 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2149 -> 2047\n",
      "[Noun Extractor] postprocessing ignore_features : 2047 -> 2011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2011 -> 2007\n",
      "[Noun Extractor] 2007 nouns (794 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 55.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9333 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=26819, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2069 words\n",
      "[Noun Extractor] checked compounds. discovered 109 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 686 -> 672\n",
      "[Noun Extractor] postprocessing ignore_features : 672 -> 653\n",
      "[Noun Extractor] postprocessing ignore_NJ : 653 -> 653\n",
      "[Noun Extractor] 653 nouns (109 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 20.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14048 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27950, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3977 words\n",
      "[Noun Extractor] checked compounds. discovered 633 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1847 -> 1787\n",
      "[Noun Extractor] postprocessing ignore_features : 1787 -> 1759\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1759 -> 1759\n",
      "[Noun Extractor] 1759 nouns (633 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9784 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20586, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3512 words\n",
      "[Noun Extractor] checked compounds. discovered 380 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1423 -> 1358\n",
      "[Noun Extractor] postprocessing ignore_features : 1358 -> 1322\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1322 -> 1321\n",
      "[Noun Extractor] 1321 nouns (380 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1738 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3094, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 517 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 162 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 155\n",
      "[Noun Extractor] postprocessing ignore_NJ : 155 -> 155\n",
      "[Noun Extractor] 155 nouns (19 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10006 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21637, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3389 words\n",
      "[Noun Extractor] checked compounds. discovered 594 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1626 -> 1442\n",
      "[Noun Extractor] postprocessing ignore_features : 1442 -> 1408\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1408 -> 1402\n",
      "[Noun Extractor] 1402 nouns (594 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 59.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2290 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 728 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 267\n",
      "[Noun Extractor] postprocessing ignore_features : 267 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 256\n",
      "[Noun Extractor] 256 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1905 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3962, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 208\n",
      "[Noun Extractor] postprocessing ignore_features : 208 -> 197\n",
      "[Noun Extractor] postprocessing ignore_NJ : 197 -> 197\n",
      "[Noun Extractor] 197 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 39243 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=89200, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 12459 words\n",
      "[Noun Extractor] checked compounds. discovered 2875 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 6009 -> 5529\n",
      "[Noun Extractor] postprocessing ignore_features : 5529 -> 5475\n",
      "[Noun Extractor] postprocessing ignore_NJ : 5475 -> 5465\n",
      "[Noun Extractor] 5465 nouns (2875 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3139 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5794, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 348 -> 345\n",
      "[Noun Extractor] postprocessing ignore_features : 345 -> 331\n",
      "[Noun Extractor] postprocessing ignore_NJ : 331 -> 331\n",
      "[Noun Extractor] 331 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2143 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4190, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 631 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 225 -> 225\n",
      "[Noun Extractor] postprocessing ignore_features : 225 -> 215\n",
      "[Noun Extractor] postprocessing ignore_NJ : 215 -> 214\n",
      "[Noun Extractor] 214 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5484 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11171, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1490 words\n",
      "[Noun Extractor] checked compounds. discovered 93 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 473 -> 469\n",
      "[Noun Extractor] postprocessing ignore_features : 469 -> 456\n",
      "[Noun Extractor] postprocessing ignore_NJ : 456 -> 456\n",
      "[Noun Extractor] 456 nouns (93 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4265, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 749 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 229 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 215\n",
      "[Noun Extractor] 215 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6074 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11840, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2163 words\n",
      "[Noun Extractor] checked compounds. discovered 308 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 857 -> 781\n",
      "[Noun Extractor] postprocessing ignore_features : 781 -> 751\n",
      "[Noun Extractor] postprocessing ignore_NJ : 751 -> 746\n",
      "[Noun Extractor] 746 nouns (308 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1828 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3543, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 541 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 179 -> 177\n",
      "[Noun Extractor] postprocessing ignore_features : 177 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2772 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5205, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 984 words\n",
      "[Noun Extractor] checked compounds. discovered 48 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 361 -> 358\n",
      "[Noun Extractor] postprocessing ignore_features : 358 -> 343\n",
      "[Noun Extractor] postprocessing ignore_NJ : 343 -> 343\n",
      "[Noun Extractor] 343 nouns (48 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2471 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4520, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 846 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 284 -> 265\n",
      "[Noun Extractor] postprocessing ignore_features : 265 -> 255\n",
      "[Noun Extractor] postprocessing ignore_NJ : 255 -> 254\n",
      "[Noun Extractor] 254 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2019 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3325, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 644 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 232 -> 232\n",
      "[Noun Extractor] postprocessing ignore_features : 232 -> 223\n",
      "[Noun Extractor] postprocessing ignore_NJ : 223 -> 222\n",
      "[Noun Extractor] 222 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9607 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=19798, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3069 words\n",
      "[Noun Extractor] checked compounds. discovered 341 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1190 -> 1140\n",
      "[Noun Extractor] postprocessing ignore_features : 1140 -> 1094\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1094 -> 1093\n",
      "[Noun Extractor] 1093 nouns (341 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 55.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2430 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4525, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 600 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 185\n",
      "[Noun Extractor] postprocessing ignore_features : 185 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7266 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14094, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2802 words\n",
      "[Noun Extractor] checked compounds. discovered 107 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 678 -> 666\n",
      "[Noun Extractor] postprocessing ignore_features : 666 -> 648\n",
      "[Noun Extractor] postprocessing ignore_NJ : 648 -> 647\n",
      "[Noun Extractor] 647 nouns (107 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7351 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15092, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2444 words\n",
      "[Noun Extractor] checked compounds. discovered 146 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 863 -> 854\n",
      "[Noun Extractor] postprocessing ignore_features : 854 -> 835\n",
      "[Noun Extractor] postprocessing ignore_NJ : 835 -> 834\n",
      "[Noun Extractor] 834 nouns (146 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2140 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3749, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 630 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 225\n",
      "[Noun Extractor] postprocessing ignore_features : 225 -> 217\n",
      "[Noun Extractor] postprocessing ignore_NJ : 217 -> 217\n",
      "[Noun Extractor] 217 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1452 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2594, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 150 -> 148\n",
      "[Noun Extractor] postprocessing ignore_features : 148 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1120 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1879, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 327 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1296 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2349, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 400 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 131\n",
      "[Noun Extractor] postprocessing ignore_NJ : 131 -> 131\n",
      "[Noun Extractor] 131 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8687 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21356, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2438 words\n",
      "[Noun Extractor] checked compounds. discovered 317 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1027 -> 1010\n",
      "[Noun Extractor] postprocessing ignore_features : 1010 -> 987\n",
      "[Noun Extractor] postprocessing ignore_NJ : 987 -> 982\n",
      "[Noun Extractor] 982 nouns (317 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 59.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1297 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2255, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.95 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4693 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8084, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1476 words\n",
      "[Noun Extractor] checked compounds. discovered 82 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 512 -> 506\n",
      "[Noun Extractor] postprocessing ignore_features : 506 -> 491\n",
      "[Noun Extractor] postprocessing ignore_NJ : 491 -> 490\n",
      "[Noun Extractor] 490 nouns (82 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1660 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2777, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 187\n",
      "[Noun Extractor] postprocessing ignore_features : 187 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2000 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3570, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 700 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 251 -> 249\n",
      "[Noun Extractor] postprocessing ignore_features : 249 -> 242\n",
      "[Noun Extractor] postprocessing ignore_NJ : 242 -> 242\n",
      "[Noun Extractor] 242 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7620 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16636, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2070 words\n",
      "[Noun Extractor] checked compounds. discovered 211 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 808 -> 803\n",
      "[Noun Extractor] postprocessing ignore_features : 803 -> 784\n",
      "[Noun Extractor] postprocessing ignore_NJ : 784 -> 784\n",
      "[Noun Extractor] 784 nouns (211 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4333 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8918, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1226 words\n",
      "[Noun Extractor] checked compounds. discovered 73 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 406 -> 404\n",
      "[Noun Extractor] postprocessing ignore_features : 404 -> 388\n",
      "[Noun Extractor] postprocessing ignore_NJ : 388 -> 387\n",
      "[Noun Extractor] 387 nouns (73 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1447 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2620, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 473 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 162 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 152\n",
      "[Noun Extractor] 152 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7367 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12735, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2849 words\n",
      "[Noun Extractor] checked compounds. discovered 205 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 859 -> 855\n",
      "[Noun Extractor] postprocessing ignore_features : 855 -> 838\n",
      "[Noun Extractor] postprocessing ignore_NJ : 838 -> 836\n",
      "[Noun Extractor] 836 nouns (205 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2298 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 658 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 242 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 229\n",
      "[Noun Extractor] 229 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1315 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2597, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 131 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5278 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10607, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1498 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 565 -> 550\n",
      "[Noun Extractor] postprocessing ignore_features : 550 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1107 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1904, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 380 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5135 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10066, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1654 words\n",
      "[Noun Extractor] checked compounds. discovered 180 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 682 -> 655\n",
      "[Noun Extractor] postprocessing ignore_features : 655 -> 634\n",
      "[Noun Extractor] postprocessing ignore_NJ : 634 -> 632\n",
      "[Noun Extractor] 632 nouns (180 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5937 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11362, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1844 words\n",
      "[Noun Extractor] checked compounds. discovered 98 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 561 -> 555\n",
      "[Noun Extractor] postprocessing ignore_features : 555 -> 533\n",
      "[Noun Extractor] postprocessing ignore_NJ : 533 -> 533\n",
      "[Noun Extractor] 533 nouns (98 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 29.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1080 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1940, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 115\n",
      "[Noun Extractor] postprocessing ignore_features : 115 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3709 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7036, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1215 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 430 -> 427\n",
      "[Noun Extractor] postprocessing ignore_features : 427 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7500 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=17097, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2286 words\n",
      "[Noun Extractor] checked compounds. discovered 279 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1013 -> 965\n",
      "[Noun Extractor] postprocessing ignore_features : 965 -> 935\n",
      "[Noun Extractor] postprocessing ignore_NJ : 935 -> 931\n",
      "[Noun Extractor] 931 nouns (279 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 56.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1434 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2415, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 430 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10143 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27309, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2588 words\n",
      "[Noun Extractor] checked compounds. discovered 248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1044 -> 1035\n",
      "[Noun Extractor] postprocessing ignore_features : 1035 -> 1011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1011 -> 1011\n",
      "[Noun Extractor] 1011 nouns (248 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8080 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16431, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2519 words\n",
      "[Noun Extractor] checked compounds. discovered 186 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 901 -> 882\n",
      "[Noun Extractor] postprocessing ignore_features : 882 -> 851\n",
      "[Noun Extractor] postprocessing ignore_NJ : 851 -> 850\n",
      "[Noun Extractor] 850 nouns (186 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1692 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2810, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 542 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 174 -> 174\n",
      "[Noun Extractor] postprocessing ignore_features : 174 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 169\n",
      "[Noun Extractor] 169 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 961 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1665, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 296 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3591 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6698, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1170 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 443 -> 436\n",
      "[Noun Extractor] postprocessing ignore_features : 436 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2231 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3803, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 747 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 248\n",
      "[Noun Extractor] postprocessing ignore_features : 248 -> 234\n",
      "[Noun Extractor] postprocessing ignore_NJ : 234 -> 234\n",
      "[Noun Extractor] 234 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2119, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 141\n",
      "[Noun Extractor] postprocessing ignore_NJ : 141 -> 141\n",
      "[Noun Extractor] 141 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6533 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10913, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2429 words\n",
      "[Noun Extractor] checked compounds. discovered 199 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 784 -> 775\n",
      "[Noun Extractor] postprocessing ignore_features : 775 -> 749\n",
      "[Noun Extractor] postprocessing ignore_NJ : 749 -> 747\n",
      "[Noun Extractor] 747 nouns (199 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4536 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8643, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1537 words\n",
      "[Noun Extractor] checked compounds. discovered 130 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 587 -> 570\n",
      "[Noun Extractor] postprocessing ignore_features : 570 -> 550\n",
      "[Noun Extractor] postprocessing ignore_NJ : 550 -> 550\n",
      "[Noun Extractor] 550 nouns (130 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 12159 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=28375, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3348 words\n",
      "[Noun Extractor] checked compounds. discovered 649 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1633 -> 1558\n",
      "[Noun Extractor] postprocessing ignore_features : 1558 -> 1524\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1524 -> 1521\n",
      "[Noun Extractor] 1521 nouns (649 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 830 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1392, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10128 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=25983, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2403 words\n",
      "[Noun Extractor] checked compounds. discovered 246 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 965 -> 956\n",
      "[Noun Extractor] postprocessing ignore_features : 956 -> 930\n",
      "[Noun Extractor] postprocessing ignore_NJ : 930 -> 930\n",
      "[Noun Extractor] 930 nouns (246 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1145 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1998, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 359 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9089 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=19072, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2802 words\n",
      "[Noun Extractor] checked compounds. discovered 390 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1283 -> 1194\n",
      "[Noun Extractor] postprocessing ignore_features : 1194 -> 1158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1158 -> 1156\n",
      "[Noun Extractor] 1156 nouns (390 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2628 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4504, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1465 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 291 -> 288\n",
      "[Noun Extractor] postprocessing ignore_features : 288 -> 269\n",
      "[Noun Extractor] postprocessing ignore_NJ : 269 -> 269\n",
      "[Noun Extractor] 269 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1471 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2492, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 464 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2101 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3806, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 652 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 235 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 208\n",
      "[Noun Extractor] postprocessing ignore_NJ : 208 -> 208\n",
      "[Noun Extractor] 208 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2018, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 385 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.51 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1603, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2238 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3922, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 783 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 240 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2004 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3467, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 689 words\n",
      "[Noun Extractor] checked compounds. discovered 48 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 236 -> 235\n",
      "[Noun Extractor] postprocessing ignore_features : 235 -> 225\n",
      "[Noun Extractor] postprocessing ignore_NJ : 225 -> 225\n",
      "[Noun Extractor] 225 nouns (48 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7475 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15727, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2603 words\n",
      "[Noun Extractor] checked compounds. discovered 248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1094 -> 1036\n",
      "[Noun Extractor] postprocessing ignore_features : 1036 -> 1007\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1007 -> 1005\n",
      "[Noun Extractor] 1005 nouns (248 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 711 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1235, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4912 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11157, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1521 words\n",
      "[Noun Extractor] checked compounds. discovered 117 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 587 -> 569\n",
      "[Noun Extractor] postprocessing ignore_features : 569 -> 543\n",
      "[Noun Extractor] postprocessing ignore_NJ : 543 -> 542\n",
      "[Noun Extractor] 542 nouns (117 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1737 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3279, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 504 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 165 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 151\n",
      "[Noun Extractor] 151 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4103 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7059, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1528 words\n",
      "[Noun Extractor] checked compounds. discovered 76 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 451 -> 438\n",
      "[Noun Extractor] postprocessing ignore_features : 438 -> 420\n",
      "[Noun Extractor] postprocessing ignore_NJ : 420 -> 418\n",
      "[Noun Extractor] 418 nouns (76 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6152 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11626, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2132 words\n",
      "[Noun Extractor] checked compounds. discovered 151 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 734 -> 700\n",
      "[Noun Extractor] postprocessing ignore_features : 700 -> 674\n",
      "[Noun Extractor] postprocessing ignore_NJ : 674 -> 672\n",
      "[Noun Extractor] 672 nouns (151 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2264 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3898, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 748 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 230 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 222\n",
      "[Noun Extractor] 222 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1524 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2434, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 540 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 148 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2419 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4236, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 800 words\n",
      "[Noun Extractor] checked compounds. discovered 69 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 281 -> 278\n",
      "[Noun Extractor] postprocessing ignore_features : 278 -> 274\n",
      "[Noun Extractor] postprocessing ignore_NJ : 274 -> 274\n",
      "[Noun Extractor] 274 nouns (69 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2065, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1799 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3550, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 558 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 190\n",
      "[Noun Extractor] postprocessing ignore_features : 190 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3060 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7192, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 840 words\n",
      "[Noun Extractor] checked compounds. discovered 53 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 330 -> 328\n",
      "[Noun Extractor] postprocessing ignore_features : 328 -> 316\n",
      "[Noun Extractor] postprocessing ignore_NJ : 316 -> 316\n",
      "[Noun Extractor] 316 nouns (53 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 56.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 745 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1218, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 217 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4747 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8668, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1541 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 480 -> 468\n",
      "[Noun Extractor] postprocessing ignore_features : 468 -> 452\n",
      "[Noun Extractor] postprocessing ignore_NJ : 452 -> 452\n",
      "[Noun Extractor] 452 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 30.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2296 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4018, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 705 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 209\n",
      "[Noun Extractor] postprocessing ignore_features : 209 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3635 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7104, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1061 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 340 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 309\n",
      "[Noun Extractor] postprocessing ignore_NJ : 309 -> 309\n",
      "[Noun Extractor] 309 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6425 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11695, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2117 words\n",
      "[Noun Extractor] checked compounds. discovered 192 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 769 -> 723\n",
      "[Noun Extractor] postprocessing ignore_features : 723 -> 704\n",
      "[Noun Extractor] postprocessing ignore_NJ : 704 -> 704\n",
      "[Noun Extractor] 704 nouns (192 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3401 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6749, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1048 words\n",
      "[Noun Extractor] checked compounds. discovered 96 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 435 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 417\n",
      "[Noun Extractor] postprocessing ignore_NJ : 417 -> 417\n",
      "[Noun Extractor] 417 nouns (96 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 58.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5683 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11871, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1733 words\n",
      "[Noun Extractor] checked compounds. discovered 239 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 774 -> 756\n",
      "[Noun Extractor] postprocessing ignore_features : 756 -> 737\n",
      "[Noun Extractor] postprocessing ignore_NJ : 737 -> 735\n",
      "[Noun Extractor] 735 nouns (239 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 56.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1437 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2403, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 537 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5727 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12231, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1820 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 631\n",
      "[Noun Extractor] postprocessing ignore_features : 631 -> 618\n",
      "[Noun Extractor] postprocessing ignore_NJ : 618 -> 618\n",
      "[Noun Extractor] 618 nouns (158 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2920 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5984, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 910 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 327 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 314\n",
      "[Noun Extractor] postprocessing ignore_NJ : 314 -> 312\n",
      "[Noun Extractor] 312 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1247 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2228, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2964, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 597 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1964 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3771, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 633 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 233 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 213\n",
      "[Noun Extractor] postprocessing ignore_NJ : 213 -> 213\n",
      "[Noun Extractor] 213 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3506 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6208, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1202 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 397 -> 369\n",
      "[Noun Extractor] postprocessing ignore_features : 369 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4332, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 719 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1074 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1921, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4175 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9435, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1355 words\n",
      "[Noun Extractor] checked compounds. discovered 76 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 379 -> 371\n",
      "[Noun Extractor] postprocessing ignore_features : 371 -> 358\n",
      "[Noun Extractor] postprocessing ignore_NJ : 358 -> 357\n",
      "[Noun Extractor] 357 nouns (76 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1685 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3016, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 528 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 156 -> 156\n",
      "[Noun Extractor] postprocessing ignore_features : 156 -> 146\n",
      "[Noun Extractor] postprocessing ignore_NJ : 146 -> 146\n",
      "[Noun Extractor] 146 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3182 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5937, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 964 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 328 -> 322\n",
      "[Noun Extractor] postprocessing ignore_features : 322 -> 306\n",
      "[Noun Extractor] postprocessing ignore_NJ : 306 -> 305\n",
      "[Noun Extractor] 305 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 17359 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=40273, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5134 words\n",
      "[Noun Extractor] checked compounds. discovered 648 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1939 -> 1930\n",
      "[Noun Extractor] postprocessing ignore_features : 1930 -> 1903\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1903 -> 1901\n",
      "[Noun Extractor] 1901 nouns (648 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.61 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1435 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2444, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1529, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1093 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1898, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1439 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2318, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 150\n",
      "[Noun Extractor] postprocessing ignore_features : 150 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 147\n",
      "[Noun Extractor] 147 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 714 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1285, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 204 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4042 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8425, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1248 words\n",
      "[Noun Extractor] checked compounds. discovered 116 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 441 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 410\n",
      "[Noun Extractor] postprocessing ignore_NJ : 410 -> 409\n",
      "[Noun Extractor] 409 nouns (116 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 55.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 830 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4692 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9313, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1444 words\n",
      "[Noun Extractor] checked compounds. discovered 114 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 539 -> 514\n",
      "[Noun Extractor] postprocessing ignore_features : 514 -> 497\n",
      "[Noun Extractor] postprocessing ignore_NJ : 497 -> 497\n",
      "[Noun Extractor] 497 nouns (114 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1565, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 295 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1238 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2006, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 167 -> 155\n",
      "[Noun Extractor] postprocessing ignore_features : 155 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 147\n",
      "[Noun Extractor] 147 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 960 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1592, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1184 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2018, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 399 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 130 -> 130\n",
      "[Noun Extractor] postprocessing ignore_features : 130 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (9 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7054 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16438, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2954 words\n",
      "[Noun Extractor] checked compounds. discovered 191 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1006 -> 981\n",
      "[Noun Extractor] postprocessing ignore_features : 981 -> 949\n",
      "[Noun Extractor] postprocessing ignore_NJ : 949 -> 946\n",
      "[Noun Extractor] 946 nouns (191 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 915 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1521, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3266 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6121, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1161 words\n",
      "[Noun Extractor] checked compounds. discovered 53 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 369 -> 363\n",
      "[Noun Extractor] postprocessing ignore_features : 363 -> 355\n",
      "[Noun Extractor] postprocessing ignore_NJ : 355 -> 354\n",
      "[Noun Extractor] 354 nouns (53 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 627 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1035, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2110 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3759, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 698 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 218 -> 217\n",
      "[Noun Extractor] postprocessing ignore_features : 217 -> 208\n",
      "[Noun Extractor] postprocessing ignore_NJ : 208 -> 208\n",
      "[Noun Extractor] 208 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1333 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2357, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 453 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 114\n",
      "[Noun Extractor] postprocessing ignore_NJ : 114 -> 114\n",
      "[Noun Extractor] 114 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1288 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2152, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 413 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 108\n",
      "[Noun Extractor] postprocessing ignore_NJ : 108 -> 108\n",
      "[Noun Extractor] 108 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 889 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1649, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5555 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9904, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2090 words\n",
      "[Noun Extractor] checked compounds. discovered 108 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 652 -> 624\n",
      "[Noun Extractor] postprocessing ignore_features : 624 -> 605\n",
      "[Noun Extractor] postprocessing ignore_NJ : 605 -> 605\n",
      "[Noun Extractor] 605 nouns (108 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1878 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3319, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1025 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1731, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6882 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13766, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1976 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 213 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 748 -> 731\n",
      "[Noun Extractor] postprocessing ignore_features : 731 -> 709\n",
      "[Noun Extractor] postprocessing ignore_NJ : 709 -> 708\n",
      "[Noun Extractor] 708 nouns (213 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4387 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8380, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1348 words\n",
      "[Noun Extractor] checked compounds. discovered 113 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 420 -> 403\n",
      "[Noun Extractor] postprocessing ignore_features : 403 -> 380\n",
      "[Noun Extractor] postprocessing ignore_NJ : 380 -> 377\n",
      "[Noun Extractor] 377 nouns (113 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9858 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20851, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3335 words\n",
      "[Noun Extractor] checked compounds. discovered 567 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1530 -> 1345\n",
      "[Noun Extractor] postprocessing ignore_features : 1345 -> 1311\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1311 -> 1306\n",
      "[Noun Extractor] 1306 nouns (567 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 58.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2250 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3868, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 681 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 223 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 216\n",
      "[Noun Extractor] 216 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1802, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 77\n",
      "[Noun Extractor] 77 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1342 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2310, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2715, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 861 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1740, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2911 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4942, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1289 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 302 -> 301\n",
      "[Noun Extractor] postprocessing ignore_features : 301 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1602, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1362 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2450, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 117\n",
      "[Noun Extractor] 117 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6695 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12515, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2134 words\n",
      "[Noun Extractor] checked compounds. discovered 260 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 779 -> 691\n",
      "[Noun Extractor] postprocessing ignore_features : 691 -> 666\n",
      "[Noun Extractor] postprocessing ignore_NJ : 666 -> 666\n",
      "[Noun Extractor] 666 nouns (260 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.32 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2058 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3747, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 695 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 215\n",
      "[Noun Extractor] postprocessing ignore_NJ : 215 -> 215\n",
      "[Noun Extractor] 215 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 684 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1136, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7378 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13715, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2650 words\n",
      "[Noun Extractor] checked compounds. discovered 150 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 915 -> 883\n",
      "[Noun Extractor] postprocessing ignore_features : 883 -> 849\n",
      "[Noun Extractor] postprocessing ignore_NJ : 849 -> 847\n",
      "[Noun Extractor] 847 nouns (150 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1143 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2132, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 324 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1856 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3309, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 596 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 233 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 218\n",
      "[Noun Extractor] postprocessing ignore_NJ : 218 -> 216\n",
      "[Noun Extractor] 216 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1221, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 73\n",
      "[Noun Extractor] 73 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 817 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1408, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2071 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3795, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 607 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 217 -> 217\n",
      "[Noun Extractor] postprocessing ignore_features : 217 -> 213\n",
      "[Noun Extractor] postprocessing ignore_NJ : 213 -> 212\n",
      "[Noun Extractor] 212 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1312 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2387, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2648, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1564 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2776, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 169\n",
      "[Noun Extractor] postprocessing ignore_features : 169 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 160\n",
      "[Noun Extractor] 160 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3449, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 655 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (10 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 22.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 978 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1658, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 295 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4415 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8218, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1286 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 370 -> 366\n",
      "[Noun Extractor] postprocessing ignore_features : 366 -> 356\n",
      "[Noun Extractor] postprocessing ignore_NJ : 356 -> 356\n",
      "[Noun Extractor] 356 nouns (32 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 23.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3687 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6841, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1105 words\n",
      "[Noun Extractor] checked compounds. discovered 77 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 399 -> 393\n",
      "[Noun Extractor] postprocessing ignore_features : 393 -> 380\n",
      "[Noun Extractor] postprocessing ignore_NJ : 380 -> 380\n",
      "[Noun Extractor] 380 nouns (77 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3856 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6407, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1244 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 352 -> 349\n",
      "[Noun Extractor] postprocessing ignore_features : 349 -> 335\n",
      "[Noun Extractor] postprocessing ignore_NJ : 335 -> 335\n",
      "[Noun Extractor] 335 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1927 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3650, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 561 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 181 -> 179\n",
      "[Noun Extractor] postprocessing ignore_features : 179 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1162, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=970, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6659 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11505, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2168 words\n",
      "[Noun Extractor] checked compounds. discovered 198 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 707 -> 686\n",
      "[Noun Extractor] postprocessing ignore_features : 686 -> 663\n",
      "[Noun Extractor] postprocessing ignore_NJ : 663 -> 662\n",
      "[Noun Extractor] 662 nouns (198 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1388 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2888, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 948 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1665, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 33.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5453 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10433, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1655 words\n",
      "[Noun Extractor] checked compounds. discovered 135 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 618\n",
      "[Noun Extractor] postprocessing ignore_features : 618 -> 591\n",
      "[Noun Extractor] postprocessing ignore_NJ : 591 -> 591\n",
      "[Noun Extractor] 591 nouns (135 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1043 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1937, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (17 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5038 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10506, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1403 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 485 -> 482\n",
      "[Noun Extractor] postprocessing ignore_features : 482 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 470\n",
      "[Noun Extractor] 470 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2316 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3770, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 750 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 231 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 214\n",
      "[Noun Extractor] postprocessing ignore_NJ : 214 -> 214\n",
      "[Noun Extractor] 214 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1399 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2424, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 512 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1729 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3008, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 531 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 168 -> 165\n",
      "[Noun Extractor] postprocessing ignore_features : 165 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 737 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1236, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 65\n",
      "[Noun Extractor] 65 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14814 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=35354, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4414 words\n",
      "[Noun Extractor] checked compounds. discovered 803 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2243 -> 2008\n",
      "[Noun Extractor] postprocessing ignore_features : 2008 -> 1967\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1967 -> 1964\n",
      "[Noun Extractor] 1964 nouns (803 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 59.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 658 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1026, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2305 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4115, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 662 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 208 -> 207\n",
      "[Noun Extractor] postprocessing ignore_features : 207 -> 200\n",
      "[Noun Extractor] postprocessing ignore_NJ : 200 -> 200\n",
      "[Noun Extractor] 200 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 407 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=815, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1379 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2841, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=993, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1357 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2475, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 442 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1014 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1661, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 921 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1671, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 803 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1316, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 900 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1378, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1534 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2756, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1047 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1877, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 580 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1106, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 840 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1198, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1139 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1942, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 462 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1365, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 82 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1048 from 1 sents. mem=1.093 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1771, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1157 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1788, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 356 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 34.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1174 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1997, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 97\n",
      "[Noun Extractor] 97 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2581 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4731, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 649 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 174\n",
      "[Noun Extractor] 174 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1865 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3259, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 583 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 144\n",
      "[Noun Extractor] postprocessing ignore_NJ : 144 -> 144\n",
      "[Noun Extractor] 144 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 659 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1024, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 775 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1285, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1507 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2607, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 569 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 131\n",
      "[Noun Extractor] postprocessing ignore_NJ : 131 -> 131\n",
      "[Noun Extractor] 131 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1306 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2389, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 402 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1143, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1367 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2681, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 154\n",
      "[Noun Extractor] postprocessing ignore_features : 154 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2281, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 427 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 134\n",
      "[Noun Extractor] 134 nouns (14 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 952 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1457, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 33.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1609 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2903, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 449 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1216 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1977, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1687, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2508 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4667, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 767 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 253 -> 248\n",
      "[Noun Extractor] postprocessing ignore_features : 248 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4988 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9140, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1496 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 412 -> 403\n",
      "[Noun Extractor] postprocessing ignore_features : 403 -> 384\n",
      "[Noun Extractor] postprocessing ignore_NJ : 384 -> 384\n",
      "[Noun Extractor] 384 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 23.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1136 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1838, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 355 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1824, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 387 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1508, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 121 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6448 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12676, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2077 words\n",
      "[Noun Extractor] checked compounds. discovered 81 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 637 -> 628\n",
      "[Noun Extractor] postprocessing ignore_features : 628 -> 603\n",
      "[Noun Extractor] postprocessing ignore_NJ : 603 -> 602\n",
      "[Noun Extractor] 602 nouns (81 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1121, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1095 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1663, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 373 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1976 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3610, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 601 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 180 -> 177\n",
      "[Noun Extractor] postprocessing ignore_features : 177 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 165\n",
      "[Noun Extractor] 165 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 540 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=897, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 414 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=706, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1158 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2085, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 348 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 110\n",
      "[Noun Extractor] 110 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1282 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2048, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 128 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 747 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1168, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1383, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1340 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2322, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1292, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2724 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5565, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 867 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 299 -> 296\n",
      "[Noun Extractor] postprocessing ignore_features : 296 -> 279\n",
      "[Noun Extractor] postprocessing ignore_NJ : 279 -> 279\n",
      "[Noun Extractor] 279 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1853 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3651, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 631 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 162\n",
      "[Noun Extractor] 162 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5640 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10126, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2107 words\n",
      "[Noun Extractor] checked compounds. discovered 111 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 553 -> 545\n",
      "[Noun Extractor] postprocessing ignore_features : 545 -> 525\n",
      "[Noun Extractor] postprocessing ignore_NJ : 525 -> 525\n",
      "[Noun Extractor] 525 nouns (111 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 998 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1518, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 736 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1384, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3347 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6020, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1037 words\n",
      "[Noun Extractor] checked compounds. discovered 81 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 374 -> 371\n",
      "[Noun Extractor] postprocessing ignore_features : 371 -> 359\n",
      "[Noun Extractor] postprocessing ignore_NJ : 359 -> 359\n",
      "[Noun Extractor] 359 nouns (81 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3290 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6137, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1236 words\n",
      "[Noun Extractor] checked compounds. discovered 75 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 413 -> 381\n",
      "[Noun Extractor] postprocessing ignore_features : 381 -> 363\n",
      "[Noun Extractor] postprocessing ignore_NJ : 363 -> 363\n",
      "[Noun Extractor] 363 nouns (75 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 832 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1477, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2599 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4578, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 802 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 303 -> 296\n",
      "[Noun Extractor] postprocessing ignore_features : 296 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 282\n",
      "[Noun Extractor] 282 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2605 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4864, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 810 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 242\n",
      "[Noun Extractor] postprocessing ignore_features : 242 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3641 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6373, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1289 words\n",
      "[Noun Extractor] checked compounds. discovered 80 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 430 -> 422\n",
      "[Noun Extractor] postprocessing ignore_features : 422 -> 406\n",
      "[Noun Extractor] postprocessing ignore_NJ : 406 -> 406\n",
      "[Noun Extractor] 406 nouns (80 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4044 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8274, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1367 words\n",
      "[Noun Extractor] checked compounds. discovered 103 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 515 -> 509\n",
      "[Noun Extractor] postprocessing ignore_features : 509 -> 497\n",
      "[Noun Extractor] postprocessing ignore_NJ : 497 -> 496\n",
      "[Noun Extractor] 496 nouns (103 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 54.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3236 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6326, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1011 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 371 -> 363\n",
      "[Noun Extractor] postprocessing ignore_features : 363 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 351\n",
      "[Noun Extractor] 351 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1033 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1611, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 316 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 94\n",
      "[Noun Extractor] postprocessing ignore_NJ : 94 -> 94\n",
      "[Noun Extractor] 94 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1316 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2414, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2830 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5334, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 890 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 271 -> 269\n",
      "[Noun Extractor] postprocessing ignore_features : 269 -> 253\n",
      "[Noun Extractor] postprocessing ignore_NJ : 253 -> 253\n",
      "[Noun Extractor] 253 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1679 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2674, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 552 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5553 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13472, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1958 words\n",
      "[Noun Extractor] checked compounds. discovered 155 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 677 -> 660\n",
      "[Noun Extractor] postprocessing ignore_features : 660 -> 642\n",
      "[Noun Extractor] postprocessing ignore_NJ : 642 -> 642\n",
      "[Noun Extractor] 642 nouns (155 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 61.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1092 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2000, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 76\n",
      "[Noun Extractor] 76 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 476 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=814, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 141 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2652 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5350, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 771 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 279 -> 276\n",
      "[Noun Extractor] postprocessing ignore_features : 276 -> 268\n",
      "[Noun Extractor] postprocessing ignore_NJ : 268 -> 268\n",
      "[Noun Extractor] 268 nouns (34 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 789 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1214, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1031 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1708, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 305 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 34.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1411 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2378, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1277 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2145, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 408 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1083, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=949, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1260 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1996, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 410 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 104\n",
      "[Noun Extractor] 104 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1133, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 639 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1076, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2619 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5074, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 759 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 236 -> 234\n",
      "[Noun Extractor] postprocessing ignore_features : 234 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1824 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3255, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 551 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 169 -> 165\n",
      "[Noun Extractor] postprocessing ignore_features : 165 -> 155\n",
      "[Noun Extractor] postprocessing ignore_NJ : 155 -> 155\n",
      "[Noun Extractor] 155 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5089 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8628, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1772 words\n",
      "[Noun Extractor] checked compounds. discovered 122 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 537 -> 503\n",
      "[Noun Extractor] postprocessing ignore_features : 503 -> 469\n",
      "[Noun Extractor] postprocessing ignore_NJ : 469 -> 469\n",
      "[Noun Extractor] 469 nouns (122 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 850 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1503, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1164 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1869, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 120 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 828 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1368, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1393, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4046 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7751, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1308 words\n",
      "[Noun Extractor] checked compounds. discovered 91 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 483 -> 468\n",
      "[Noun Extractor] postprocessing ignore_features : 468 -> 454\n",
      "[Noun Extractor] postprocessing ignore_NJ : 454 -> 453\n",
      "[Noun Extractor] 453 nouns (91 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1875 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3130, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 188 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (18 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10041 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=19218, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3310 words\n",
      "[Noun Extractor] checked compounds. discovered 275 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1148 -> 1099\n",
      "[Noun Extractor] postprocessing ignore_features : 1099 -> 1064\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1064 -> 1064\n",
      "[Noun Extractor] 1064 nouns (275 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 895 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1409, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 869 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1322, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1838 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3287, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 542 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 181 -> 181\n",
      "[Noun Extractor] postprocessing ignore_features : 181 -> 173\n",
      "[Noun Extractor] postprocessing ignore_NJ : 173 -> 172\n",
      "[Noun Extractor] 172 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1260 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2216, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 398 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 147 -> 147\n",
      "[Noun Extractor] postprocessing ignore_features : 147 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2116 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3757, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 702 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 219 -> 219\n",
      "[Noun Extractor] postprocessing ignore_features : 219 -> 212\n",
      "[Noun Extractor] postprocessing ignore_NJ : 212 -> 211\n",
      "[Noun Extractor] 211 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2822 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5142, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1042 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 332 -> 311\n",
      "[Noun Extractor] postprocessing ignore_features : 311 -> 293\n",
      "[Noun Extractor] postprocessing ignore_NJ : 293 -> 292\n",
      "[Noun Extractor] 292 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 891 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1467, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1033 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1537, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4841 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9169, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1422 words\n",
      "[Noun Extractor] checked compounds. discovered 103 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 571 -> 565\n",
      "[Noun Extractor] postprocessing ignore_features : 565 -> 547\n",
      "[Noun Extractor] postprocessing ignore_NJ : 547 -> 547\n",
      "[Noun Extractor] 547 nouns (103 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 719 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1029, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2730, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 341 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (13 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1999 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3698, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 709 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 203 -> 202\n",
      "[Noun Extractor] postprocessing ignore_features : 202 -> 198\n",
      "[Noun Extractor] postprocessing ignore_NJ : 198 -> 198\n",
      "[Noun Extractor] 198 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1724 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2777, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 592 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 186 -> 177\n",
      "[Noun Extractor] postprocessing ignore_features : 177 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 171\n",
      "[Noun Extractor] 171 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3471 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7088, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1125 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 317 -> 313\n",
      "[Noun Extractor] postprocessing ignore_features : 313 -> 303\n",
      "[Noun Extractor] postprocessing ignore_NJ : 303 -> 303\n",
      "[Noun Extractor] 303 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6145 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10316, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2141 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 634\n",
      "[Noun Extractor] postprocessing ignore_features : 634 -> 607\n",
      "[Noun Extractor] postprocessing ignore_NJ : 607 -> 607\n",
      "[Noun Extractor] 607 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 30.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4320 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7832, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1271 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 443 -> 439\n",
      "[Noun Extractor] postprocessing ignore_features : 439 -> 419\n",
      "[Noun Extractor] postprocessing ignore_NJ : 419 -> 419\n",
      "[Noun Extractor] 419 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1287 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2327, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2435 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4039, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 789 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 228\n",
      "[Noun Extractor] postprocessing ignore_NJ : 228 -> 227\n",
      "[Noun Extractor] 227 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 815 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1499, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1886 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3428, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 587 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 211 -> 207\n",
      "[Noun Extractor] postprocessing ignore_features : 207 -> 193\n",
      "[Noun Extractor] postprocessing ignore_NJ : 193 -> 193\n",
      "[Noun Extractor] 193 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9677 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21755, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3295 words\n",
      "[Noun Extractor] checked compounds. discovered 314 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1270 -> 1212\n",
      "[Noun Extractor] postprocessing ignore_features : 1212 -> 1179\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1179 -> 1176\n",
      "[Noun Extractor] 1176 nouns (314 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 57.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4991 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8667, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1572 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 417 -> 412\n",
      "[Noun Extractor] postprocessing ignore_features : 412 -> 394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 394 -> 394\n",
      "[Noun Extractor] 394 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 22.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1608 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2632, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 448 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3826 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7434, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1447 words\n",
      "[Noun Extractor] checked compounds. discovered 84 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 532 -> 531\n",
      "[Noun Extractor] postprocessing ignore_features : 531 -> 513\n",
      "[Noun Extractor] postprocessing ignore_NJ : 513 -> 513\n",
      "[Noun Extractor] 513 nouns (84 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 55.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1200 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2123, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 934 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1538, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 304 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 404 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=733, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 100 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 33.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 681 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1161, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2121 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3659, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 687 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 229 -> 224\n",
      "[Noun Extractor] postprocessing ignore_features : 224 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2729 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4600, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 917 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 284 -> 278\n",
      "[Noun Extractor] postprocessing ignore_features : 278 -> 268\n",
      "[Noun Extractor] postprocessing ignore_NJ : 268 -> 268\n",
      "[Noun Extractor] 268 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1073, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 210 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1098 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1746, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 371 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1162, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1796, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=1.093 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=941, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 853 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1327, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 273 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1569 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2518, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 488 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2739 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5114, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 849 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 263\n",
      "[Noun Extractor] postprocessing ignore_features : 263 -> 252\n",
      "[Noun Extractor] postprocessing ignore_NJ : 252 -> 252\n",
      "[Noun Extractor] 252 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1153 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1845, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 382 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3085 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5824, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1026 words\n",
      "[Noun Extractor] checked compounds. discovered 73 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 368 -> 368\n",
      "[Noun Extractor] postprocessing ignore_features : 368 -> 361\n",
      "[Noun Extractor] postprocessing ignore_NJ : 361 -> 360\n",
      "[Noun Extractor] 360 nouns (73 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2845 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4940, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 864 words\n",
      "[Noun Extractor] checked compounds. discovered 51 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 295 -> 293\n",
      "[Noun Extractor] postprocessing ignore_features : 293 -> 284\n",
      "[Noun Extractor] postprocessing ignore_NJ : 284 -> 284\n",
      "[Noun Extractor] 284 nouns (51 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4154 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6806, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1229 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 372 -> 372\n",
      "[Noun Extractor] postprocessing ignore_features : 372 -> 359\n",
      "[Noun Extractor] postprocessing ignore_NJ : 359 -> 358\n",
      "[Noun Extractor] 358 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1870 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3267, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 474 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1243, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4508 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9381, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1363 words\n",
      "[Noun Extractor] checked compounds. discovered 221 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 669 -> 653\n",
      "[Noun Extractor] postprocessing ignore_features : 653 -> 634\n",
      "[Noun Extractor] postprocessing ignore_NJ : 634 -> 633\n",
      "[Noun Extractor] 633 nouns (221 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 59.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 610 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=953, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 153 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 36.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4321 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8200, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1695 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 454 -> 454\n",
      "[Noun Extractor] postprocessing ignore_features : 454 -> 440\n",
      "[Noun Extractor] postprocessing ignore_NJ : 440 -> 440\n",
      "[Noun Extractor] 440 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1987 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3336, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 762 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 184\n",
      "[Noun Extractor] postprocessing ignore_features : 184 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1453 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2367, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 515 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2616 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4803, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 776 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 238 -> 236\n",
      "[Noun Extractor] postprocessing ignore_features : 236 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1460, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7420 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12810, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2466 words\n",
      "[Noun Extractor] checked compounds. discovered 138 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 792 -> 747\n",
      "[Noun Extractor] postprocessing ignore_features : 747 -> 722\n",
      "[Noun Extractor] postprocessing ignore_NJ : 722 -> 722\n",
      "[Noun Extractor] 722 nouns (138 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 35.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2270, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 940 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 347 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 33.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2410, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 716 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1257 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2300, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 111\n",
      "[Noun Extractor] 111 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 66047 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=200641, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 19826 words\n",
      "[Noun Extractor] checked compounds. discovered 9060 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 13711 -> 12066\n",
      "[Noun Extractor] postprocessing ignore_features : 12066 -> 11956\n",
      "[Noun Extractor] postprocessing ignore_NJ : 11956 -> 11885\n",
      "[Noun Extractor] 11885 nouns (9060 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 68.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1030 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1655, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 326 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 720 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1267, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 34.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2004 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3696, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 591 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 221 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 203\n",
      "[Noun Extractor] postprocessing ignore_NJ : 203 -> 203\n",
      "[Noun Extractor] 203 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1097, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 569 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1030, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1951 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3177, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 630 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 171\n",
      "[Noun Extractor] 171 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7179 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14143, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2397 words\n",
      "[Noun Extractor] checked compounds. discovered 397 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1056 -> 1031\n",
      "[Noun Extractor] postprocessing ignore_features : 1031 -> 1008\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1008 -> 1000\n",
      "[Noun Extractor] 1000 nouns (397 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 58.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4323 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8442, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1552 words\n",
      "[Noun Extractor] checked compounds. discovered 86 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 499 -> 486\n",
      "[Noun Extractor] postprocessing ignore_features : 486 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 469\n",
      "[Noun Extractor] 469 nouns (86 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1531 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2596, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 495 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 161\n",
      "[Noun Extractor] postprocessing ignore_NJ : 161 -> 161\n",
      "[Noun Extractor] 161 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1388 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2230, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 457 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 137 -> 137\n",
      "[Noun Extractor] postprocessing ignore_features : 137 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1986 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3945, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 645 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 230 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 220\n",
      "[Noun Extractor] 220 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EojeolCounter] n eojeol = 812 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1278, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 34.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3350 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5381, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1034 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 315 -> 315\n",
      "[Noun Extractor] postprocessing ignore_features : 315 -> 300\n",
      "[Noun Extractor] postprocessing ignore_NJ : 300 -> 300\n",
      "[Noun Extractor] 300 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2112, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 438 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 16366 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=37636, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5169 words\n",
      "[Noun Extractor] checked compounds. discovered 1401 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2799 -> 2443\n",
      "[Noun Extractor] postprocessing ignore_features : 2443 -> 2394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2394 -> 2391\n",
      "[Noun Extractor] 2391 nouns (1401 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 56.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 560 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=836, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1730 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3065, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 542 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 172 -> 168\n",
      "[Noun Extractor] postprocessing ignore_features : 168 -> 158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 158 -> 157\n",
      "[Noun Extractor] 157 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2286 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3713, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 717 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 218 -> 218\n",
      "[Noun Extractor] postprocessing ignore_features : 218 -> 214\n",
      "[Noun Extractor] postprocessing ignore_NJ : 214 -> 214\n",
      "[Noun Extractor] 214 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1523, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1658 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2790, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 489 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 133\n",
      "[Noun Extractor] 133 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1515 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2870, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 481 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 150\n",
      "[Noun Extractor] postprocessing ignore_NJ : 150 -> 149\n",
      "[Noun Extractor] 149 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1855 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4192, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 568 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 203 -> 200\n",
      "[Noun Extractor] postprocessing ignore_features : 200 -> 189\n",
      "[Noun Extractor] postprocessing ignore_NJ : 189 -> 188\n",
      "[Noun Extractor] 188 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5238 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12604, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1419 words\n",
      "[Noun Extractor] checked compounds. discovered 132 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 575 -> 563\n",
      "[Noun Extractor] postprocessing ignore_features : 563 -> 542\n",
      "[Noun Extractor] postprocessing ignore_NJ : 542 -> 542\n",
      "[Noun Extractor] 542 nouns (132 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1083 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1695, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1108 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2049, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5034 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8894, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1605 words\n",
      "[Noun Extractor] checked compounds. discovered 135 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 623 -> 606\n",
      "[Noun Extractor] postprocessing ignore_features : 606 -> 588\n",
      "[Noun Extractor] postprocessing ignore_NJ : 588 -> 588\n",
      "[Noun Extractor] 588 nouns (135 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2924 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5547, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 982 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 270 -> 263\n",
      "[Noun Extractor] postprocessing ignore_features : 263 -> 249\n",
      "[Noun Extractor] postprocessing ignore_NJ : 249 -> 249\n",
      "[Noun Extractor] 249 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=840, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2523 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4566, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 770 words\n",
      "[Noun Extractor] checked compounds. discovered 60 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 252\n",
      "[Noun Extractor] postprocessing ignore_features : 252 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (60 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 605 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3043 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5486, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 896 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 302\n",
      "[Noun Extractor] postprocessing ignore_features : 302 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 364 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=572, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2779 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5223, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 883 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 257 -> 256\n",
      "[Noun Extractor] postprocessing ignore_features : 256 -> 248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 248 -> 247\n",
      "[Noun Extractor] 247 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 501 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=727, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2514, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (10 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3106 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5327, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1012 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 267 -> 267\n",
      "[Noun Extractor] postprocessing ignore_features : 267 -> 255\n",
      "[Noun Extractor] postprocessing ignore_NJ : 255 -> 255\n",
      "[Noun Extractor] 255 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 682 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1107, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1016 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1653, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4938 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9814, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1722 words\n",
      "[Noun Extractor] checked compounds. discovered 161 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 651 -> 582\n",
      "[Noun Extractor] postprocessing ignore_features : 582 -> 566\n",
      "[Noun Extractor] postprocessing ignore_NJ : 566 -> 565\n",
      "[Noun Extractor] 565 nouns (161 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1121, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 31.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1212 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1987, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 363 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2262 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4155, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 719 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 249\n",
      "[Noun Extractor] postprocessing ignore_features : 249 -> 231\n",
      "[Noun Extractor] postprocessing ignore_NJ : 231 -> 229\n",
      "[Noun Extractor] 229 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1947 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3436, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 682 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 196\n",
      "[Noun Extractor] postprocessing ignore_features : 196 -> 185\n",
      "[Noun Extractor] postprocessing ignore_NJ : 185 -> 185\n",
      "[Noun Extractor] 185 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5963 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11404, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2475 words\n",
      "[Noun Extractor] checked compounds. discovered 83 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 748 -> 732\n",
      "[Noun Extractor] postprocessing ignore_features : 732 -> 707\n",
      "[Noun Extractor] postprocessing ignore_NJ : 707 -> 707\n",
      "[Noun Extractor] 707 nouns (83 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 34.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2979 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5350, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1163 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 265 -> 264\n",
      "[Noun Extractor] postprocessing ignore_features : 264 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 257\n",
      "[Noun Extractor] 257 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4442 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8698, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1868 words\n",
      "[Noun Extractor] checked compounds. discovered 99 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 585 -> 544\n",
      "[Noun Extractor] postprocessing ignore_features : 544 -> 521\n",
      "[Noun Extractor] postprocessing ignore_NJ : 521 -> 520\n",
      "[Noun Extractor] 520 nouns (99 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 50.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3562 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6874, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1054 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 97 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 399 -> 377\n",
      "[Noun Extractor] postprocessing ignore_features : 377 -> 355\n",
      "[Noun Extractor] postprocessing ignore_NJ : 355 -> 354\n",
      "[Noun Extractor] 354 nouns (97 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1850 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3215, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 698 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 149 -> 149\n",
      "[Noun Extractor] postprocessing ignore_features : 149 -> 140\n",
      "[Noun Extractor] postprocessing ignore_NJ : 140 -> 140\n",
      "[Noun Extractor] 140 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1907, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 45.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1062 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1632, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 331 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 597 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4382 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7920, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1844 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 503 -> 494\n",
      "[Noun Extractor] postprocessing ignore_features : 494 -> 472\n",
      "[Noun Extractor] postprocessing ignore_NJ : 472 -> 472\n",
      "[Noun Extractor] 472 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5494 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11507, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1688 words\n",
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 700 -> 675\n",
      "[Noun Extractor] postprocessing ignore_features : 675 -> 655\n",
      "[Noun Extractor] postprocessing ignore_NJ : 655 -> 655\n",
      "[Noun Extractor] 655 nouns (158 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 53.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1762 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3517, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 531 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 173 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 49.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1259, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1041 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1829, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3650 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5819, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1200 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 360 -> 356\n",
      "[Noun Extractor] postprocessing ignore_features : 356 -> 341\n",
      "[Noun Extractor] postprocessing ignore_NJ : 341 -> 341\n",
      "[Noun Extractor] 341 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1179 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1945, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 355 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 110 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 103\n",
      "[Noun Extractor] 103 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4239 from 1 sents. mem=1.093 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7728, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1245 words\n",
      "[Noun Extractor] checked compounds. discovered 86 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 470 -> 453\n",
      "[Noun Extractor] postprocessing ignore_features : 453 -> 435\n",
      "[Noun Extractor] postprocessing ignore_NJ : 435 -> 434\n",
      "[Noun Extractor] 434 nouns (86 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2399 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3832, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 778 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 247 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 238\n",
      "[Noun Extractor] postprocessing ignore_NJ : 238 -> 238\n",
      "[Noun Extractor] 238 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 46.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1610 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2739, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 544 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 191\n",
      "[Noun Extractor] postprocessing ignore_features : 191 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 32.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1354 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2282, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2801 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5137, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 881 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 278 -> 268\n",
      "[Noun Extractor] postprocessing ignore_features : 268 -> 255\n",
      "[Noun Extractor] postprocessing ignore_NJ : 255 -> 254\n",
      "[Noun Extractor] 254 nouns (32 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1931 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3512, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 632 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (34 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 790 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1336, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 847 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1414, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3868 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7139, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1221 words\n",
      "[Noun Extractor] checked compounds. discovered 87 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 456 -> 451\n",
      "[Noun Extractor] postprocessing ignore_features : 451 -> 442\n",
      "[Noun Extractor] postprocessing ignore_NJ : 442 -> 439\n",
      "[Noun Extractor] 439 nouns (87 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 51.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=667, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 40.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 974 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1647, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 43.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5164 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8252, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1807 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 474 -> 473\n",
      "[Noun Extractor] postprocessing ignore_features : 473 -> 458\n",
      "[Noun Extractor] postprocessing ignore_NJ : 458 -> 458\n",
      "[Noun Extractor] 458 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 24.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2522 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4876, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 859 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 218\n",
      "[Noun Extractor] postprocessing ignore_features : 218 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 207\n",
      "[Noun Extractor] 207 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1620, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2232 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4125, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 761 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 251 -> 251\n",
      "[Noun Extractor] postprocessing ignore_features : 251 -> 242\n",
      "[Noun Extractor] postprocessing ignore_NJ : 242 -> 242\n",
      "[Noun Extractor] 242 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1147 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2039, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 354 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 48.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3815 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7830, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1295 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 488 -> 447\n",
      "[Noun Extractor] postprocessing ignore_features : 447 -> 429\n",
      "[Noun Extractor] postprocessing ignore_NJ : 429 -> 425\n",
      "[Noun Extractor] 425 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 52.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 594 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 39.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2864 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5158, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1065 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 293 -> 285\n",
      "[Noun Extractor] postprocessing ignore_features : 285 -> 270\n",
      "[Noun Extractor] postprocessing ignore_NJ : 270 -> 270\n",
      "[Noun Extractor] 270 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1161 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1889, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=900, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1206, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 41.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1476 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2377, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 480 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 125 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 42.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1133 from 1 sents. mem=1.093 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1965, mem=1.093 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.093 Gb                    \n",
      "[Noun Extractor] 44.94 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head',conn)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "ext = Extracter(df)\n",
    "df = ext.cleaning()\n",
    "\n",
    "new_words = ext.search_dict(sorted(ext.extract_nouns().items(),key=lambda _:_[1], reverse=True))\n",
    "sent = ext.extract_sent(new_words)\n",
    "\n",
    "# 변수 생성\n",
    "statistic = ext.extract_statistic_value(sent)\n",
    "# rpprat : 명사의 오른쪽에 조사가 오는 비율, rwsrat : 명사의 오른쪽에 white space가 오는 비율\n",
    "r_rat = ext.extract_r_rat(sent,statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.Extracter.extract_r_rat.<locals>.<lambda>()>,\n",
       "            {'신천지': {'rpprat': 0.22842105263157894,\n",
       "              'rwsrat': 0.6547368421052632},\n",
       "             '확진자': {'rpprat': 0.17532325224632916,\n",
       "              'rwsrat': 0.7565198334429104},\n",
       "             '알아보자': {'rpprat': 0.007107133456172677,\n",
       "              'rwsrat': 0.9648153022725279},\n",
       "             '문재앙': {'rpprat': 0.20966010733452595,\n",
       "              'rwsrat': 0.7220035778175313},\n",
       "             '요즘': {'rpprat': 0.05871949062610541,\n",
       "              'rwsrat': 0.8906968517863459},\n",
       "             '재업': {'rpprat': 0.009609224855861628,\n",
       "              'rwsrat': 0.5675848814862268},\n",
       "             '너네': {'rpprat': 0.1800162910670649,\n",
       "              'rwsrat': 0.7303828400760249},\n",
       "             '아이돌': {'rpprat': 0.15474947807933195,\n",
       "              'rwsrat': 0.7210334029227558},\n",
       "             '얘들아': {'rpprat': 0.004133654839820875,\n",
       "              'rwsrat': 0.9724423010678608},\n",
       "             '스압': {'rpprat': 0.00490402129746392,\n",
       "              'rwsrat': 0.6368221941992434},\n",
       "             '많이': {'rpprat': 0.02918438345574024,\n",
       "              'rwsrat': 0.8283726323927328},\n",
       "             '박원순': {'rpprat': 0.22396349930569331,\n",
       "              'rwsrat': 0.7097798055941281},\n",
       "             '남친': {'rpprat': 0.46153846153846156,\n",
       "              'rwsrat': 0.37485818016791467},\n",
       "             '강다니엘': {'rpprat': 0.15749811605124342,\n",
       "              'rwsrat': 0.7776940467219292},\n",
       "             '트와이스': {'rpprat': 0.087248322147651,\n",
       "              'rwsrat': 0.8541793776693105},\n",
       "             '광화문': {'rpprat': 0.15336658354114713,\n",
       "              'rwsrat': 0.684954280964256},\n",
       "             '워너원': {'rpprat': 0.12192622950819672,\n",
       "              'rwsrat': 0.7756147540983607},\n",
       "             '남자친구': {'rpprat': 0.48952543822146216,\n",
       "              'rwsrat': 0.42967079948696024},\n",
       "             '여자들': {'rpprat': 0.5814985795454546,\n",
       "              'rwsrat': 0.3815696022727273},\n",
       "             '세월호': {'rpprat': 0.09121459433509362,\n",
       "              'rwsrat': 0.8094095055208833},\n",
       "             '어떻게': {'rpprat': 0.015166835187057633,\n",
       "              'rwsrat': 0.8624873609706775},\n",
       "             '새끼들': {'rpprat': 0.3387096774193548,\n",
       "              'rwsrat': 0.636512388966807},\n",
       "             '프듀': {'rpprat': 0.10090466249130133,\n",
       "              'rwsrat': 0.7272094641614475},\n",
       "             '존나': {'rpprat': 0.03392259913999045,\n",
       "              'rwsrat': 0.8086478738652652},\n",
       "             '레전드': {'rpprat': 0.06074342701722575,\n",
       "              'rwsrat': 0.8772179769459915},\n",
       "             '분들': {'rpprat': 0.31936017650303367,\n",
       "              'rwsrat': 0.649200220628792},\n",
       "             '우리나라': {'rpprat': 0.3485714285714286,\n",
       "              'rwsrat': 0.6007453416149068},\n",
       "             '정몽준': {'rpprat': 0.18473282442748093,\n",
       "              'rwsrat': 0.7465648854961832},\n",
       "             '김치년': {'rpprat': 0.36316501352569885,\n",
       "              'rwsrat': 0.37330928764652843},\n",
       "             '방탄소년단': {'rpprat': 0.1682743837084673,\n",
       "              'rwsrat': 0.7995712754555199},\n",
       "             '레드벨벳': {'rpprat': 0.11032388663967611,\n",
       "              'rwsrat': 0.8360323886639676},\n",
       "             '저장소': {'rpprat': 0.01351709514974821,\n",
       "              'rwsrat': 0.8314338722501988},\n",
       "             '황교안': {'rpprat': 0.1865242399342646,\n",
       "              'rwsrat': 0.7411668036154478},\n",
       "             '세븐틴': {'rpprat': 0.1366120218579235,\n",
       "              'rwsrat': 0.8163934426229508},\n",
       "             '보고가라': {'rpprat': 0.00176522506619594,\n",
       "              'rwsrat': 0.9620476610767873},\n",
       "             '강남역': {'rpprat': 0.1145124716553288,\n",
       "              'rwsrat': 0.7925170068027211},\n",
       "             '김치녀': {'rpprat': 0.3218986836856801,\n",
       "              'rwsrat': 0.4918228958915038},\n",
       "             '아이린': {'rpprat': 0.1640625, 'rwsrat': 0.7544642857142857},\n",
       "             '연습생': {'rpprat': 0.15355450236966825,\n",
       "              'rwsrat': 0.7440758293838863},\n",
       "             '김제동': {'rpprat': 0.26476377952755903,\n",
       "              'rwsrat': 0.6820866141732284},\n",
       "             '최순실': {'rpprat': 0.1715686274509804,\n",
       "              'rwsrat': 0.7320261437908496},\n",
       "             '여친': {'rpprat': 0.38084507042253524,\n",
       "              'rwsrat': 0.4183098591549296},\n",
       "             '뽐게': {'rpprat': 0.2516778523489933,\n",
       "              'rwsrat': 0.6353467561521253},\n",
       "             '게이야': {'rpprat': 0.0017371163867979154,\n",
       "              'rwsrat': 0.9797336421540244},\n",
       "             '천조국': {'rpprat': 0.24408238549031663,\n",
       "              'rwsrat': 0.6738395327390101},\n",
       "             '메르스': {'rpprat': 0.1553956834532374,\n",
       "              'rwsrat': 0.6532374100719425},\n",
       "             '뉴이스트': {'rpprat': 0.15485996705107083,\n",
       "              'rwsrat': 0.7347611202635914},\n",
       "             '스토브리그': {'rpprat': 0.11136890951276102,\n",
       "              'rwsrat': 0.8561484918793504},\n",
       "             '비투비': {'rpprat': 0.18485915492957747,\n",
       "              'rwsrat': 0.7077464788732394},\n",
       "             '네이버': {'rpprat': 0.11792666364870982,\n",
       "              'rwsrat': 0.7290629244001811},\n",
       "             '박지훈': {'rpprat': 0.14074074074074075,\n",
       "              'rwsrat': 0.812962962962963},\n",
       "             '뽐뿌': {'rpprat': 0.308472553699284,\n",
       "              'rwsrat': 0.45584725536992843},\n",
       "             '부정선거': {'rpprat': 0.13359528487229863,\n",
       "              'rwsrat': 0.7956777996070727},\n",
       "             '박주신': {'rpprat': 0.10793650793650794,\n",
       "              'rwsrat': 0.7825396825396825},\n",
       "             '아니냐': {'rpprat': 0.022603599832565928,\n",
       "              'rwsrat': 0.9660946002511511},\n",
       "             '번째': {'rpprat': 0.05247225025227043,\n",
       "              'rwsrat': 0.8385469223007064},\n",
       "             '이만희': {'rpprat': 0.20634920634920634,\n",
       "              'rwsrat': 0.7341269841269841},\n",
       "             '약스압': {'rpprat': 0.005208333333333333,\n",
       "              'rwsrat': 0.7119791666666667},\n",
       "             '자가격리': {'rpprat': 0.16872427983539096,\n",
       "              'rwsrat': 0.6296296296296297},\n",
       "             '코스트코': {'rpprat': 0.12234910277324633,\n",
       "              'rwsrat': 0.833605220228385},\n",
       "             '유튜브': {'rpprat': 0.22270114942528735,\n",
       "              'rwsrat': 0.6973180076628352},\n",
       "             '갓건배': {'rpprat': 0.1375921375921376,\n",
       "              'rwsrat': 0.7837837837837838},\n",
       "             '여자친구': {'rpprat': 0.40880782918149466,\n",
       "              'rwsrat': 0.5231316725978647},\n",
       "             '이기야': {'rpprat': 0.007792207792207792,\n",
       "              'rwsrat': 0.9761038961038961},\n",
       "             '공적마스크': {'rpprat': 0.07061503416856492,\n",
       "              'rwsrat': 0.8952164009111617},\n",
       "             '유튜버': {'rpprat': 0.23842592592592593,\n",
       "              'rwsrat': 0.691358024691358},\n",
       "             '노무현': {'rpprat': 0.17723492723492723,\n",
       "              'rwsrat': 0.7089397089397089},\n",
       "             '비트코인': {'rpprat': 0.1732142857142857,\n",
       "              'rwsrat': 0.6964285714285714},\n",
       "             '오늘자': {'rpprat': 0.0015348123343100321,\n",
       "              'rwsrat': 0.981861308776336},\n",
       "             '솔직히': {'rpprat': 0.0003006614552014432,\n",
       "              'rwsrat': 0.9861695730607336},\n",
       "             '표창원': {'rpprat': 0.21875, 'rwsrat': 0.690625},\n",
       "             '황민현': {'rpprat': 0.15914489311163896,\n",
       "              'rwsrat': 0.7672209026128266},\n",
       "             '우한폐렴': {'rpprat': 0.17041800643086816,\n",
       "              'rwsrat': 0.7491961414790996},\n",
       "             '성재기': {'rpprat': 0.20308483290488433,\n",
       "              'rwsrat': 0.62853470437018},\n",
       "             '낸시랭': {'rpprat': 0.20394736842105263,\n",
       "              'rwsrat': 0.7236842105263158},\n",
       "             '묻재업': {'rpprat': 0.008655804480651732,\n",
       "              'rwsrat': 0.6512219959266803},\n",
       "             '중국인': {'rpprat': 0.2924704418170504,\n",
       "              'rwsrat': 0.5544492843808339},\n",
       "             '댓글': {'rpprat': 0.15633581796783053,\n",
       "              'rwsrat': 0.5980776775205963},\n",
       "             '강동호': {'rpprat': 0.14245014245014245,\n",
       "              'rwsrat': 0.7749287749287749},\n",
       "             '클라스': {'rpprat': 0.03146417445482866,\n",
       "              'rwsrat': 0.9322429906542056},\n",
       "             '힐러리': {'rpprat': 0.18528610354223432,\n",
       "              'rwsrat': 0.7629427792915532},\n",
       "             '박근혜': {'rpprat': 0.23194748358862144,\n",
       "              'rwsrat': 0.6706783369803063},\n",
       "             '아라보자': {'rpprat': 0.00375, 'rwsrat': 0.9725},\n",
       "             '이준석': {'rpprat': 0.2517857142857143, 'rwsrat': 0.7},\n",
       "             '사전투표': {'rpprat': 0.1591928251121076,\n",
       "              'rwsrat': 0.484304932735426},\n",
       "             '드루킹': {'rpprat': 0.17329545454545456,\n",
       "              'rwsrat': 0.7272727272727273},\n",
       "             '레디백': {'rpprat': 0.07954545454545454,\n",
       "              'rwsrat': 0.8920454545454546},\n",
       "             '문재인이': {'rpprat': 0.05585106382978723,\n",
       "              'rwsrat': 0.9202127659574468},\n",
       "             '팬덤': {'rpprat': 0.1472, 'rwsrat': 0.6224},\n",
       "             '한국인': {'rpprat': 0.36665548473666554,\n",
       "              'rwsrat': 0.47936933914793695},\n",
       "             '페이코인': {'rpprat': 0.05211726384364821,\n",
       "              'rwsrat': 0.9218241042345277},\n",
       "             '걸그룹': {'rpprat': 0.10418410041841004,\n",
       "              'rwsrat': 0.8142259414225942},\n",
       "             '넷플릭스': {'rpprat': 0.19293478260869565,\n",
       "              'rwsrat': 0.782608695652174},\n",
       "             '노짱': {'rpprat': 0.19745222929936307,\n",
       "              'rwsrat': 0.548349739432542},\n",
       "             '일베충': {'rpprat': 0.32327919566898683,\n",
       "              'rwsrat': 0.37548337200309356},\n",
       "             '진중권': {'rpprat': 0.2345132743362832,\n",
       "              'rwsrat': 0.6780973451327433},\n",
       "             '일베인': {'rpprat': 0.30952380952380953,\n",
       "              'rwsrat': 0.2108843537414966},\n",
       "             '워마드': {'rpprat': 0.18646080760095013,\n",
       "              'rwsrat': 0.6912114014251781},\n",
       "             '프로듀스': {'rpprat': 0.08281573498964803,\n",
       "              'rwsrat': 0.865424430641822},\n",
       "             '감염자': {'rpprat': 0.26476190476190475,\n",
       "              'rwsrat': 0.6323809523809524},\n",
       "             '갤럭시': {'rpprat': 0.05266666666666667, 'rwsrat': 0.612},\n",
       "             '김우석': {'rpprat': 0.1284722222222222, 'rwsrat': 0.8125},\n",
       "             '틀딱': {'rpprat': 0.26907857546636516,\n",
       "              'rwsrat': 0.3583945732052007},\n",
       "             '통진당': {'rpprat': 0.10786802030456853,\n",
       "              'rwsrat': 0.7715736040609137},\n",
       "             '트위터': {'rpprat': 0.23257328990228013,\n",
       "              'rwsrat': 0.6794788273615635},\n",
       "             '성님': {'rpprat': 0.4135702746365105,\n",
       "              'rwsrat': 0.4596122778675283},\n",
       "             '재난지원금': {'rpprat': 0.13513513513513514,\n",
       "              'rwsrat': 0.8221447253705318},\n",
       "             '엑소': {'rpprat': 0.1660952380952381,\n",
       "              'rwsrat': 0.5702857142857143},\n",
       "             '나왔네요': {'rpprat': 0.002074688796680498,\n",
       "              'rwsrat': 0.9896265560165975},\n",
       "             '행게이': {'rpprat': 0.10759493670886076,\n",
       "              'rwsrat': 0.5953322784810127},\n",
       "             '손흥민': {'rpprat': 0.18054583624912526,\n",
       "              'rwsrat': 0.7746675997200839},\n",
       "             '크리스탈': {'rpprat': 0.12218045112781954,\n",
       "              'rwsrat': 0.8007518796992481},\n",
       "             '전남친': {'rpprat': 0.438136826783115,\n",
       "              'rwsrat': 0.4585152838427948},\n",
       "             '월드컵': {'rpprat': 0.07848837209302326,\n",
       "              'rwsrat': 0.8226744186046512},\n",
       "             '남친이': {'rpprat': 0.2838983050847458,\n",
       "              'rwsrat': 0.6593220338983051},\n",
       "             '흙수저': {'rpprat': 0.18280632411067194,\n",
       "              'rwsrat': 0.6363636363636364},\n",
       "             '러블리즈': {'rpprat': 0.15730337078651685,\n",
       "              'rwsrat': 0.7932584269662921},\n",
       "             '우리집': {'rpprat': 0.12249346120313863,\n",
       "              'rwsrat': 0.7654751525719268},\n",
       "             '백현': {'rpprat': 0.1838351822503962,\n",
       "              'rwsrat': 0.6719492868462758},\n",
       "             '아이폰': {'rpprat': 0.1406480117820324,\n",
       "              'rwsrat': 0.7827687776141384},\n",
       "             '왔다': {'rpprat': 0.069140625, 'rwsrat': 0.86171875},\n",
       "             '머한민국': {'rpprat': 0.16047297297297297,\n",
       "              'rwsrat': 0.7905405405405406},\n",
       "             '옹성우': {'rpprat': 0.1712707182320442,\n",
       "              'rwsrat': 0.7569060773480663},\n",
       "             '동물저장소': {'rpprat': 0.010660980810234541,\n",
       "              'rwsrat': 0.6780383795309168},\n",
       "             '이석기': {'rpprat': 0.18284424379232506,\n",
       "              'rwsrat': 0.7562076749435666},\n",
       "             '육성재': {'rpprat': 0.11926605504587157,\n",
       "              'rwsrat': 0.7889908256880734},\n",
       "             '찐따': {'rpprat': 0.3155440414507772,\n",
       "              'rwsrat': 0.38134715025906735},\n",
       "             '빅스마일데이': {'rpprat': 0.1686746987951807, 'rwsrat': 0.75},\n",
       "             '편의점': {'rpprat': 0.1823929961089494,\n",
       "              'rwsrat': 0.705739299610895},\n",
       "             '김경수': {'rpprat': 0.20819112627986347,\n",
       "              'rwsrat': 0.7372013651877133},\n",
       "             '새언니': {'rpprat': 0.4490861618798956,\n",
       "              'rwsrat': 0.4412532637075718},\n",
       "             '다이빙벨': {'rpprat': 0.1495601173020528,\n",
       "              'rwsrat': 0.8093841642228738},\n",
       "             '봉준호': {'rpprat': 0.18421052631578946,\n",
       "              'rwsrat': 0.718421052631579},\n",
       "             '안되는': {'rpprat': 0.08678847505270555,\n",
       "              'rwsrat': 0.7565003513703443},\n",
       "             '하성운': {'rpprat': 0.13740458015267176,\n",
       "              'rwsrat': 0.7938931297709924},\n",
       "             '전세계': {'rpprat': 0.2985480943738657,\n",
       "              'rwsrat': 0.5970961887477314},\n",
       "             '이진혁': {'rpprat': 0.1411764705882353,\n",
       "              'rwsrat': 0.7764705882352941},\n",
       "             '셀카': {'rpprat': 0.15126050420168066,\n",
       "              'rwsrat': 0.6074429771908764},\n",
       "             '너넨': {'rpprat': 0.0, 'rwsrat': 0.9817073170731707},\n",
       "             '김장훈': {'rpprat': 0.16666666666666666, 'rwsrat': 0.75},\n",
       "             '킹덤': {'rpprat': 0.06552706552706553,\n",
       "              'rwsrat': 0.8518518518518519},\n",
       "             '얼마나': {'rpprat': 0.0016685205784204673,\n",
       "              'rwsrat': 0.9471635150166852},\n",
       "             '갓본': {'rpprat': 0.6411290322580645,\n",
       "              'rwsrat': 0.2590725806451613},\n",
       "             '섬노예': {'rpprat': 0.18421052631578946,\n",
       "              'rwsrat': 0.6900584795321637},\n",
       "             '페북': {'rpprat': 0.2678393627613674,\n",
       "              'rwsrat': 0.4576833720544308},\n",
       "             '떡밥': {'rpprat': 0.26233023588277343,\n",
       "              'rwsrat': 0.5625446747676912},\n",
       "             '좌좀': {'rpprat': 0.32725060827250607,\n",
       "              'rwsrat': 0.22019464720194648},\n",
       "             '소속사': {'rpprat': 0.2537313432835821,\n",
       "              'rwsrat': 0.6282225237449118},\n",
       "             '우리반': {'rpprat': 0.31990521327014215,\n",
       "              'rwsrat': 0.5355450236966824},\n",
       "             '배현진': {'rpprat': 0.1902071563088512,\n",
       "              'rwsrat': 0.7777777777777778},\n",
       "             '짝남': {'rpprat': 0.5825396825396826,\n",
       "              'rwsrat': 0.32222222222222224},\n",
       "             '요기요': {'rpprat': 0.11658031088082901,\n",
       "              'rwsrat': 0.8523316062176166},\n",
       "             '정보글': {'rpprat': 0.08807588075880758,\n",
       "              'rwsrat': 0.6273712737127372},\n",
       "             '안희정': {'rpprat': 0.19076923076923077,\n",
       "              'rwsrat': 0.7446153846153846},\n",
       "             '단원고': {'rpprat': 0.062135922330097085,\n",
       "              'rwsrat': 0.829126213592233},\n",
       "             '게이들아': {'rpprat': 0.0016574585635359116,\n",
       "              'rwsrat': 0.9718232044198895},\n",
       "             '봐주세요': {'rpprat': 0.0056022408963585435,\n",
       "              'rwsrat': 0.9243697478991597},\n",
       "             '갈현동': {'rpprat': 0.05714285714285714,\n",
       "              'rwsrat': 0.8163265306122449},\n",
       "             '사람이': {'rpprat': 0.13415892672858618,\n",
       "              'rwsrat': 0.7574819401444789},\n",
       "             '블랙핑크': {'rpprat': 0.09433962264150944,\n",
       "              'rwsrat': 0.8616352201257862},\n",
       "             '윤아': {'rpprat': 0.2055800293685756,\n",
       "              'rwsrat': 0.6798825256975036},\n",
       "             '스베누': {'rpprat': 0.17475728155339806,\n",
       "              'rwsrat': 0.7184466019417476},\n",
       "             '노회찬': {'rpprat': 0.2318840579710145,\n",
       "              'rwsrat': 0.7391304347826086},\n",
       "             '대구시': {'rpprat': 0.3649425287356322,\n",
       "              'rwsrat': 0.34770114942528735},\n",
       "             '에이핑크': {'rpprat': 0.15008431703204048,\n",
       "              'rwsrat': 0.7976391231028668},\n",
       "             '티저': {'rpprat': 0.03512014787430684,\n",
       "              'rwsrat': 0.7726432532347505},\n",
       "             '소녀시대': {'rpprat': 0.1321928460342146,\n",
       "              'rwsrat': 0.833592534992224},\n",
       "             '느낀점': {'rpprat': 0.045859872611464965,\n",
       "              'rwsrat': 0.932484076433121},\n",
       "             '엔시티': {'rpprat': 0.11796246648793565,\n",
       "              'rwsrat': 0.7372654155495979},\n",
       "             '나왔다': {'rpprat': 0.05861581920903955,\n",
       "              'rwsrat': 0.894774011299435},\n",
       "             '대학교': {'rpprat': 0.1318785578747628,\n",
       "              'rwsrat': 0.7087286527514232},\n",
       "             '아직도': {'rpprat': 0.0008361204013377926,\n",
       "              'rwsrat': 0.975752508361204},\n",
       "             '중고나라': {'rpprat': 0.21184510250569477,\n",
       "              'rwsrat': 0.7539863325740319},\n",
       "             '조원진': {'rpprat': 0.23628691983122363,\n",
       "              'rwsrat': 0.7257383966244726},\n",
       "             '투바투': {'rpprat': 0.1085972850678733,\n",
       "              'rwsrat': 0.8461538461538461},\n",
       "             '극혐': {'rpprat': 0.06864064602960969,\n",
       "              'rwsrat': 0.4737550471063257},\n",
       "             '이시각': {'rpprat': 0.005050505050505051,\n",
       "              'rwsrat': 0.9709595959595959},\n",
       "             '훈남': {'rpprat': 0.2289855072463768,\n",
       "              'rwsrat': 0.48405797101449277},\n",
       "             '김정은': {'rpprat': 0.1885603490063015,\n",
       "              'rwsrat': 0.7658749394086282},\n",
       "             '진단키트': {'rpprat': 0.12758620689655173,\n",
       "              'rwsrat': 0.8241379310344827},\n",
       "             '역대급': {'rpprat': 0.08241534067727459,\n",
       "              'rwsrat': 0.8270093839249286},\n",
       "             '박원숭': {'rpprat': 0.2323856613102596,\n",
       "              'rwsrat': 0.6736711990111248},\n",
       "             '아이돌들': {'rpprat': 0.18326693227091634,\n",
       "              'rwsrat': 0.7908366533864541},\n",
       "             '설현': {'rpprat': 0.20463847203274216,\n",
       "              'rwsrat': 0.679399727148704},\n",
       "             '이세돌': {'rpprat': 0.20938628158844766,\n",
       "              'rwsrat': 0.7111913357400722},\n",
       "             '문재인': {'rpprat': 0.21144809910294746,\n",
       "              'rwsrat': 0.7152214153495657},\n",
       "             '엑스원': {'rpprat': 0.12334801762114538,\n",
       "              'rwsrat': 0.7797356828193832},\n",
       "             '조선일보': {'rpprat': 0.24858115777525538,\n",
       "              'rwsrat': 0.6969353007945517},\n",
       "             '반일종족주의': {'rpprat': 0.07211538461538461,\n",
       "              'rwsrat': 0.8846153846153846},\n",
       "             '이거지': {'rpprat': 0.005633802816901409,\n",
       "              'rwsrat': 0.9830985915492958},\n",
       "             '슈가맨': {'rpprat': 0.2073732718894009,\n",
       "              'rwsrat': 0.7788018433179723},\n",
       "             '윤지성': {'rpprat': 0.20253164556962025,\n",
       "              'rwsrat': 0.6962025316455697},\n",
       "             '손나은': {'rpprat': 0.10485133020344288,\n",
       "              'rwsrat': 0.8169014084507042},\n",
       "             '장동민': {'rpprat': 0.2210796915167095,\n",
       "              'rwsrat': 0.7223650385604113},\n",
       "             '오마이걸': {'rpprat': 0.10632911392405063,\n",
       "              'rwsrat': 0.8632911392405064},\n",
       "             '대구시장': {'rpprat': 0.25, 'rwsrat': 0.6567164179104478},\n",
       "             '크레용팝': {'rpprat': 0.1757188498402556,\n",
       "              'rwsrat': 0.7507987220447284},\n",
       "             '일베특파원': {'rpprat': 0.014598540145985401,\n",
       "              'rwsrat': 0.6978102189781021},\n",
       "             '분향소': {'rpprat': 0.12758620689655173,\n",
       "              'rwsrat': 0.8068965517241379},\n",
       "             '보니하니': {'rpprat': 0.05223880597014925,\n",
       "              'rwsrat': 0.9104477611940298},\n",
       "             '윤미향': {'rpprat': 0.14218009478672985,\n",
       "              'rwsrat': 0.8104265402843602},\n",
       "             '아이즈원': {'rpprat': 0.11061946902654868,\n",
       "              'rwsrat': 0.8606194690265486},\n",
       "             '캐시워크': {'rpprat': 0.00646551724137931,\n",
       "              'rwsrat': 0.9051724137931034},\n",
       "             '여자분들': {'rpprat': 0.1712846347607053,\n",
       "              'rwsrat': 0.7959697732997482},\n",
       "             '새민련': {'rpprat': 0.24096385542168675,\n",
       "              'rwsrat': 0.6746987951807228},\n",
       "             '불매운동': {'rpprat': 0.16666666666666666,\n",
       "              'rwsrat': 0.6791044776119403},\n",
       "             '이와중': {'rpprat': 0.9141886151231946,\n",
       "              'rwsrat': 0.062022090059473234},\n",
       "             '그알': {'rpprat': 0.17045454545454544,\n",
       "              'rwsrat': 0.7159090909090909},\n",
       "             '윤지오': {'rpprat': 0.12719298245614036,\n",
       "              'rwsrat': 0.7456140350877193},\n",
       "             '무섭네요': {'rpprat': 0.0, 'rwsrat': 0.9764309764309764},\n",
       "             '위대한': {'rpprat': 0.08241758241758242,\n",
       "              'rwsrat': 0.8021978021978022},\n",
       "             '옥션': {'rpprat': 0.128, 'rwsrat': 0.75},\n",
       "             '농약급식': {'rpprat': 0.2, 'rwsrat': 0.7463414634146341},\n",
       "             '페이코': {'rpprat': 0.46776611694152925,\n",
       "              'rwsrat': 0.4572713643178411},\n",
       "             '민식이법': {'rpprat': 0.19829424307036247,\n",
       "              'rwsrat': 0.7633262260127932},\n",
       "             '피꺼솟': {'rpprat': 0.05965909090909091,\n",
       "              'rwsrat': 0.6363636363636364},\n",
       "             '뮤비': {'rpprat': 0.1879432624113475,\n",
       "              'rwsrat': 0.6737588652482269},\n",
       "             '정준영': {'rpprat': 0.14457831325301204, 'rwsrat': 0.8},\n",
       "             '김세의': {'rpprat': 0.05099150141643059,\n",
       "              'rwsrat': 0.8243626062322946},\n",
       "             '이명박': {'rpprat': 0.1798201798201798,\n",
       "              'rwsrat': 0.6633366633366633},\n",
       "             '이새끼': {'rpprat': 0.34715346534653463,\n",
       "              'rwsrat': 0.5278465346534653},\n",
       "             '인피니트': {'rpprat': 0.16666666666666666,\n",
       "              'rwsrat': 0.6990740740740741},\n",
       "             '문슬람': {'rpprat': 0.3418079096045198,\n",
       "              'rwsrat': 0.4774011299435028},\n",
       "             '특이점이': {'rpprat': 0.002403846153846154,\n",
       "              'rwsrat': 0.8966346153846154},\n",
       "             '있냐': {'rpprat': 0.022777369581190303,\n",
       "              'rwsrat': 0.9654665686994857},\n",
       "             '이종인': {'rpprat': 0.21138211382113822,\n",
       "              'rwsrat': 0.6626016260162602},\n",
       "             '시부모님': {'rpprat': 0.39104477611940297,\n",
       "              'rwsrat': 0.5402985074626866},\n",
       "             '코스프레': {'rpprat': 0.10283687943262411,\n",
       "              'rwsrat': 0.75177304964539},\n",
       "             '수민이': {'rpprat': 0.16822429906542055,\n",
       "              'rwsrat': 0.7990654205607477},\n",
       "             '한승우': {'rpprat': 0.12698412698412698,\n",
       "              'rwsrat': 0.8148148148148148},\n",
       "             '토트넘': {'rpprat': 0.18004338394793926,\n",
       "              'rwsrat': 0.7613882863340564},\n",
       "             '조주빈': {'rpprat': 0.24456521739130435,\n",
       "              'rwsrat': 0.7092391304347826},\n",
       "             '민경욱': {'rpprat': 0.21008403361344538,\n",
       "              'rwsrat': 0.7100840336134454},\n",
       "             '윤석열': {'rpprat': 0.21235521235521235,\n",
       "              'rwsrat': 0.7181467181467182},\n",
       "             '까보전': {'rpprat': 0.26032315978456017,\n",
       "              'rwsrat': 0.6104129263913824},\n",
       "             '신종코로나': {'rpprat': 0.08796296296296297,\n",
       "              'rwsrat': 0.8287037037037037},\n",
       "             '미세먼지': {'rpprat': 0.20665499124343256,\n",
       "              'rwsrat': 0.7294220665499125},\n",
       "             '윤서인': {'rpprat': 0.14853801169590644,\n",
       "              'rwsrat': 0.7871345029239766},\n",
       "             '브금': {'rpprat': 0.02900763358778626,\n",
       "              'rwsrat': 0.5526717557251909},\n",
       "             '펭수': {'rpprat': 0.14910025706940874,\n",
       "              'rwsrat': 0.6375321336760925},\n",
       "             '새내기': {'rpprat': 0.2345679012345679,\n",
       "              'rwsrat': 0.5709876543209876},\n",
       "             '국정원': {'rpprat': 0.32597173144876324,\n",
       "              'rwsrat': 0.5247349823321554},\n",
       "             '한국이': {'rpprat': 0.0532825880114177,\n",
       "              'rwsrat': 0.884871550903901},\n",
       "             '코스피': {'rpprat': 0.09269662921348315,\n",
       "              'rwsrat': 0.8735955056179775},\n",
       "             '변희재': {'rpprat': 0.21725571725571727,\n",
       "              'rwsrat': 0.7224532224532224},\n",
       "             '극딜': {'rpprat': 0.0989433237271854,\n",
       "              'rwsrat': 0.46397694524495675},\n",
       "             '남편이': {'rpprat': 0.11311475409836065,\n",
       "              'rwsrat': 0.8557377049180328},\n",
       "             '박정희': {'rpprat': 0.22323166774821546,\n",
       "              'rwsrat': 0.6508760545100584},\n",
       "             '이재명': {'rpprat': 0.18801498127340824,\n",
       "              'rwsrat': 0.7460674157303371},\n",
       "             '태블릿': {'rpprat': 0.1793103448275862,\n",
       "              'rwsrat': 0.7482758620689656},\n",
       "             '골목식당': {'rpprat': 0.08839779005524862,\n",
       "              'rwsrat': 0.8821362799263351},\n",
       "             '꿀팁': {'rpprat': 0.034195933456561925,\n",
       "              'rwsrat': 0.8170055452865065},\n",
       "             '무개념': {'rpprat': 0.03966942148760331,\n",
       "              'rwsrat': 0.6776859504132231},\n",
       "             '좋아하는': {'rpprat': 0.10698159083014935,\n",
       "              'rwsrat': 0.7176102813476901},\n",
       "             '냥이저장소': {'rpprat': 0.013779527559055118,\n",
       "              'rwsrat': 0.6318897637795275},\n",
       "             '인천상륙작전': {'rpprat': 0.08415841584158416,\n",
       "              'rwsrat': 0.8663366336633663},\n",
       "             '이태원': {'rpprat': 0.05105105105105105,\n",
       "              'rwsrat': 0.6646646646646647},\n",
       "             '신은미': {'rpprat': 0.1297709923664122,\n",
       "              'rwsrat': 0.7519083969465649},\n",
       "             '청문회': {'rpprat': 0.2515527950310559,\n",
       "              'rwsrat': 0.6832298136645962},\n",
       "             '얘네': {'rpprat': 0.25181159420289856,\n",
       "              'rwsrat': 0.5561594202898551},\n",
       "             '쯔위': {'rpprat': 0.17234848484848486,\n",
       "              'rwsrat': 0.7234848484848485},\n",
       "             '멍뭉찡': {'rpprat': 0.16608996539792387,\n",
       "              'rwsrat': 0.6643598615916955},\n",
       "             '손소독제': {'rpprat': 0.211340206185567,\n",
       "              'rwsrat': 0.7680412371134021},\n",
       "             '김무성': {'rpprat': 0.2158273381294964,\n",
       "              'rwsrat': 0.7002398081534772},\n",
       "             '박주영': {'rpprat': 0.17288135593220338,\n",
       "              'rwsrat': 0.7661016949152543},\n",
       "             '재난기본소득': {'rpprat': 0.13432835820895522,\n",
       "              'rwsrat': 0.8407960199004975},\n",
       "             '페이스북': {'rpprat': 0.19638242894056848,\n",
       "              'rwsrat': 0.7390180878552972},\n",
       "             '운마': {'rpprat': 0.35642317380352645,\n",
       "              'rwsrat': 0.46851385390428213},\n",
       "             '집에서': {'rpprat': 0.03908484270734033,\n",
       "              'rwsrat': 0.9370829361296473},\n",
       "             '어때요': {'rpprat': 0.0, 'rwsrat': 0.9730538922155688},\n",
       "             '최저임금': {'rpprat': 0.16986301369863013,\n",
       "              'rwsrat': 0.7315068493150685},\n",
       "             '남자분들': {'rpprat': 0.16129032258064516,\n",
       "              'rwsrat': 0.7806451612903226},\n",
       "             '버닝썬': {'rpprat': 0.07462686567164178,\n",
       "              'rwsrat': 0.8619402985074627},\n",
       "             '김대중': {'rpprat': 0.26703499079189685,\n",
       "              'rwsrat': 0.6022099447513812},\n",
       "             '새누리당': {'rpprat': 0.22903225806451613,\n",
       "              'rwsrat': 0.7225806451612903},\n",
       "             '집에': {'rpprat': 0.43799582463465553,\n",
       "              'rwsrat': 0.4450939457202505},\n",
       "             '노무쿤': {'rpprat': 0.23369565217391305,\n",
       "              'rwsrat': 0.720108695652174},\n",
       "             '고영태': {'rpprat': 0.21212121212121213,\n",
       "              'rwsrat': 0.7159090909090909},\n",
       "             '강용석': {'rpprat': 0.2701612903225806,\n",
       "              'rwsrat': 0.6706989247311828},\n",
       "             '의료진': {'rpprat': 0.3313953488372093,\n",
       "              'rwsrat': 0.5436046511627907},\n",
       "             '홍팍': {'rpprat': 0.22691879866518352,\n",
       "              'rwsrat': 0.4916573971078977},\n",
       "             '보지들': {'rpprat': 0.5819905213270142,\n",
       "              'rwsrat': 0.38293838862559243},\n",
       "             '블핑': {'rpprat': 0.076158940397351, 'rwsrat': 0.8079470198675497},\n",
       "             '언니들': {'rpprat': 0.14792899408284024,\n",
       "              'rwsrat': 0.7751479289940828},\n",
       "             '서울시': {'rpprat': 0.24429771908763506,\n",
       "              'rwsrat': 0.48259303721488594},\n",
       "             '김기종': {'rpprat': 0.23316062176165803,\n",
       "              'rwsrat': 0.6994818652849741},\n",
       "             '영화저장소': {'rpprat': 0.011441647597254004,\n",
       "              'rwsrat': 0.7276887871853547},\n",
       "             '전두환': {'rpprat': 0.23575129533678757,\n",
       "              'rwsrat': 0.6606217616580311},\n",
       "             '여성부': {'rpprat': 0.34536082474226804,\n",
       "              'rwsrat': 0.5412371134020618},\n",
       "             '뭐냐': {'rpprat': 0.10823529411764705,\n",
       "              'rwsrat': 0.8549019607843137},\n",
       "             '있는데': {'rpprat': 0.11556603773584906,\n",
       "              'rwsrat': 0.8702830188679245},\n",
       "             '롯데': {'rpprat': 0.08491686460807601,\n",
       "              'rwsrat': 0.25534441805225655},\n",
       "             '현재상황': {'rpprat': 0.025198938992042442,\n",
       "              'rwsrat': 0.9535809018567639},\n",
       "             '맘충': {'rpprat': 0.2505720823798627,\n",
       "              'rwsrat': 0.41304347826086957},\n",
       "             '걸스데이': {'rpprat': 0.07242339832869081,\n",
       "              'rwsrat': 0.8746518105849582},\n",
       "             '민식이': {'rpprat': 0.019746121297602257,\n",
       "              'rwsrat': 0.2820874471086037},\n",
       "             '연예인': {'rpprat': 0.22009679649688868,\n",
       "              'rwsrat': 0.6598294537911962},\n",
       "             '봤는데': {'rpprat': 0.08957219251336898,\n",
       "              'rwsrat': 0.8957219251336899},\n",
       "             '이번주': {'rpprat': 0.23655913978494625,\n",
       "              'rwsrat': 0.7240143369175627},\n",
       "             '일본인': {'rpprat': 0.3811447811447811,\n",
       "              'rwsrat': 0.4484848484848485},\n",
       "             '입국금지': {'rpprat': 0.19672131147540983,\n",
       "              'rwsrat': 0.6612021857923497},\n",
       "             '대자보': {'rpprat': 0.10491803278688525,\n",
       "              'rwsrat': 0.8098360655737705},\n",
       "             '추반좀': {'rpprat': 0.00980392156862745,\n",
       "              'rwsrat': 0.9362745098039216},\n",
       "             '알파고': {'rpprat': 0.2764227642276423,\n",
       "              'rwsrat': 0.6666666666666666},\n",
       "             '손석희': {'rpprat': 0.23390275952693823,\n",
       "              'rwsrat': 0.7109067017082786},\n",
       "             '왕따': {'rpprat': 0.1592554291623578,\n",
       "              'rwsrat': 0.3609100310237849},\n",
       "             '미스터트롯': {'rpprat': 0.125, 'rwsrat': 0.8508064516129032},\n",
       "             '삼일한': {'rpprat': 0.14251207729468598,\n",
       "              'rwsrat': 0.6714975845410628},\n",
       "             '권영진': {'rpprat': 0.15422885572139303,\n",
       "              'rwsrat': 0.7910447761194029},\n",
       "             '가짜뉴스': {'rpprat': 0.18098159509202455,\n",
       "              'rwsrat': 0.6717791411042945},\n",
       "             '재난소득': {'rpprat': 0.12087912087912088,\n",
       "              'rwsrat': 0.7967032967032966},\n",
       "             '이시국': {'rpprat': 0.6253968253968254,\n",
       "              'rwsrat': 0.31746031746031744},\n",
       "             '다음주': {'rpprat': 0.3045186640471513,\n",
       "              'rwsrat': 0.6542239685658153},\n",
       "             '웹툰': {'rpprat': 0.1194331983805668,\n",
       "              'rwsrat': 0.6336032388663968},\n",
       "             '질본': {'rpprat': 0.30851063829787234,\n",
       "              'rwsrat': 0.6170212765957447},\n",
       "             '여고생': {'rpprat': 0.25983037779491136,\n",
       "              'rwsrat': 0.6553585196607556},\n",
       "             '새누리': {'rpprat': 0.5327783558792925,\n",
       "              'rwsrat': 0.24557752341311134},\n",
       "             '동영상': {'rpprat': 0.1509090909090909,\n",
       "              'rwsrat': 0.7481818181818182},\n",
       "             '클라쓰': {'rpprat': 0.048600883652430045,\n",
       "              'rwsrat': 0.9204712812960235},\n",
       "             '박보검': {'rpprat': 0.19377162629757785,\n",
       "              'rwsrat': 0.71280276816609},\n",
       "             '지원금': {'rpprat': 0.15675675675675677,\n",
       "              'rwsrat': 0.7945945945945946},\n",
       "             '정경심': {'rpprat': 0.14606741573033707,\n",
       "              'rwsrat': 0.7921348314606742},\n",
       "             '씹스압': {'rpprat': 0.00688298918387414,\n",
       "              'rwsrat': 0.6597836774827925},\n",
       "             '저격일베': {'rpprat': 0.02249134948096886,\n",
       "              'rwsrat': 0.05190311418685121},\n",
       "             '노알라': {'rpprat': 0.2107843137254902,\n",
       "              'rwsrat': 0.5947712418300654},\n",
       "             '트윗': {'rpprat': 0.14551083591331268,\n",
       "              'rwsrat': 0.6790505675954592},\n",
       "             '유니클로': {'rpprat': 0.12195121951219512,\n",
       "              'rwsrat': 0.8353658536585366},\n",
       "             '놈들': {'rpprat': 0.36973833902161546,\n",
       "              'rwsrat': 0.5927189988623436},\n",
       "             '빅스마일': {'rpprat': 0.038535645472061654,\n",
       "              'rwsrat': 0.2678227360308285},\n",
       "             '조국이': {'rpprat': 0.11797752808988764,\n",
       "              'rwsrat': 0.8314606741573034},\n",
       "             '만들어보자': {'rpprat': 0.007029876977152899,\n",
       "              'rwsrat': 0.9507908611599297},\n",
       "             '투표인증': {'rpprat': 0.06620209059233449,\n",
       "              'rwsrat': 0.5888501742160279},\n",
       "             '일베': {'rpprat': 0.16881897754661931,\n",
       "              'rwsrat': 0.24361283775212483},\n",
       "             '유승민': {'rpprat': 0.2628398791540785,\n",
       "              'rwsrat': 0.6737160120845922},\n",
       "             '마리텔': {'rpprat': 0.066006600660066,\n",
       "              'rwsrat': 0.9075907590759076},\n",
       "             '스타벅스': {'rpprat': 0.12252475247524752,\n",
       "              'rwsrat': 0.8292079207920792},\n",
       "             '아육대': {'rpprat': 0.2490118577075099,\n",
       "              'rwsrat': 0.7114624505928854},\n",
       "             '싹쓰리': {'rpprat': 0.09278350515463918,\n",
       "              'rwsrat': 0.8247422680412371},\n",
       "             '리얼돌': {'rpprat': 0.15748031496062992,\n",
       "              'rwsrat': 0.7834645669291339},\n",
       "             '문죄인': {'rpprat': 0.21013412816691504,\n",
       "              'rwsrat': 0.7347242921013413},\n",
       "             '카톡': {'rpprat': 0.1295387634936212,\n",
       "              'rwsrat': 0.5080143931959438},\n",
       "             '김치년들': {'rpprat': 0.47167630057803467,\n",
       "              'rwsrat': 0.5069364161849711},\n",
       "             '유시민': {'rpprat': 0.2593283582089552,\n",
       "              'rwsrat': 0.6977611940298507},\n",
       "             '김진태': {'rpprat': 0.13495575221238937,\n",
       "              'rwsrat': 0.7013274336283186},\n",
       "             '연평해전': {'rpprat': 0.11381074168797954,\n",
       "              'rwsrat': 0.8312020460358056},\n",
       "             '정청래': {'rpprat': 0.14715719063545152,\n",
       "              'rwsrat': 0.8060200668896321},\n",
       "             '아닌가요': {'rpprat': 0.0012135922330097086,\n",
       "              'rwsrat': 0.9890776699029126},\n",
       "             '전여친': {'rpprat': 0.4149377593360996,\n",
       "              'rwsrat': 0.45643153526970953},\n",
       "             '게이들': {'rpprat': 0.7112470862470862,\n",
       "              'rwsrat': 0.2517482517482518},\n",
       "             '가세연': {'rpprat': 0.20710059171597633,\n",
       "              'rwsrat': 0.7692307692307693},\n",
       "             '이재용': {'rpprat': 0.17504051863857376,\n",
       "              'rwsrat': 0.7828200972447326},\n",
       "             '국회의원': {'rpprat': 0.22821576763485477,\n",
       "              'rwsrat': 0.656984785615491},\n",
       "             '투표율': {'rpprat': 0.2235294117647059,\n",
       "              'rwsrat': 0.7568627450980392},\n",
       "             '헬스장': {'rpprat': 0.267515923566879,\n",
       "              'rwsrat': 0.6719745222929936},\n",
       "             '여론조사': {'rpprat': 0.15625, 'rwsrat': 0.7647058823529411},\n",
       "             '지지율': {'rpprat': 0.15906288532675708,\n",
       "              'rwsrat': 0.8076448828606658},\n",
       "             '인스타': {'rpprat': 0.16789603090972954,\n",
       "              'rwsrat': 0.5830698981383913},\n",
       "             '개성공단': {'rpprat': 0.1360759493670886,\n",
       "              'rwsrat': 0.810126582278481},\n",
       "             '헤어지고': {'rpprat': 0.004056795131845842,\n",
       "              'rwsrat': 0.7991886409736308},\n",
       "             '부모님': {'rpprat': 0.4400939702427565,\n",
       "              'rwsrat': 0.45653876272513705},\n",
       "             '여동생': {'rpprat': 0.3089788732394366,\n",
       "              'rwsrat': 0.5572183098591549},\n",
       "             '틴탑': {'rpprat': 0.18055555555555555,\n",
       "              'rwsrat': 0.5972222222222222},\n",
       "             '애국보수': {'rpprat': 0.09847198641765705,\n",
       "              'rwsrat': 0.7232597623089984},\n",
       "             '좆냥이': {'rpprat': 0.14098360655737704,\n",
       "              'rwsrat': 0.819672131147541},\n",
       "             '안철수': {'rpprat': 0.2003530450132392,\n",
       "              'rwsrat': 0.7228596646072374},\n",
       "             '조승연': {'rpprat': 0.14084507042253522,\n",
       "              'rwsrat': 0.8169014084507042},\n",
       "             '야갤': {'rpprat': 0.2882882882882883,\n",
       "              'rwsrat': 0.25625625625625625},\n",
       "             '임병장': {'rpprat': 0.18012422360248448,\n",
       "              'rwsrat': 0.7515527950310559},\n",
       "             '좆됐다': {'rpprat': 0.03907637655417407,\n",
       "              'rwsrat': 0.9005328596802842},\n",
       "             '뭐가': {'rpprat': 0.006815968841285297,\n",
       "              'rwsrat': 0.8373904576436222},\n",
       "             '긴급재난지원금': {'rpprat': 0.10300429184549356,\n",
       "              'rwsrat': 0.871244635193133},\n",
       "             '우리학교': {'rpprat': 0.21379310344827587,\n",
       "              'rwsrat': 0.639080459770115},\n",
       "             '좌좀들': {'rpprat': 0.47964509394572025,\n",
       "              'rwsrat': 0.4963465553235908},\n",
       "             '더민주': {'rpprat': 0.23383084577114427,\n",
       "              'rwsrat': 0.7014925373134329},\n",
       "             '박지원': {'rpprat': 0.18485523385300667,\n",
       "              'rwsrat': 0.7572383073496659},\n",
       "             '김연아': {'rpprat': 0.21941354903943378,\n",
       "              'rwsrat': 0.7219413549039434},\n",
       "             '인기글': {'rpprat': 0.1551155115511551,\n",
       "              'rwsrat': 0.5907590759075908},\n",
       "             '않는': {'rpprat': 0.20298507462686566, 'rwsrat': 0.68},\n",
       "             '빡친': {'rpprat': 0.10202020202020202,\n",
       "              'rwsrat': 0.797979797979798},\n",
       "             '못하는': {'rpprat': 0.08470764617691154,\n",
       "              'rwsrat': 0.802848575712144},\n",
       "             '태극기': {'rpprat': 0.11636363636363636,\n",
       "              'rwsrat': 0.6298181818181818},\n",
       "             '그곳': {'rpprat': 0.5751295336787565,\n",
       "              'rwsrat': 0.3536269430051813},\n",
       "             '남성연대': {'rpprat': 0.1595238095238095,\n",
       "              'rwsrat': 0.7452380952380953},\n",
       "             '이자스민': {'rpprat': 0.2132564841498559,\n",
       "              'rwsrat': 0.7118155619596542},\n",
       "             '재기갑': {'rpprat': 0.21052631578947367,\n",
       "              'rwsrat': 0.7157894736842105},\n",
       "             '이유가': {'rpprat': 0.001774622892635315,\n",
       "              'rwsrat': 0.9582963620230701},\n",
       "             '홍준표': {'rpprat': 0.26048951048951047,\n",
       "              'rwsrat': 0.6835664335664335},\n",
       "             '지잡대': {'rpprat': 0.15851063829787235,\n",
       "              'rwsrat': 0.6074468085106383},\n",
       "             '도쿄올림픽': {'rpprat': 0.07264957264957266,\n",
       "              'rwsrat': 0.8974358974358975},\n",
       "             '당근마켓': {'rpprat': 0.27388535031847133,\n",
       "              'rwsrat': 0.6900212314225053},\n",
       "             '짱개': {'rpprat': 0.19285714285714287, 'rwsrat': 0.3875},\n",
       "             '오세훈': {'rpprat': 0.27577937649880097,\n",
       "              'rwsrat': 0.6402877697841727},\n",
       "             '청와대': {'rpprat': 0.19744318181818182,\n",
       "              'rwsrat': 0.7386363636363636},\n",
       "             '국뽕': {'rpprat': 0.18922305764411027,\n",
       "              'rwsrat': 0.4298245614035088},\n",
       "             '샤이니': {'rpprat': 0.12035398230088495,\n",
       "              'rwsrat': 0.7592920353982301},\n",
       "             '이국주': {'rpprat': 0.2549800796812749,\n",
       "              'rwsrat': 0.6733067729083665},\n",
       "             '베츙이': {'rpprat': 0.18388791593695272,\n",
       "              'rwsrat': 0.5464098073555166},\n",
       "             '선생님': {'rpprat': 0.5006031363088058,\n",
       "              'rwsrat': 0.4270205066344994},\n",
       "             '좌음': {'rpprat': 0.2602921646746348,\n",
       "              'rwsrat': 0.5232403718459495},\n",
       "             '시크릿': {'rpprat': 0.046762589928057555,\n",
       "              'rwsrat': 0.7158273381294964},\n",
       "             '화웨이': {'rpprat': 0.183206106870229,\n",
       "              'rwsrat': 0.7595419847328244},\n",
       "             '서울대': {'rpprat': 0.06497175141242938,\n",
       "              'rwsrat': 0.617231638418079},\n",
       "             '호주국자': {'rpprat': 0.18181818181818182,\n",
       "              'rwsrat': 0.7692307692307693},\n",
       "             '태영호': {'rpprat': 0.1643835616438356,\n",
       "              'rwsrat': 0.726027397260274},\n",
       "             '약혐': {'rpprat': 0.0047562425683709865,\n",
       "              'rwsrat': 0.6486325802615933},\n",
       "             '충격적': {'rpprat': 0.7717601547388782,\n",
       "              'rwsrat': 0.08897485493230174},\n",
       "             '지마켓': {'rpprat': 0.1346153846153846,\n",
       "              'rwsrat': 0.8205128205128205},\n",
       "             '여배우': {'rpprat': 0.22130177514792898,\n",
       "              'rwsrat': 0.6946745562130178},\n",
       "             '에어팟': {'rpprat': 0.05672268907563025,\n",
       "              'rwsrat': 0.6764705882352942},\n",
       "             '스시녀': {'rpprat': 0.26612021857923496,\n",
       "              'rwsrat': 0.6387978142076502},\n",
       "             '엑소엘': {'rpprat': 0.411214953271028,\n",
       "              'rwsrat': 0.4205607476635514},\n",
       "             '알려준다': {'rpprat': 0.006696428571428571,\n",
       "              'rwsrat': 0.9598214285714286},\n",
       "             '변땅크': {'rpprat': 0.1981981981981982,\n",
       "              'rwsrat': 0.713963963963964},\n",
       "             '유병언': {'rpprat': 0.1488095238095238,\n",
       "              'rwsrat': 0.7797619047619048},\n",
       "             '박사방': {'rpprat': 0.09615384615384616, 'rwsrat': 0.875},\n",
       "             '고소미': {'rpprat': 0.05, 'rwsrat': 0.6326086956521739},\n",
       "             '티아라': {'rpprat': 0.06802721088435375,\n",
       "              'rwsrat': 0.8299319727891157}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_rat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
