{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from string import punctuation\n",
    "\n",
    "# Stopwords 처리\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation))) # punctuation 제거\n",
    "pattern2 = re.compile(r'[^가-힣 ]') # 특수문자, 모음, 숫자, 영어 제거\n",
    "pattern3 = re.compile(r'\\s{2,}') # white space 1개로 바꾸기.\n",
    "\n",
    "class Extracter:\n",
    "    def __init__(self):\n",
    "        self.noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "        \n",
    "    def cleaning(self,text):\n",
    "        return pattern3.sub(' ', \n",
    "                  pattern2.sub('',\n",
    "                   pattern1.sub('', text)))\n",
    "\n",
    "    def extract_nouns(self,df):\n",
    "        tempary = np.linspace(0,1,11)\n",
    "        nouns = [self.noun_extractor.train_extract(_['head'], min_noun_frequency=50) for _ in [df.iloc[math.ceil(len(df)*tempary[_]):math.ceil(len(df)*tempary[_+1])] for _ in range(len(tempary)-1)]]\n",
    "        words = {k:v for i in range(len(nouns)) for k,v in nouns[i].items() if len(k) > 1}\n",
    "        return words\n",
    "\n",
    "    def search_dict(self,nouns):\n",
    "        # 사전 검색 결과 없는 단어 추출\n",
    "        conn = sqlite3.connect('kr_korean.db')\n",
    "        cur = conn.cursor()\n",
    "        data = pd.read_sql('SELECT word FROM kr WHERE part=\"명사\"', conn)\n",
    "        data = ' '.join(data['word'])\n",
    "        return pd.DataFrame([_ for _ in nouns if _[0] not in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 141099 from 125252 sents. mem=0.485 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=532046, mem=0.680 Gb\n",
      "[Noun Extractor] batch prediction was completed for 37314 words\n",
      "[Noun Extractor] checked compounds. discovered 21766 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1249 -> 1244\n",
      "[Noun Extractor] postprocessing ignore_features : 1244 -> 1190\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1190 -> 1182\n",
      "[Noun Extractor] 1182 nouns (21766 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.747 Gb                    \n",
      "[Noun Extractor] 44.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 150363 from 125252 sents. mem=0.795 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=530478, mem=0.897 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39760 words\n",
      "[Noun Extractor] checked compounds. discovered 26243 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1231 -> 1224\n",
      "[Noun Extractor] postprocessing ignore_features : 1224 -> 1175\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1175 -> 1169\n",
      "[Noun Extractor] 1169 nouns (26243 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.897 Gb                    \n",
      "[Noun Extractor] 45.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 151840 from 125253 sents. mem=0.911 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=523703, mem=0.948 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41063 words\n",
      "[Noun Extractor] checked compounds. discovered 26083 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1145 -> 1136\n",
      "[Noun Extractor] postprocessing ignore_features : 1136 -> 1087\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1087 -> 1082\n",
      "[Noun Extractor] 1082 nouns (26083 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.946 Gb                    \n",
      "[Noun Extractor] 45.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 150899 from 125251 sents. mem=0.946 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=507895, mem=0.968 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41424 words\n",
      "[Noun Extractor] checked compounds. discovered 28540 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1175 -> 1168\n",
      "[Noun Extractor] postprocessing ignore_features : 1168 -> 1116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1116 -> 1112\n",
      "[Noun Extractor] 1112 nouns (28540 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.965 Gb                    \n",
      "[Noun Extractor] 46.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 158170 from 125252 sents. mem=0.965 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=512452, mem=1.004 Gb\n",
      "[Noun Extractor] batch prediction was completed for 44871 words\n",
      "[Noun Extractor] checked compounds. discovered 28585 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1122 -> 1115\n",
      "[Noun Extractor] postprocessing ignore_features : 1115 -> 1072\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1072 -> 1066\n",
      "[Noun Extractor] 1066 nouns (28585 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=1.012 Gb                    \n",
      "[Noun Extractor] 44.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 163557 from 125253 sents. mem=1.012 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=517283, mem=1.049 Gb\n",
      "[Noun Extractor] batch prediction was completed for 46858 words\n",
      "[Noun Extractor] checked compounds. discovered 35739 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1030 -> 1024\n",
      "[Noun Extractor] postprocessing ignore_features : 1024 -> 978\n",
      "[Noun Extractor] postprocessing ignore_NJ : 978 -> 973\n",
      "[Noun Extractor] 973 nouns (35739 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=1.044 Gb                    \n",
      "[Noun Extractor] 44.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 125298 from 125252 sents. mem=1.044 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=344537, mem=1.015 Gb\n",
      "[Noun Extractor] batch prediction was completed for 31475 words\n",
      "[Noun Extractor] checked compounds. discovered 14248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 599 -> 599\n",
      "[Noun Extractor] postprocessing ignore_features : 599 -> 568\n",
      "[Noun Extractor] postprocessing ignore_NJ : 568 -> 566\n",
      "[Noun Extractor] 566 nouns (14248 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=1.011 Gb                    \n",
      "[Noun Extractor] 29.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 110018 from 125251 sents. mem=1.011 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=300224, mem=0.999 Gb\n",
      "[Noun Extractor] batch prediction was completed for 25566 words\n",
      "[Noun Extractor] checked compounds. discovered 9529 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 447 -> 447\n",
      "[Noun Extractor] postprocessing ignore_features : 447 -> 423\n",
      "[Noun Extractor] postprocessing ignore_NJ : 423 -> 423\n",
      "[Noun Extractor] 423 nouns (9529 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.975 Gb                    \n",
      "[Noun Extractor] 26.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 99699 from 125252 sents. mem=0.975 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=273114, mem=0.948 Gb\n",
      "[Noun Extractor] batch prediction was completed for 22510 words\n",
      "[Noun Extractor] checked compounds. discovered 7573 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 416 -> 416\n",
      "[Noun Extractor] postprocessing ignore_features : 416 -> 399\n",
      "[Noun Extractor] postprocessing ignore_NJ : 399 -> 399\n",
      "[Noun Extractor] 399 nouns (7573 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.938 Gb                    \n",
      "[Noun Extractor] 24.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 112195 from 125252 sents. mem=0.938 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=313183, mem=0.935 Gb\n",
      "[Noun Extractor] batch prediction was completed for 26687 words\n",
      "[Noun Extractor] checked compounds. discovered 9637 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 478 -> 478\n",
      "[Noun Extractor] postprocessing ignore_features : 478 -> 455\n",
      "[Noun Extractor] postprocessing ignore_NJ : 455 -> 455\n",
      "[Noun Extractor] 455 nouns (9637 compounds) with min frequency=50\n",
      "[Noun Extractor] flushing was done. mem=0.934 Gb                    \n",
      "[Noun Extractor] 26.03 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head',conn)\n",
    "df.drop_duplicates(keep='first')\n",
    "ext = Extracter()\n",
    "df['head'] = df['head'].map(lambda x:ext.cleaning(x))\n",
    "nouns = ext.extract_nouns(df)\n",
    "words = sorted(nouns.items(),key=lambda _:_[1], reverse=True)\n",
    "new_words = ext.search_dict(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
