{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 153629 from 127907 sents. mem=0.497 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556166, mem=0.731 Gb\n",
      "[Noun Extractor] batch prediction was completed for 40185 words\n",
      "[Noun Extractor] checked compounds. discovered 27173 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4608 -> 4576\n",
      "[Noun Extractor] postprocessing ignore_features : 4576 -> 4467\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4467 -> 4430\n",
      "[Noun Extractor] 4430 nouns (27173 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=0.806 Gb                    \n",
      "[Noun Extractor] 61.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 156261 from 127907 sents. mem=0.846 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=547864, mem=0.946 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41507 words\n",
      "[Noun Extractor] checked compounds. discovered 28598 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4606 -> 4568\n",
      "[Noun Extractor] postprocessing ignore_features : 4568 -> 4476\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4476 -> 4450\n",
      "[Noun Extractor] 4450 nouns (28598 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=0.946 Gb                    \n",
      "[Noun Extractor] 62.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 152909 from 127906 sents. mem=0.946 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=522178, mem=0.987 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41573 words\n",
      "[Noun Extractor] checked compounds. discovered 26220 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4392 -> 4347\n",
      "[Noun Extractor] postprocessing ignore_features : 4347 -> 4248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4248 -> 4219\n",
      "[Noun Extractor] 4219 nouns (26220 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=0.973 Gb                    \n",
      "[Noun Extractor] 61.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157855 from 127907 sents. mem=0.973 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=521544, mem=0.992 Gb\n",
      "[Noun Extractor] batch prediction was completed for 43681 words\n",
      "[Noun Extractor] checked compounds. discovered 28034 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4380 -> 4342\n",
      "[Noun Extractor] postprocessing ignore_features : 4342 -> 4247\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4247 -> 4217\n",
      "[Noun Extractor] 4217 nouns (28034 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=0.987 Gb                    \n",
      "[Noun Extractor] 61.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166295 from 127906 sents. mem=0.983 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=529708, mem=1.037 Gb\n",
      "[Noun Extractor] batch prediction was completed for 47892 words\n",
      "[Noun Extractor] checked compounds. discovered 31704 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4263 -> 4226\n",
      "[Noun Extractor] postprocessing ignore_features : 4226 -> 4132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4132 -> 4103\n",
      "[Noun Extractor] 4103 nouns (31704 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.037 Gb                    \n",
      "[Noun Extractor] 61.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 162683 from 127907 sents. mem=1.034 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=513031, mem=1.041 Gb\n",
      "[Noun Extractor] batch prediction was completed for 45035 words\n",
      "[Noun Extractor] checked compounds. discovered 29556 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4235 -> 4208\n",
      "[Noun Extractor] postprocessing ignore_features : 4208 -> 4103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4103 -> 4069\n",
      "[Noun Extractor] 4069 nouns (29556 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.041 Gb                    \n",
      "[Noun Extractor] 59.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157585 from 127907 sents. mem=1.041 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=538505, mem=1.015 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39854 words\n",
      "[Noun Extractor] checked compounds. discovered 23050 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4248 -> 4218\n",
      "[Noun Extractor] postprocessing ignore_features : 4218 -> 4119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4119 -> 4087\n",
      "[Noun Extractor] 4087 nouns (23050 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.015 Gb                    \n",
      "[Noun Extractor] 58.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 148501 from 127906 sents. mem=1.015 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=514069, mem=1.036 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39879 words\n",
      "[Noun Extractor] checked compounds. discovered 24916 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3822 -> 3778\n",
      "[Noun Extractor] postprocessing ignore_features : 3778 -> 3676\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3676 -> 3627\n",
      "[Noun Extractor] 3627 nouns (24916 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.035 Gb                    \n",
      "[Noun Extractor] 62.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 159979 from 127907 sents. mem=1.035 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=510274, mem=1.044 Gb\n",
      "[Noun Extractor] batch prediction was completed for 44551 words\n",
      "[Noun Extractor] checked compounds. discovered 28238 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4277 -> 4238\n",
      "[Noun Extractor] postprocessing ignore_features : 4238 -> 4130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4130 -> 4087\n",
      "[Noun Extractor] 4087 nouns (28238 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.044 Gb                    \n",
      "[Noun Extractor] 58.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166948 from 127906 sents. mem=1.044 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=590217, mem=1.048 Gb\n",
      "[Noun Extractor] batch prediction was completed for 42936 words\n",
      "[Noun Extractor] checked compounds. discovered 25619 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4536 -> 4507\n",
      "[Noun Extractor] postprocessing ignore_features : 4507 -> 4400\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4400 -> 4359\n",
      "[Noun Extractor] 4359 nouns (25619 compounds) with min frequency=12\n",
      "[Noun Extractor] flushing was done. mem=1.048 Gb                    \n",
      "[Noun Extractor] 61.50 % eojeols are covered\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 31\n",
      "all branching entropies was computed # words = 3301\n",
      "all accessor variety was computed # words = 3301\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 89\n",
      "all branching entropies was computed # words = 14382\n",
      "all accessor variety was computed # words = 14382\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 95\n",
      "all branching entropies was computed # words = 15330\n",
      "all accessor variety was computed # words = 15330\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 102\n",
      "all branching entropies was computed # words = 16336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 16336\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 144\n",
      "all branching entropies was computed # words = 24359\n",
      "all accessor variety was computed # words = 24359\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 157\n",
      "all branching entropies was computed # words = 25297\n",
      "all accessor variety was computed # words = 25297\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 162\n",
      "all branching entropies was computed # words = 25586\n",
      "all accessor variety was computed # words = 25586\n",
      "training was done. used memory 1.048 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 165\n",
      "all branching entropies was computed # words = 25784\n",
      "all accessor variety was computed # words = 25784\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 166\n",
      "all branching entropies was computed # words = 26115\n",
      "all accessor variety was computed # words = 26115\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 170\n",
      "all branching entropies was computed # words = 26416\n",
      "all accessor variety was computed # words = 26416\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 213\n",
      "all branching entropies was computed # words = 28856\n",
      "all accessor variety was computed # words = 28856\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 214\n",
      "all branching entropies was computed # words = 29271\n",
      "all accessor variety was computed # words = 29271\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 223\n",
      "all branching entropies was computed # words = 29782\n",
      "all accessor variety was computed # words = 29782\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 226\n",
      "all branching entropies was computed # words = 30117\n",
      "all accessor variety was computed # words = 30117\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 229\n",
      "all branching entropies was computed # words = 31267\n",
      "all accessor variety was computed # words = 31267\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 234\n",
      "all branching entropies was computed # words = 31336\n",
      "all accessor variety was computed # words = 31336\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 237\n",
      "all branching entropies was computed # words = 31392\n",
      "all accessor variety was computed # words = 31392\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 241\n",
      "all branching entropies was computed # words = 31723\n",
      "all accessor variety was computed # words = 31723\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 254\n",
      "all branching entropies was computed # words = 32433\n",
      "all accessor variety was computed # words = 32433\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 254\n",
      "all branching entropies was computed # words = 32489\n",
      "all accessor variety was computed # words = 32489\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 258\n",
      "all branching entropies was computed # words = 32680\n",
      "all accessor variety was computed # words = 32680\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 265\n",
      "all branching entropies was computed # words = 33064\n",
      "all accessor variety was computed # words = 33064\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 268\n",
      "all branching entropies was computed # words = 33142\n",
      "all accessor variety was computed # words = 33142\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 273\n",
      "all branching entropies was computed # words = 33285\n",
      "all accessor variety was computed # words = 33285\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 283\n",
      "all branching entropies was computed # words = 34980\n",
      "all accessor variety was computed # words = 34980\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 286\n",
      "all branching entropies was computed # words = 34993\n",
      "all accessor variety was computed # words = 34993\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 289\n",
      "all branching entropies was computed # words = 35202\n",
      "all accessor variety was computed # words = 35202\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 292\n",
      "all branching entropies was computed # words = 35295\n",
      "all accessor variety was computed # words = 35295\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 292\n",
      "all branching entropies was computed # words = 35703\n",
      "all accessor variety was computed # words = 35703\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 296\n",
      "all branching entropies was computed # words = 35881\n",
      "all accessor variety was computed # words = 35881\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 301\n",
      "all branching entropies was computed # words = 36095\n",
      "all accessor variety was computed # words = 36095\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 304\n",
      "all branching entropies was computed # words = 36152\n",
      "all accessor variety was computed # words = 36152\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 309\n",
      "all branching entropies was computed # words = 36204\n",
      "all accessor variety was computed # words = 36204\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 314\n",
      "all branching entropies was computed # words = 36376\n",
      "all accessor variety was computed # words = 36376\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 317\n",
      "all branching entropies was computed # words = 36495\n",
      "all accessor variety was computed # words = 36495\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 322\n",
      "all branching entropies was computed # words = 36527\n",
      "all accessor variety was computed # words = 36527\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 327\n",
      "all branching entropies was computed # words = 36801\n",
      "all accessor variety was computed # words = 36801\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 342\n",
      "all branching entropies was computed # words = 37226\n",
      "all accessor variety was computed # words = 37226\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 345\n",
      "all branching entropies was computed # words = 37271\n",
      "all accessor variety was computed # words = 37271\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 360\n",
      "all branching entropies was computed # words = 38018\n",
      "all accessor variety was computed # words = 38018\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 363\n",
      "all branching entropies was computed # words = 38235\n",
      "all accessor variety was computed # words = 38235\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 372\n",
      "all branching entropies was computed # words = 38665\n",
      "all accessor variety was computed # words = 38665\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 374\n",
      "all branching entropies was computed # words = 38912\n",
      "all accessor variety was computed # words = 38912\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 377\n",
      "all branching entropies was computed # words = 38939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 38939\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 382\n",
      "all branching entropies was computed # words = 39014\n",
      "all accessor variety was computed # words = 39014\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 387\n",
      "all branching entropies was computed # words = 39056\n",
      "all accessor variety was computed # words = 39056\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 390\n",
      "all branching entropies was computed # words = 39094\n",
      "all accessor variety was computed # words = 39094\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 393\n",
      "all branching entropies was computed # words = 39124\n",
      "all accessor variety was computed # words = 39124\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 394\n",
      "all branching entropies was computed # words = 39216\n",
      "all accessor variety was computed # words = 39216\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 399\n",
      "all branching entropies was computed # words = 40285\n",
      "all accessor variety was computed # words = 40285\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 402\n",
      "all branching entropies was computed # words = 40347\n",
      "all accessor variety was computed # words = 40347\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 417\n",
      "all branching entropies was computed # words = 41171\n",
      "all accessor variety was computed # words = 41171\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 420\n",
      "all branching entropies was computed # words = 41290\n",
      "all accessor variety was computed # words = 41290\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 420\n",
      "all branching entropies was computed # words = 41419\n",
      "all accessor variety was computed # words = 41419\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 428\n",
      "all branching entropies was computed # words = 41840\n",
      "all accessor variety was computed # words = 41840\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 431\n",
      "all branching entropies was computed # words = 42052\n",
      "all accessor variety was computed # words = 42052\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 436\n",
      "all branching entropies was computed # words = 42090\n",
      "all accessor variety was computed # words = 42090\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 440\n",
      "all branching entropies was computed # words = 42162\n",
      "all accessor variety was computed # words = 42162\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 445\n",
      "all branching entropies was computed # words = 42232\n",
      "all accessor variety was computed # words = 42232\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 446\n",
      "all branching entropies was computed # words = 42412\n",
      "all accessor variety was computed # words = 42412\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 452\n",
      "all branching entropies was computed # words = 42545\n",
      "all accessor variety was computed # words = 42545\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 457\n",
      "all branching entropies was computed # words = 42688\n",
      "all accessor variety was computed # words = 42688\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 463\n",
      "all branching entropies was computed # words = 43009\n",
      "all accessor variety was computed # words = 43009\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 467\n",
      "all branching entropies was computed # words = 43083\n",
      "all accessor variety was computed # words = 43083\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 470\n",
      "all branching entropies was computed # words = 43114\n",
      "all accessor variety was computed # words = 43114\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 478\n",
      "all branching entropies was computed # words = 43145\n",
      "all accessor variety was computed # words = 43145\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 485\n",
      "all branching entropies was computed # words = 43278\n",
      "all accessor variety was computed # words = 43278\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 488\n",
      "all branching entropies was computed # words = 43319\n",
      "all accessor variety was computed # words = 43319\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 491\n",
      "all branching entropies was computed # words = 43337\n",
      "all accessor variety was computed # words = 43337\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 494\n",
      "all branching entropies was computed # words = 43399\n",
      "all accessor variety was computed # words = 43399\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 497\n",
      "all branching entropies was computed # words = 43432\n",
      "all accessor variety was computed # words = 43432\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 497\n",
      "all branching entropies was computed # words = 43447\n",
      "all accessor variety was computed # words = 43447\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 498\n",
      "all branching entropies was computed # words = 43535\n",
      "all accessor variety was computed # words = 43535\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 500\n",
      "all branching entropies was computed # words = 43567\n",
      "all accessor variety was computed # words = 43567\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 504\n",
      "all branching entropies was computed # words = 43582\n",
      "all accessor variety was computed # words = 43582\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 509\n",
      "all branching entropies was computed # words = 43623\n",
      "all accessor variety was computed # words = 43623\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 514\n",
      "all branching entropies was computed # words = 43710\n",
      "all accessor variety was computed # words = 43710\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 517\n",
      "all branching entropies was computed # words = 43776\n",
      "all accessor variety was computed # words = 43776\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 524\n",
      "all branching entropies was computed # words = 44076\n",
      "all accessor variety was computed # words = 44076\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 528\n",
      "all branching entropies was computed # words = 44097\n",
      "all accessor variety was computed # words = 44097\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 534\n",
      "all branching entropies was computed # words = 44490\n",
      "all accessor variety was computed # words = 44490\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 537\n",
      "all branching entropies was computed # words = 44499\n",
      "all accessor variety was computed # words = 44499\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 540\n",
      "all branching entropies was computed # words = 44574\n",
      "all accessor variety was computed # words = 44574\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 543\n",
      "all branching entropies was computed # words = 44591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 44591\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 546\n",
      "all branching entropies was computed # words = 44683\n",
      "all accessor variety was computed # words = 44683\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 551\n",
      "all branching entropies was computed # words = 44706\n",
      "all accessor variety was computed # words = 44706\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 551\n",
      "all branching entropies was computed # words = 45024\n",
      "all accessor variety was computed # words = 45024\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 552\n",
      "all branching entropies was computed # words = 45102\n",
      "all accessor variety was computed # words = 45102\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 555\n",
      "all branching entropies was computed # words = 45203\n",
      "all accessor variety was computed # words = 45203\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 560\n",
      "all branching entropies was computed # words = 45338\n",
      "all accessor variety was computed # words = 45338\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 564\n",
      "all branching entropies was computed # words = 45433\n",
      "all accessor variety was computed # words = 45433\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 567\n",
      "all branching entropies was computed # words = 45480\n",
      "all accessor variety was computed # words = 45480\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 572\n",
      "all branching entropies was computed # words = 45521\n",
      "all accessor variety was computed # words = 45521\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 580\n",
      "all branching entropies was computed # words = 45541\n",
      "all accessor variety was computed # words = 45541\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 581\n",
      "all branching entropies was computed # words = 45633\n",
      "all accessor variety was computed # words = 45633\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 582\n",
      "all branching entropies was computed # words = 45653\n",
      "all accessor variety was computed # words = 45653\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 585\n",
      "all branching entropies was computed # words = 45701\n",
      "all accessor variety was computed # words = 45701\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 588\n",
      "all branching entropies was computed # words = 45725\n",
      "all accessor variety was computed # words = 45725\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 589\n",
      "all branching entropies was computed # words = 45972\n",
      "all accessor variety was computed # words = 45972\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 591\n",
      "all branching entropies was computed # words = 46129\n",
      "all accessor variety was computed # words = 46129\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 596\n",
      "all branching entropies was computed # words = 46167\n",
      "all accessor variety was computed # words = 46167\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 597\n",
      "all branching entropies was computed # words = 46218\n",
      "all accessor variety was computed # words = 46218\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 600\n",
      "all branching entropies was computed # words = 46293\n",
      "all accessor variety was computed # words = 46293\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 604\n",
      "all branching entropies was computed # words = 46546\n",
      "all accessor variety was computed # words = 46546\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 610\n",
      "all branching entropies was computed # words = 46628\n",
      "all accessor variety was computed # words = 46628\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 613\n",
      "all branching entropies was computed # words = 46645\n",
      "all accessor variety was computed # words = 46645\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 616\n",
      "all branching entropies was computed # words = 46665\n",
      "all accessor variety was computed # words = 46665\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 623\n",
      "all branching entropies was computed # words = 46723\n",
      "all accessor variety was computed # words = 46723\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 627\n",
      "all branching entropies was computed # words = 46781\n",
      "all accessor variety was computed # words = 46781\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 632\n",
      "all branching entropies was computed # words = 46801\n",
      "all accessor variety was computed # words = 46801\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 635\n",
      "all branching entropies was computed # words = 46869\n",
      "all accessor variety was computed # words = 46869\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 638\n",
      "all branching entropies was computed # words = 46893\n",
      "all accessor variety was computed # words = 46893\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 641\n",
      "all branching entropies was computed # words = 46920\n",
      "all accessor variety was computed # words = 46920\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 644\n",
      "all branching entropies was computed # words = 47023\n",
      "all accessor variety was computed # words = 47023\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 651\n",
      "all branching entropies was computed # words = 47108\n",
      "all accessor variety was computed # words = 47108\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 651\n",
      "all branching entropies was computed # words = 47145\n",
      "all accessor variety was computed # words = 47145\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 661\n",
      "all branching entropies was computed # words = 47244\n",
      "all accessor variety was computed # words = 47244\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 664\n",
      "all branching entropies was computed # words = 47272\n",
      "all accessor variety was computed # words = 47272\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 672\n",
      "all branching entropies was computed # words = 47541\n",
      "all accessor variety was computed # words = 47541\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 675\n",
      "all branching entropies was computed # words = 47572\n",
      "all accessor variety was computed # words = 47572\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 678\n",
      "all branching entropies was computed # words = 48354\n",
      "all accessor variety was computed # words = 48354\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 691\n",
      "all branching entropies was computed # words = 48587\n",
      "all accessor variety was computed # words = 48587\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 696\n",
      "all branching entropies was computed # words = 48749\n",
      "all accessor variety was computed # words = 48749\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 699\n",
      "all branching entropies was computed # words = 48828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 48828\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 710\n",
      "all branching entropies was computed # words = 49277\n",
      "all accessor variety was computed # words = 49277\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 711\n",
      "all branching entropies was computed # words = 49312\n",
      "all accessor variety was computed # words = 49312\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 713\n",
      "all branching entropies was computed # words = 49320\n",
      "all accessor variety was computed # words = 49320\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 715\n",
      "all branching entropies was computed # words = 49332\n",
      "all accessor variety was computed # words = 49332\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 715\n",
      "all branching entropies was computed # words = 49340\n",
      "all accessor variety was computed # words = 49340\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 720\n",
      "all branching entropies was computed # words = 49410\n",
      "all accessor variety was computed # words = 49410\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 725\n",
      "all branching entropies was computed # words = 49473\n",
      "all accessor variety was computed # words = 49473\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 732\n",
      "all branching entropies was computed # words = 49542\n",
      "all accessor variety was computed # words = 49542\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 737\n",
      "all branching entropies was computed # words = 49599\n",
      "all accessor variety was computed # words = 49599\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 740\n",
      "all branching entropies was computed # words = 49679\n",
      "all accessor variety was computed # words = 49679\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 743\n",
      "all branching entropies was computed # words = 49759\n",
      "all accessor variety was computed # words = 49759\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 746\n",
      "all branching entropies was computed # words = 49783\n",
      "all accessor variety was computed # words = 49783\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 749\n",
      "all branching entropies was computed # words = 49815\n",
      "all accessor variety was computed # words = 49815\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 751\n",
      "all branching entropies was computed # words = 49875\n",
      "all accessor variety was computed # words = 49875\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 754\n",
      "all branching entropies was computed # words = 49923\n",
      "all accessor variety was computed # words = 49923\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 756\n",
      "all branching entropies was computed # words = 49952\n",
      "all accessor variety was computed # words = 49952\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 761\n",
      "all branching entropies was computed # words = 50080\n",
      "all accessor variety was computed # words = 50080\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 764\n",
      "all branching entropies was computed # words = 50105\n",
      "all accessor variety was computed # words = 50105\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 767\n",
      "all branching entropies was computed # words = 50161\n",
      "all accessor variety was computed # words = 50161\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 770\n",
      "all branching entropies was computed # words = 50190\n",
      "all accessor variety was computed # words = 50190\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 772\n",
      "all branching entropies was computed # words = 50224\n",
      "all accessor variety was computed # words = 50224\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 775\n",
      "all branching entropies was computed # words = 50235\n",
      "all accessor variety was computed # words = 50235\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 779\n",
      "all branching entropies was computed # words = 50274\n",
      "all accessor variety was computed # words = 50274\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 780\n",
      "all branching entropies was computed # words = 50433\n",
      "all accessor variety was computed # words = 50433\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 783\n",
      "all branching entropies was computed # words = 50459\n",
      "all accessor variety was computed # words = 50459\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 785\n",
      "all branching entropies was computed # words = 50465\n",
      "all accessor variety was computed # words = 50465\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 789\n",
      "all branching entropies was computed # words = 50483\n",
      "all accessor variety was computed # words = 50483\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 791\n",
      "all branching entropies was computed # words = 50532\n",
      "all accessor variety was computed # words = 50532\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 797\n",
      "all branching entropies was computed # words = 50560\n",
      "all accessor variety was computed # words = 50560\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 802\n",
      "all branching entropies was computed # words = 50685\n",
      "all accessor variety was computed # words = 50685\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 805\n",
      "all branching entropies was computed # words = 50708\n",
      "all accessor variety was computed # words = 50708\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 810\n",
      "all branching entropies was computed # words = 50787\n",
      "all accessor variety was computed # words = 50787\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 813\n",
      "all branching entropies was computed # words = 50796\n",
      "all accessor variety was computed # words = 50796\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 816\n",
      "all branching entropies was computed # words = 50801\n",
      "all accessor variety was computed # words = 50801\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 819\n",
      "all branching entropies was computed # words = 50835\n",
      "all accessor variety was computed # words = 50835\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 822\n",
      "all branching entropies was computed # words = 50864\n",
      "all accessor variety was computed # words = 50864\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 825\n",
      "all branching entropies was computed # words = 50992\n",
      "all accessor variety was computed # words = 50992\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 826\n",
      "all branching entropies was computed # words = 51031\n",
      "all accessor variety was computed # words = 51031\n",
      "'그런남자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 834\n",
      "all branching entropies was computed # words = 51077\n",
      "all accessor variety was computed # words = 51077\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 51277\n",
      "all accessor variety was computed # words = 51277\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 840\n",
      "all branching entropies was computed # words = 51282\n",
      "all accessor variety was computed # words = 51282\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 843\n",
      "all branching entropies was computed # words = 51335\n",
      "all accessor variety was computed # words = 51335\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 846\n",
      "all branching entropies was computed # words = 51376\n",
      "all accessor variety was computed # words = 51376\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 849\n",
      "all branching entropies was computed # words = 51398\n",
      "all accessor variety was computed # words = 51398\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 858\n",
      "all branching entropies was computed # words = 51424\n",
      "all accessor variety was computed # words = 51424\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 862\n",
      "all branching entropies was computed # words = 51683\n",
      "all accessor variety was computed # words = 51683\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 864\n",
      "all branching entropies was computed # words = 51733\n",
      "all accessor variety was computed # words = 51733\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 866\n",
      "all branching entropies was computed # words = 51736\n",
      "all accessor variety was computed # words = 51736\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 868\n",
      "all branching entropies was computed # words = 51906\n",
      "all accessor variety was computed # words = 51906\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 873\n",
      "all branching entropies was computed # words = 51932\n",
      "all accessor variety was computed # words = 51932\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 874\n",
      "all branching entropies was computed # words = 52001\n",
      "all accessor variety was computed # words = 52001\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 875\n",
      "all branching entropies was computed # words = 52152\n",
      "all accessor variety was computed # words = 52152\n",
      "'스벅'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 881\n",
      "all branching entropies was computed # words = 52194\n",
      "all accessor variety was computed # words = 52194\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 886\n",
      "all branching entropies was computed # words = 52228\n",
      "all accessor variety was computed # words = 52228\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 886\n",
      "all branching entropies was computed # words = 52235\n",
      "all accessor variety was computed # words = 52235\n",
      "'김요한'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 888\n",
      "all branching entropies was computed # words = 52241\n",
      "all accessor variety was computed # words = 52241\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 891\n",
      "all branching entropies was computed # words = 52303\n",
      "all accessor variety was computed # words = 52303\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 894\n",
      "all branching entropies was computed # words = 52362\n",
      "all accessor variety was computed # words = 52362\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 896\n",
      "all branching entropies was computed # words = 52376\n",
      "all accessor variety was computed # words = 52376\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 896\n",
      "all branching entropies was computed # words = 52385\n",
      "all accessor variety was computed # words = 52385\n",
      "'동태탕'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 901\n",
      "all branching entropies was computed # words = 52420\n",
      "all accessor variety was computed # words = 52420\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 906\n",
      "all branching entropies was computed # words = 52512\n",
      "all accessor variety was computed # words = 52512\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 909\n",
      "all branching entropies was computed # words = 52575\n",
      "all accessor variety was computed # words = 52575\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 911\n",
      "all branching entropies was computed # words = 52581\n",
      "all accessor variety was computed # words = 52581\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 911\n",
      "all branching entropies was computed # words = 52587\n",
      "all accessor variety was computed # words = 52587\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 911\n",
      "all branching entropies was computed # words = 52593\n",
      "all accessor variety was computed # words = 52593\n",
      "'박우진'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 911\n",
      "all branching entropies was computed # words = 52593\n",
      "all accessor variety was computed # words = 52593\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 915\n",
      "all branching entropies was computed # words = 52607\n",
      "all accessor variety was computed # words = 52607\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 917\n",
      "all branching entropies was computed # words = 52695\n",
      "all accessor variety was computed # words = 52695\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 921\n",
      "all branching entropies was computed # words = 52962\n",
      "all accessor variety was computed # words = 52962\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 924\n",
      "all branching entropies was computed # words = 53036\n",
      "all accessor variety was computed # words = 53036\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 924\n",
      "all branching entropies was computed # words = 53045\n",
      "all accessor variety was computed # words = 53045\n",
      "'오방관'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 929\n",
      "all branching entropies was computed # words = 53089\n",
      "all accessor variety was computed # words = 53089\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 932\n",
      "all branching entropies was computed # words = 53151\n",
      "all accessor variety was computed # words = 53151\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 935\n",
      "all branching entropies was computed # words = 53196\n",
      "all accessor variety was computed # words = 53196\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 937\n",
      "all branching entropies was computed # words = 53225\n",
      "all accessor variety was computed # words = 53225\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 937\n",
      "all branching entropies was computed # words = 53243\n",
      "all accessor variety was computed # words = 53243\n",
      "'빅톤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 944\n",
      "all branching entropies was computed # words = 53252\n",
      "all accessor variety was computed # words = 53252\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 947\n",
      "all branching entropies was computed # words = 53280\n",
      "all accessor variety was computed # words = 53280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 947\n",
      "all branching entropies was computed # words = 53280\n",
      "all accessor variety was computed # words = 53280\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 948\n",
      "all branching entropies was computed # words = 53393\n",
      "all accessor variety was computed # words = 53393\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 953\n",
      "all branching entropies was computed # words = 53457\n",
      "all accessor variety was computed # words = 53457\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 955\n",
      "all branching entropies was computed # words = 53468\n",
      "all accessor variety was computed # words = 53468\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 958\n",
      "all branching entropies was computed # words = 53580\n",
      "all accessor variety was computed # words = 53580\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 961\n",
      "all branching entropies was computed # words = 53610\n",
      "all accessor variety was computed # words = 53610\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 966\n",
      "all branching entropies was computed # words = 53630\n",
      "all accessor variety was computed # words = 53630\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 968\n",
      "all branching entropies was computed # words = 53640\n",
      "all accessor variety was computed # words = 53640\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 969\n",
      "all branching entropies was computed # words = 53667\n",
      "all accessor variety was computed # words = 53667\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 974\n",
      "all branching entropies was computed # words = 53706\n",
      "all accessor variety was computed # words = 53706\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 977\n",
      "all branching entropies was computed # words = 53833\n",
      "all accessor variety was computed # words = 53833\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 980\n",
      "all branching entropies was computed # words = 53864\n",
      "all accessor variety was computed # words = 53864\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 988\n",
      "all branching entropies was computed # words = 54053\n",
      "all accessor variety was computed # words = 54053\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 990\n",
      "all branching entropies was computed # words = 54062\n",
      "all accessor variety was computed # words = 54062\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 995\n",
      "all branching entropies was computed # words = 54159\n",
      "all accessor variety was computed # words = 54159\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 997\n",
      "all branching entropies was computed # words = 54186\n",
      "all accessor variety was computed # words = 54186\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 997\n",
      "all branching entropies was computed # words = 54196\n",
      "all accessor variety was computed # words = 54196\n",
      "'인간수업'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 997\n",
      "all branching entropies was computed # words = 54205\n",
      "all accessor variety was computed # words = 54205\n",
      "'메갈리안'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1000\n",
      "all branching entropies was computed # words = 54219\n",
      "all accessor variety was computed # words = 54219\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1000\n",
      "all branching entropies was computed # words = 54220\n",
      "all accessor variety was computed # words = 54220\n",
      "'확진자수'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1003\n",
      "all branching entropies was computed # words = 54247\n",
      "all accessor variety was computed # words = 54247\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1006\n",
      "all branching entropies was computed # words = 54270\n",
      "all accessor variety was computed # words = 54270\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1019\n",
      "all branching entropies was computed # words = 54340\n",
      "all accessor variety was computed # words = 54340\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1021\n",
      "all branching entropies was computed # words = 54346\n",
      "all accessor variety was computed # words = 54346\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1021\n",
      "all branching entropies was computed # words = 54359\n",
      "all accessor variety was computed # words = 54359\n",
      "'서영교'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1022\n",
      "all branching entropies was computed # words = 54397\n",
      "all accessor variety was computed # words = 54397\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1022\n",
      "all branching entropies was computed # words = 54416\n",
      "all accessor variety was computed # words = 54416\n",
      "'워너블'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1024\n",
      "all branching entropies was computed # words = 54437\n",
      "all accessor variety was computed # words = 54437\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1024\n",
      "all branching entropies was computed # words = 54443\n",
      "all accessor variety was computed # words = 54443\n",
      "'변서은'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1025\n",
      "all branching entropies was computed # words = 54534\n",
      "all accessor variety was computed # words = 54534\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1025\n",
      "all branching entropies was computed # words = 54538\n",
      "all accessor variety was computed # words = 54538\n",
      "'남도현'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1025\n",
      "all branching entropies was computed # words = 54542\n",
      "all accessor variety was computed # words = 54542\n",
      "'홍땅크'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1031\n",
      "all branching entropies was computed # words = 55376\n",
      "all accessor variety was computed # words = 55376\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1033\n",
      "all branching entropies was computed # words = 55423\n",
      "all accessor variety was computed # words = 55423\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1033\n",
      "all branching entropies was computed # words = 55433\n",
      "all accessor variety was computed # words = 55433\n",
      "'현송월'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1036\n",
      "all branching entropies was computed # words = 55460\n",
      "all accessor variety was computed # words = 55460\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1039\n",
      "all branching entropies was computed # words = 55480\n",
      "all accessor variety was computed # words = 55480\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1044\n",
      "all branching entropies was computed # words = 55508\n",
      "all accessor variety was computed # words = 55508\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1047\n",
      "all branching entropies was computed # words = 55604\n",
      "all accessor variety was computed # words = 55604\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 55627\n",
      "all accessor variety was computed # words = 55627\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1054\n",
      "all branching entropies was computed # words = 55765\n",
      "all accessor variety was computed # words = 55765\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1054\n",
      "all branching entropies was computed # words = 55771\n",
      "all accessor variety was computed # words = 55771\n",
      "'재기찡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1059\n",
      "all branching entropies was computed # words = 55812\n",
      "all accessor variety was computed # words = 55812\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1059\n",
      "all branching entropies was computed # words = 55813\n",
      "all accessor variety was computed # words = 55813\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1059\n",
      "all branching entropies was computed # words = 55903\n",
      "all accessor variety was computed # words = 55903\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1060\n",
      "all branching entropies was computed # words = 55929\n",
      "all accessor variety was computed # words = 55929\n",
      "'마닷'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1061\n",
      "all branching entropies was computed # words = 55942\n",
      "all accessor variety was computed # words = 55942\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1064\n",
      "all branching entropies was computed # words = 56067\n",
      "all accessor variety was computed # words = 56067\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1067\n",
      "all branching entropies was computed # words = 56091\n",
      "all accessor variety was computed # words = 56091\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1069\n",
      "all branching entropies was computed # words = 56344\n",
      "all accessor variety was computed # words = 56344\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1070\n",
      "all branching entropies was computed # words = 56351\n",
      "all accessor variety was computed # words = 56351\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1071\n",
      "all branching entropies was computed # words = 56400\n",
      "all accessor variety was computed # words = 56400\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1074\n",
      "all branching entropies was computed # words = 56413\n",
      "all accessor variety was computed # words = 56413\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1079\n",
      "all branching entropies was computed # words = 56784\n",
      "all accessor variety was computed # words = 56784\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1079\n",
      "all branching entropies was computed # words = 56805\n",
      "all accessor variety was computed # words = 56805\n",
      "'리퍼트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1086\n",
      "all branching entropies was computed # words = 56831\n",
      "all accessor variety was computed # words = 56831\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1092\n",
      "all branching entropies was computed # words = 56896\n",
      "all accessor variety was computed # words = 56896\n",
      "training was done. used memory 1.057 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1100\n",
      "all branching entropies was computed # words = 56952\n",
      "all accessor variety was computed # words = 56952\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 57052\n",
      "all accessor variety was computed # words = 57052\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 57056\n",
      "all accessor variety was computed # words = 57056\n",
      "'신태일'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 57067\n",
      "all accessor variety was computed # words = 57067\n",
      "'김기식'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1111\n",
      "all branching entropies was computed # words = 57072\n",
      "all accessor variety was computed # words = 57072\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1113\n",
      "all branching entropies was computed # words = 57091\n",
      "all accessor variety was computed # words = 57091\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1117\n",
      "all branching entropies was computed # words = 57117\n",
      "all accessor variety was computed # words = 57117\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1119\n",
      "all branching entropies was computed # words = 57146\n",
      "all accessor variety was computed # words = 57146\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1123\n",
      "all branching entropies was computed # words = 57321\n",
      "all accessor variety was computed # words = 57321\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1123\n",
      "all branching entropies was computed # words = 57334\n",
      "all accessor variety was computed # words = 57334\n",
      "'대하이햄'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1127\n",
      "all branching entropies was computed # words = 57378\n",
      "all accessor variety was computed # words = 57378\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1127\n",
      "all branching entropies was computed # words = 57387\n",
      "all accessor variety was computed # words = 57387\n",
      "'리섭'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1127\n",
      "all branching entropies was computed # words = 57391\n",
      "all accessor variety was computed # words = 57391\n",
      "'김아랑'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1131\n",
      "all branching entropies was computed # words = 57410\n",
      "all accessor variety was computed # words = 57410\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1132\n",
      "all branching entropies was computed # words = 57429\n",
      "all accessor variety was computed # words = 57429\n",
      "'중궈'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1136\n",
      "all branching entropies was computed # words = 57478\n",
      "all accessor variety was computed # words = 57478\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1136\n",
      "all branching entropies was computed # words = 57495\n",
      "all accessor variety was computed # words = 57495\n",
      "'샛별이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1137\n",
      "all branching entropies was computed # words = 57576\n",
      "all accessor variety was computed # words = 57576\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1139\n",
      "all branching entropies was computed # words = 57582\n",
      "all accessor variety was computed # words = 57582\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1139\n",
      "all branching entropies was computed # words = 57593\n",
      "all accessor variety was computed # words = 57593\n",
      "'우민끼'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1142\n",
      "all branching entropies was computed # words = 57640\n",
      "all accessor variety was computed # words = 57640\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1144\n",
      "all branching entropies was computed # words = 57686\n",
      "all accessor variety was computed # words = 57686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1147\n",
      "all branching entropies was computed # words = 57728\n",
      "all accessor variety was computed # words = 57728\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1150\n",
      "all branching entropies was computed # words = 57748\n",
      "all accessor variety was computed # words = 57748\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1156\n",
      "all branching entropies was computed # words = 58091\n",
      "all accessor variety was computed # words = 58091\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1159\n",
      "all branching entropies was computed # words = 58132\n",
      "all accessor variety was computed # words = 58132\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1162\n",
      "all branching entropies was computed # words = 58217\n",
      "all accessor variety was computed # words = 58217\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1162\n",
      "all branching entropies was computed # words = 58227\n",
      "all accessor variety was computed # words = 58227\n",
      "'타임스퀘어'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1164\n",
      "all branching entropies was computed # words = 58252\n",
      "all accessor variety was computed # words = 58252\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1164\n",
      "all branching entropies was computed # words = 58265\n",
      "all accessor variety was computed # words = 58265\n",
      "'워크맨'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1167\n",
      "all branching entropies was computed # words = 58442\n",
      "all accessor variety was computed # words = 58442\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1171\n",
      "all branching entropies was computed # words = 58469\n",
      "all accessor variety was computed # words = 58469\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1174\n",
      "all branching entropies was computed # words = 58536\n",
      "all accessor variety was computed # words = 58536\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1180\n",
      "all branching entropies was computed # words = 58595\n",
      "all accessor variety was computed # words = 58595\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1183\n",
      "all branching entropies was computed # words = 58699\n",
      "all accessor variety was computed # words = 58699\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1187\n",
      "all branching entropies was computed # words = 58723\n",
      "all accessor variety was computed # words = 58723\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1187\n",
      "all branching entropies was computed # words = 58741\n",
      "all accessor variety was computed # words = 58741\n",
      "'플레디스'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1187\n",
      "all branching entropies was computed # words = 58745\n",
      "all accessor variety was computed # words = 58745\n",
      "'주학년'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1189\n",
      "all branching entropies was computed # words = 58756\n",
      "all accessor variety was computed # words = 58756\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1194\n",
      "all branching entropies was computed # words = 59006\n",
      "all accessor variety was computed # words = 59006\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1194\n",
      "all branching entropies was computed # words = 59021\n",
      "all accessor variety was computed # words = 59021\n",
      "'송형준'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1197\n",
      "all branching entropies was computed # words = 59027\n",
      "all accessor variety was computed # words = 59027\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1200\n",
      "all branching entropies was computed # words = 59035\n",
      "all accessor variety was computed # words = 59035\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1203\n",
      "all branching entropies was computed # words = 59069\n",
      "all accessor variety was computed # words = 59069\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1203\n",
      "all branching entropies was computed # words = 59072\n",
      "all accessor variety was computed # words = 59072\n",
      "'핑코게이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1208\n",
      "all branching entropies was computed # words = 59114\n",
      "all accessor variety was computed # words = 59114\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1210\n",
      "all branching entropies was computed # words = 59187\n",
      "all accessor variety was computed # words = 59187\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1211\n",
      "all branching entropies was computed # words = 59208\n",
      "all accessor variety was computed # words = 59208\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1214\n",
      "all branching entropies was computed # words = 59262\n",
      "all accessor variety was computed # words = 59262\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1217\n",
      "all branching entropies was computed # words = 59293\n",
      "all accessor variety was computed # words = 59293\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1217\n",
      "all branching entropies was computed # words = 59298\n",
      "all accessor variety was computed # words = 59298\n",
      "'새무현운동'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1220\n",
      "all branching entropies was computed # words = 59374\n",
      "all accessor variety was computed # words = 59374\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1220\n",
      "all branching entropies was computed # words = 59377\n",
      "all accessor variety was computed # words = 59377\n",
      "'아몰랑'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1225\n",
      "all branching entropies was computed # words = 59420\n",
      "all accessor variety was computed # words = 59420\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1230\n",
      "all branching entropies was computed # words = 59466\n",
      "all accessor variety was computed # words = 59466\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1230\n",
      "all branching entropies was computed # words = 59472\n",
      "all accessor variety was computed # words = 59472\n",
      "'아이오아이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1235\n",
      "all branching entropies was computed # words = 59485\n",
      "all accessor variety was computed # words = 59485\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1235\n",
      "all branching entropies was computed # words = 59498\n",
      "all accessor variety was computed # words = 59498\n",
      "'스카이캐슬'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1235\n",
      "all branching entropies was computed # words = 59504\n",
      "all accessor variety was computed # words = 59504\n",
      "'국대떡볶이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1235\n",
      "all branching entropies was computed # words = 59521\n",
      "all accessor variety was computed # words = 59521\n",
      "'마이크로닷'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1240\n",
      "all branching entropies was computed # words = 59527\n",
      "all accessor variety was computed # words = 59527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59531\n",
      "all accessor variety was computed # words = 59531\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59543\n",
      "all accessor variety was computed # words = 59543\n",
      "'명현만'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59570\n",
      "all accessor variety was computed # words = 59570\n",
      "'홍혜걸'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59584\n",
      "all accessor variety was computed # words = 59584\n",
      "'테러방지법'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59596\n",
      "all accessor variety was computed # words = 59596\n",
      "'레진코믹스'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1243\n",
      "all branching entropies was computed # words = 59609\n",
      "all accessor variety was computed # words = 59609\n",
      "'포스트잇'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1246\n",
      "all branching entropies was computed # words = 59649\n",
      "all accessor variety was computed # words = 59649\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1246\n",
      "all branching entropies was computed # words = 59664\n",
      "all accessor variety was computed # words = 59664\n",
      "'크루즈선'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1251\n",
      "all branching entropies was computed # words = 59698\n",
      "all accessor variety was computed # words = 59698\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1251\n",
      "all branching entropies was computed # words = 59711\n",
      "all accessor variety was computed # words = 59711\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1262\n",
      "all branching entropies was computed # words = 60214\n",
      "all accessor variety was computed # words = 60214\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1265\n",
      "all branching entropies was computed # words = 60250\n",
      "all accessor variety was computed # words = 60250\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1265\n",
      "all branching entropies was computed # words = 60258\n",
      "all accessor variety was computed # words = 60258\n",
      "'송대익'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1268\n",
      "all branching entropies was computed # words = 60270\n",
      "all accessor variety was computed # words = 60270\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1268\n",
      "all branching entropies was computed # words = 60274\n",
      "all accessor variety was computed # words = 60274\n",
      "'이재명이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1268\n",
      "all branching entropies was computed # words = 60290\n",
      "all accessor variety was computed # words = 60290\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1268\n",
      "all branching entropies was computed # words = 60299\n",
      "all accessor variety was computed # words = 60299\n",
      "'가상화폐'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1270\n",
      "all branching entropies was computed # words = 60346\n",
      "all accessor variety was computed # words = 60346\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1273\n",
      "all branching entropies was computed # words = 60383\n",
      "all accessor variety was computed # words = 60383\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 60492\n",
      "all accessor variety was computed # words = 60492\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 60494\n",
      "all accessor variety was computed # words = 60494\n",
      "'양정원'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 60494\n",
      "all accessor variety was computed # words = 60494\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1280\n",
      "all branching entropies was computed # words = 60557\n",
      "all accessor variety was computed # words = 60557\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1285\n",
      "all branching entropies was computed # words = 60574\n",
      "all accessor variety was computed # words = 60574\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1286\n",
      "all branching entropies was computed # words = 60599\n",
      "all accessor variety was computed # words = 60599\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1286\n",
      "all branching entropies was computed # words = 60620\n",
      "all accessor variety was computed # words = 60620\n",
      "'엄복동'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1290\n",
      "all branching entropies was computed # words = 60792\n",
      "all accessor variety was computed # words = 60792\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1290\n",
      "all branching entropies was computed # words = 60802\n",
      "all accessor variety was computed # words = 60802\n",
      "'최여진'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1292\n",
      "all branching entropies was computed # words = 60820\n",
      "all accessor variety was computed # words = 60820\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1294\n",
      "all branching entropies was computed # words = 60833\n",
      "all accessor variety was computed # words = 60833\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1294\n",
      "all branching entropies was computed # words = 60846\n",
      "all accessor variety was computed # words = 60846\n",
      "'정대세'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 60897\n",
      "all accessor variety was computed # words = 60897\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 60926\n",
      "all accessor variety was computed # words = 60926\n",
      "'곽동수'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 60935\n",
      "all accessor variety was computed # words = 60935\n",
      "'쿠앤크'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 60939\n",
      "all accessor variety was computed # words = 60939\n",
      "'장난아니네요'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 60995\n",
      "all accessor variety was computed # words = 60995\n",
      "'전광훈'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 61005\n",
      "all accessor variety was computed # words = 61005\n",
      "'메오후'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 61013\n",
      "all accessor variety was computed # words = 61013\n",
      "'송포유'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1297\n",
      "all branching entropies was computed # words = 61027\n",
      "all accessor variety was computed # words = 61027\n",
      "'개표방송'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1300\n",
      "all branching entropies was computed # words = 61065\n",
      "all accessor variety was computed # words = 61065\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1300\n",
      "all branching entropies was computed # words = 61081\n",
      "all accessor variety was computed # words = 61081\n",
      "'벌레소년'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1302\n",
      "all branching entropies was computed # words = 61097\n",
      "all accessor variety was computed # words = 61097\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1302\n",
      "all branching entropies was computed # words = 61097\n",
      "all accessor variety was computed # words = 61097\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1305\n",
      "all branching entropies was computed # words = 61131\n",
      "all accessor variety was computed # words = 61131\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1305\n",
      "all branching entropies was computed # words = 61137\n",
      "all accessor variety was computed # words = 61137\n",
      "'평양올림픽'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1308\n",
      "all branching entropies was computed # words = 61153\n",
      "all accessor variety was computed # words = 61153\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1309\n",
      "all branching entropies was computed # words = 61189\n",
      "all accessor variety was computed # words = 61189\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1313\n",
      "all branching entropies was computed # words = 61243\n",
      "all accessor variety was computed # words = 61243\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1313\n",
      "all branching entropies was computed # words = 61247\n",
      "all accessor variety was computed # words = 61247\n",
      "'재택근무'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1313\n",
      "all branching entropies was computed # words = 61249\n",
      "all accessor variety was computed # words = 61249\n",
      "'슈퍼엠'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1315\n",
      "all branching entropies was computed # words = 61267\n",
      "all accessor variety was computed # words = 61267\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1315\n",
      "all branching entropies was computed # words = 61278\n",
      "all accessor variety was computed # words = 61278\n",
      "'이언주'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1318\n",
      "all branching entropies was computed # words = 61325\n",
      "all accessor variety was computed # words = 61325\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1321\n",
      "all branching entropies was computed # words = 61345\n",
      "all accessor variety was computed # words = 61345\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1322\n",
      "all branching entropies was computed # words = 61398\n",
      "all accessor variety was computed # words = 61398\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1327\n",
      "all branching entropies was computed # words = 61487\n",
      "all accessor variety was computed # words = 61487\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1327\n",
      "all branching entropies was computed # words = 61491\n",
      "all accessor variety was computed # words = 61491\n",
      "'따봉북'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1330\n",
      "all branching entropies was computed # words = 61534\n",
      "all accessor variety was computed # words = 61534\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1330\n",
      "all branching entropies was computed # words = 61544\n",
      "all accessor variety was computed # words = 61544\n",
      "'성완종'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1333\n",
      "all branching entropies was computed # words = 61552\n",
      "all accessor variety was computed # words = 61552\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1333\n",
      "all branching entropies was computed # words = 61553\n",
      "all accessor variety was computed # words = 61553\n",
      "'남돌들'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1333\n",
      "all branching entropies was computed # words = 61555\n",
      "all accessor variety was computed # words = 61555\n",
      "'구정모'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1336\n",
      "all branching entropies was computed # words = 61604\n",
      "all accessor variety was computed # words = 61604\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1339\n",
      "all branching entropies was computed # words = 61643\n",
      "all accessor variety was computed # words = 61643\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1339\n",
      "all branching entropies was computed # words = 61645\n",
      "all accessor variety was computed # words = 61645\n",
      "'퀸덤'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61667\n",
      "all accessor variety was computed # words = 61667\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61702\n",
      "all accessor variety was computed # words = 61702\n",
      "'대남병원'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61713\n",
      "all accessor variety was computed # words = 61713\n",
      "'함익병'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61714\n",
      "all accessor variety was computed # words = 61714\n",
      "'이한결'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61722\n",
      "all accessor variety was computed # words = 61722\n",
      "'산다라박'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61725\n",
      "all accessor variety was computed # words = 61725\n",
      "'공항패션'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1342\n",
      "all branching entropies was computed # words = 61733\n",
      "all accessor variety was computed # words = 61733\n",
      "'태구민'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1343\n",
      "all branching entropies was computed # words = 61752\n",
      "all accessor variety was computed # words = 61752\n",
      "'박봄'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1348\n",
      "all branching entropies was computed # words = 61805\n",
      "all accessor variety was computed # words = 61805\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1353\n",
      "all branching entropies was computed # words = 61819\n",
      "all accessor variety was computed # words = 61819\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1353\n",
      "all branching entropies was computed # words = 61833\n",
      "all accessor variety was computed # words = 61833\n",
      "'종족주의'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1358\n",
      "all branching entropies was computed # words = 61900\n",
      "all accessor variety was computed # words = 61900\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1361\n",
      "all branching entropies was computed # words = 61910\n",
      "all accessor variety was computed # words = 61910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1361\n",
      "all branching entropies was computed # words = 61919\n",
      "all accessor variety was computed # words = 61919\n",
      "'서두원'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1364\n",
      "all branching entropies was computed # words = 61959\n",
      "all accessor variety was computed # words = 61959\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1364\n",
      "all branching entropies was computed # words = 61960\n",
      "all accessor variety was computed # words = 61960\n",
      "'최민기'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1366\n",
      "all branching entropies was computed # words = 61991\n",
      "all accessor variety was computed # words = 61991\n",
      "'지역화폐'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1366\n",
      "all branching entropies was computed # words = 61999\n",
      "all accessor variety was computed # words = 61999\n",
      "'톰브라운'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1369\n",
      "all branching entropies was computed # words = 62026\n",
      "all accessor variety was computed # words = 62026\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1374\n",
      "all branching entropies was computed # words = 62055\n",
      "all accessor variety was computed # words = 62055\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1377\n",
      "all branching entropies was computed # words = 62095\n",
      "all accessor variety was computed # words = 62095\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1382\n",
      "all branching entropies was computed # words = 62110\n",
      "all accessor variety was computed # words = 62110\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1382\n",
      "all branching entropies was computed # words = 62110\n",
      "all accessor variety was computed # words = 62110\n",
      "'미펜툰'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1382\n",
      "all branching entropies was computed # words = 62129\n",
      "all accessor variety was computed # words = 62129\n",
      "'한상균'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1385\n",
      "all branching entropies was computed # words = 62145\n",
      "all accessor variety was computed # words = 62145\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1385\n",
      "all branching entropies was computed # words = 62176\n",
      "all accessor variety was computed # words = 62176\n",
      "'무관중'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1388\n",
      "all branching entropies was computed # words = 62223\n",
      "all accessor variety was computed # words = 62223\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1389\n",
      "all branching entropies was computed # words = 62229\n",
      "all accessor variety was computed # words = 62229\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1395\n",
      "all branching entropies was computed # words = 62236\n",
      "all accessor variety was computed # words = 62236\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1402\n",
      "all branching entropies was computed # words = 62243\n",
      "all accessor variety was computed # words = 62243\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1402\n",
      "all branching entropies was computed # words = 62244\n",
      "all accessor variety was computed # words = 62244\n",
      "'세이브일베'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1405\n",
      "all branching entropies was computed # words = 62319\n",
      "all accessor variety was computed # words = 62319\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1408\n",
      "all branching entropies was computed # words = 62347\n",
      "all accessor variety was computed # words = 62347\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1408\n",
      "all branching entropies was computed # words = 62353\n",
      "all accessor variety was computed # words = 62353\n",
      "'문복희'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1408\n",
      "all branching entropies was computed # words = 62368\n",
      "all accessor variety was computed # words = 62368\n",
      "'제이홉'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1408\n",
      "all branching entropies was computed # words = 62370\n",
      "all accessor variety was computed # words = 62370\n",
      "'강민희'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1409\n",
      "all branching entropies was computed # words = 62378\n",
      "all accessor variety was computed # words = 62378\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1409\n",
      "all branching entropies was computed # words = 62389\n",
      "all accessor variety was computed # words = 62389\n",
      "'짱개폐렴'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1412\n",
      "all branching entropies was computed # words = 62419\n",
      "all accessor variety was computed # words = 62419\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1415\n",
      "all branching entropies was computed # words = 62572\n",
      "all accessor variety was computed # words = 62572\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1415\n",
      "all branching entropies was computed # words = 62576\n",
      "all accessor variety was computed # words = 62576\n",
      "'대마녀'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1415\n",
      "all branching entropies was computed # words = 62600\n",
      "all accessor variety was computed # words = 62600\n",
      "'출산후기'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1420\n",
      "all branching entropies was computed # words = 62632\n",
      "all accessor variety was computed # words = 62632\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1421\n",
      "all branching entropies was computed # words = 62642\n",
      "all accessor variety was computed # words = 62642\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1421\n",
      "all branching entropies was computed # words = 62653\n",
      "all accessor variety was computed # words = 62653\n",
      "'하나로마트'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1426\n",
      "all branching entropies was computed # words = 62784\n",
      "all accessor variety was computed # words = 62784\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1429\n",
      "all branching entropies was computed # words = 62803\n",
      "all accessor variety was computed # words = 62803\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1435\n",
      "all branching entropies was computed # words = 62934\n",
      "all accessor variety was computed # words = 62934\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1435\n",
      "all branching entropies was computed # words = 62946\n",
      "all accessor variety was computed # words = 62946\n",
      "'펜스룰'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1437\n",
      "all branching entropies was computed # words = 62969\n",
      "all accessor variety was computed # words = 62969\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1439\n",
      "all branching entropies was computed # words = 63018\n",
      "all accessor variety was computed # words = 63018\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1445\n",
      "all branching entropies was computed # words = 63036\n",
      "all accessor variety was computed # words = 63036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1445\n",
      "all branching entropies was computed # words = 63036\n",
      "all accessor variety was computed # words = 63036\n",
      "'크루즈국'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1445\n",
      "all branching entropies was computed # words = 63043\n",
      "all accessor variety was computed # words = 63043\n",
      "'발사체'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1450\n",
      "all branching entropies was computed # words = 63059\n",
      "all accessor variety was computed # words = 63059\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1454\n",
      "all branching entropies was computed # words = 63083\n",
      "all accessor variety was computed # words = 63083\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1457\n",
      "all branching entropies was computed # words = 63103\n",
      "all accessor variety was computed # words = 63103\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1457\n",
      "all branching entropies was computed # words = 63104\n",
      "all accessor variety was computed # words = 63104\n",
      "'박일산'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1460\n",
      "all branching entropies was computed # words = 63128\n",
      "all accessor variety was computed # words = 63128\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1460\n",
      "all branching entropies was computed # words = 63156\n",
      "all accessor variety was computed # words = 63156\n",
      "'양승오'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1461\n",
      "all branching entropies was computed # words = 63175\n",
      "all accessor variety was computed # words = 63175\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1461\n",
      "all branching entropies was computed # words = 63175\n",
      "all accessor variety was computed # words = 63175\n",
      "'유선호'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1462\n",
      "all branching entropies was computed # words = 63192\n",
      "all accessor variety was computed # words = 63192\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1462\n",
      "all branching entropies was computed # words = 63207\n",
      "all accessor variety was computed # words = 63207\n",
      "'지디게이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1463\n",
      "all branching entropies was computed # words = 63283\n",
      "all accessor variety was computed # words = 63283\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1468\n",
      "all branching entropies was computed # words = 63344\n",
      "all accessor variety was computed # words = 63344\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1468\n",
      "all branching entropies was computed # words = 63357\n",
      "all accessor variety was computed # words = 63357\n",
      "'선거조작'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1469\n",
      "all branching entropies was computed # words = 63428\n",
      "all accessor variety was computed # words = 63428\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1472\n",
      "all branching entropies was computed # words = 63503\n",
      "all accessor variety was computed # words = 63503\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1472\n",
      "all branching entropies was computed # words = 63510\n",
      "all accessor variety was computed # words = 63510\n",
      "'평창유감'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1472\n",
      "all branching entropies was computed # words = 63522\n",
      "all accessor variety was computed # words = 63522\n",
      "'가난한집'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1477\n",
      "all branching entropies was computed # words = 63612\n",
      "all accessor variety was computed # words = 63612\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1477\n",
      "all branching entropies was computed # words = 63615\n",
      "all accessor variety was computed # words = 63615\n",
      "'황윤성'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1480\n",
      "all branching entropies was computed # words = 63636\n",
      "all accessor variety was computed # words = 63636\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1481\n",
      "all branching entropies was computed # words = 63658\n",
      "all accessor variety was computed # words = 63658\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1485\n",
      "all branching entropies was computed # words = 63693\n",
      "all accessor variety was computed # words = 63693\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1485\n",
      "all branching entropies was computed # words = 63707\n",
      "all accessor variety was computed # words = 63707\n",
      "'시미켄'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1486\n",
      "all branching entropies was computed # words = 63725\n",
      "all accessor variety was computed # words = 63725\n",
      "'한녀'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1486\n",
      "all branching entropies was computed # words = 63731\n",
      "all accessor variety was computed # words = 63731\n",
      "'운매'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1486\n",
      "all branching entropies was computed # words = 63753\n",
      "all accessor variety was computed # words = 63753\n",
      "'단통법'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1488\n",
      "all branching entropies was computed # words = 63759\n",
      "all accessor variety was computed # words = 63759\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1488\n",
      "all branching entropies was computed # words = 63774\n",
      "all accessor variety was computed # words = 63774\n",
      "'선동꾼'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1488\n",
      "all branching entropies was computed # words = 63780\n",
      "all accessor variety was computed # words = 63780\n",
      "'접촉자'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1491\n",
      "all branching entropies was computed # words = 63810\n",
      "all accessor variety was computed # words = 63810\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1494\n",
      "all branching entropies was computed # words = 63857\n",
      "all accessor variety was computed # words = 63857\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1500\n",
      "all branching entropies was computed # words = 63962\n",
      "all accessor variety was computed # words = 63962\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1501\n",
      "all branching entropies was computed # words = 63978\n",
      "all accessor variety was computed # words = 63978\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1501\n",
      "all branching entropies was computed # words = 63988\n",
      "all accessor variety was computed # words = 63988\n",
      "'모모랜드'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1505\n",
      "all branching entropies was computed # words = 64013\n",
      "all accessor variety was computed # words = 64013\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1505\n",
      "all branching entropies was computed # words = 64019\n",
      "all accessor variety was computed # words = 64019\n",
      "'신재민'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1510\n",
      "all branching entropies was computed # words = 64112\n",
      "all accessor variety was computed # words = 64112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1515\n",
      "all branching entropies was computed # words = 64156\n",
      "all accessor variety was computed # words = 64156\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1517\n",
      "all branching entropies was computed # words = 64180\n",
      "all accessor variety was computed # words = 64180\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1517\n",
      "all branching entropies was computed # words = 64188\n",
      "all accessor variety was computed # words = 64188\n",
      "'멜뮤'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1519\n",
      "all branching entropies was computed # words = 64214\n",
      "all accessor variety was computed # words = 64214\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1520\n",
      "all branching entropies was computed # words = 64337\n",
      "all accessor variety was computed # words = 64337\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1520\n",
      "all branching entropies was computed # words = 64347\n",
      "all accessor variety was computed # words = 64347\n",
      "'필리버스터'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1520\n",
      "all branching entropies was computed # words = 64354\n",
      "all accessor variety was computed # words = 64354\n",
      "'조국딸'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1525\n",
      "all branching entropies was computed # words = 64397\n",
      "all accessor variety was computed # words = 64397\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1525\n",
      "all branching entropies was computed # words = 64397\n",
      "all accessor variety was computed # words = 64397\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1525\n",
      "all branching entropies was computed # words = 64399\n",
      "all accessor variety was computed # words = 64399\n",
      "'김민규'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1525\n",
      "all branching entropies was computed # words = 64404\n",
      "all accessor variety was computed # words = 64404\n",
      "'김현중'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1534\n",
      "all branching entropies was computed # words = 64717\n",
      "all accessor variety was computed # words = 64717\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1534\n",
      "all branching entropies was computed # words = 64721\n",
      "all accessor variety was computed # words = 64721\n",
      "'희재갑'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1536\n",
      "all branching entropies was computed # words = 65167\n",
      "all accessor variety was computed # words = 65167\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1536\n",
      "all branching entropies was computed # words = 65180\n",
      "all accessor variety was computed # words = 65180\n",
      "'효성이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1536\n",
      "all branching entropies was computed # words = 65197\n",
      "all accessor variety was computed # words = 65197\n",
      "'남태현'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1536\n",
      "all branching entropies was computed # words = 65215\n",
      "all accessor variety was computed # words = 65215\n",
      "'소독제'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1538\n",
      "all branching entropies was computed # words = 65388\n",
      "all accessor variety was computed # words = 65388\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1538\n",
      "all branching entropies was computed # words = 65395\n",
      "all accessor variety was computed # words = 65395\n",
      "'수컷닷컴'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1538\n",
      "all branching entropies was computed # words = 65408\n",
      "all accessor variety was computed # words = 65408\n",
      "'의심환자'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1541\n",
      "all branching entropies was computed # words = 65433\n",
      "all accessor variety was computed # words = 65433\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1544\n",
      "all branching entropies was computed # words = 65468\n",
      "all accessor variety was computed # words = 65468\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1545\n",
      "all branching entropies was computed # words = 65537\n",
      "all accessor variety was computed # words = 65537\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1545\n",
      "all branching entropies was computed # words = 65551\n",
      "all accessor variety was computed # words = 65551\n",
      "'화덕게이네'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 65562\n",
      "all accessor variety was computed # words = 65562\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 65587\n",
      "all accessor variety was computed # words = 65587\n",
      "'맹기용'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 65596\n",
      "all accessor variety was computed # words = 65596\n",
      "'가수게이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 65600\n",
      "all accessor variety was computed # words = 65600\n",
      "'그런여자'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 65610\n",
      "all accessor variety was computed # words = 65610\n",
      "'비말마스크'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1550\n",
      "all branching entropies was computed # words = 65644\n",
      "all accessor variety was computed # words = 65644\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1551\n",
      "all branching entropies was computed # words = 65692\n",
      "all accessor variety was computed # words = 65692\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1551\n",
      "all branching entropies was computed # words = 65692\n",
      "all accessor variety was computed # words = 65692\n",
      "'확진자들'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1551\n",
      "all branching entropies was computed # words = 65702\n",
      "all accessor variety was computed # words = 65702\n",
      "'임동규'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1552\n",
      "all branching entropies was computed # words = 65709\n",
      "all accessor variety was computed # words = 65709\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1552\n",
      "all branching entropies was computed # words = 65721\n",
      "all accessor variety was computed # words = 65721\n",
      "'배달앱'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1555\n",
      "all branching entropies was computed # words = 65766\n",
      "all accessor variety was computed # words = 65766\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1556\n",
      "all branching entropies was computed # words = 65862\n",
      "all accessor variety was computed # words = 65862\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1556\n",
      "all branching entropies was computed # words = 65866\n",
      "all accessor variety was computed # words = 65866\n",
      "'반일운동'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1556\n",
      "all branching entropies was computed # words = 65871\n",
      "all accessor variety was computed # words = 65871\n",
      "'일베여신'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1560\n",
      "all branching entropies was computed # words = 65961\n",
      "all accessor variety was computed # words = 65961\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1563\n",
      "all branching entropies was computed # words = 65989\n",
      "all accessor variety was computed # words = 65989\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1563\n",
      "all branching entropies was computed # words = 65993\n",
      "all accessor variety was computed # words = 65993\n",
      "'쓰리썸'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1563\n",
      "all branching entropies was computed # words = 66003\n",
      "all accessor variety was computed # words = 66003\n",
      "'안재현'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1563\n",
      "all branching entropies was computed # words = 66008\n",
      "all accessor variety was computed # words = 66008\n",
      "'홍럼프'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1568\n",
      "all branching entropies was computed # words = 66038\n",
      "all accessor variety was computed # words = 66038\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1568\n",
      "all branching entropies was computed # words = 66057\n",
      "all accessor variety was computed # words = 66057\n",
      "'조희연'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1571\n",
      "all branching entropies was computed # words = 66116\n",
      "all accessor variety was computed # words = 66116\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1573\n",
      "all branching entropies was computed # words = 66156\n",
      "all accessor variety was computed # words = 66156\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1579\n",
      "all branching entropies was computed # words = 66265\n",
      "all accessor variety was computed # words = 66265\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1582\n",
      "all branching entropies was computed # words = 66283\n",
      "all accessor variety was computed # words = 66283\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1583\n",
      "all branching entropies was computed # words = 66302\n",
      "all accessor variety was computed # words = 66302\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1585\n",
      "all branching entropies was computed # words = 66363\n",
      "all accessor variety was computed # words = 66363\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1585\n",
      "all branching entropies was computed # words = 66372\n",
      "all accessor variety was computed # words = 66372\n",
      "'기름값'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1588\n",
      "all branching entropies was computed # words = 66378\n",
      "all accessor variety was computed # words = 66378\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1593\n",
      "all branching entropies was computed # words = 66443\n",
      "all accessor variety was computed # words = 66443\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1593\n",
      "all branching entropies was computed # words = 66447\n",
      "all accessor variety was computed # words = 66447\n",
      "'고준희'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1594\n",
      "all branching entropies was computed # words = 66460\n",
      "all accessor variety was computed # words = 66460\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1597\n",
      "all branching entropies was computed # words = 66465\n",
      "all accessor variety was computed # words = 66465\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1598\n",
      "all branching entropies was computed # words = 66477\n",
      "all accessor variety was computed # words = 66477\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1601\n",
      "all branching entropies was computed # words = 66503\n",
      "all accessor variety was computed # words = 66503\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1601\n",
      "all branching entropies was computed # words = 66507\n",
      "all accessor variety was computed # words = 66507\n",
      "'차기환'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1603\n",
      "all branching entropies was computed # words = 66537\n",
      "all accessor variety was computed # words = 66537\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1607\n",
      "all branching entropies was computed # words = 66565\n",
      "all accessor variety was computed # words = 66565\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1607\n",
      "all branching entropies was computed # words = 66572\n",
      "all accessor variety was computed # words = 66572\n",
      "'프리스틴'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1607\n",
      "all branching entropies was computed # words = 66618\n",
      "all accessor variety was computed # words = 66618\n",
      "'흔녀'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1612\n",
      "all branching entropies was computed # words = 66656\n",
      "all accessor variety was computed # words = 66656\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1612\n",
      "all branching entropies was computed # words = 66691\n",
      "all accessor variety was computed # words = 66691\n",
      "'개냥이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 66695\n",
      "all accessor variety was computed # words = 66695\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 66696\n",
      "all accessor variety was computed # words = 66696\n",
      "'투샷'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 66701\n",
      "all accessor variety was computed # words = 66701\n",
      "'댓글조작'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 66710\n",
      "all accessor variety was computed # words = 66710\n",
      "'짜파구리'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 66715\n",
      "all accessor variety was computed # words = 66715\n",
      "'중국몽'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1614\n",
      "all branching entropies was computed # words = 66734\n",
      "all accessor variety was computed # words = 66734\n",
      "'민낯'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1614\n",
      "all branching entropies was computed # words = 66746\n",
      "all accessor variety was computed # words = 66746\n",
      "'까칠남녀'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1614\n",
      "all branching entropies was computed # words = 66752\n",
      "all accessor variety was computed # words = 66752\n",
      "'서든어택'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1614\n",
      "all branching entropies was computed # words = 66758\n",
      "all accessor variety was computed # words = 66758\n",
      "'스까국'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1616\n",
      "all branching entropies was computed # words = 66765\n",
      "all accessor variety was computed # words = 66765\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1616\n",
      "all branching entropies was computed # words = 66773\n",
      "all accessor variety was computed # words = 66773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1619\n",
      "all branching entropies was computed # words = 66784\n",
      "all accessor variety was computed # words = 66784\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1619\n",
      "all branching entropies was computed # words = 66788\n",
      "all accessor variety was computed # words = 66788\n",
      "'윤창중'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1619\n",
      "all branching entropies was computed # words = 66791\n",
      "all accessor variety was computed # words = 66791\n",
      "'권현빈'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1620\n",
      "all branching entropies was computed # words = 66816\n",
      "all accessor variety was computed # words = 66816\n",
      "'이쁨'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1620\n",
      "all branching entropies was computed # words = 66826\n",
      "all accessor variety was computed # words = 66826\n",
      "'테드창'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1620\n",
      "all branching entropies was computed # words = 66835\n",
      "all accessor variety was computed # words = 66835\n",
      "'씨젠'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1622\n",
      "all branching entropies was computed # words = 66847\n",
      "all accessor variety was computed # words = 66847\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1624\n",
      "all branching entropies was computed # words = 66855\n",
      "all accessor variety was computed # words = 66855\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1625\n",
      "all branching entropies was computed # words = 66864\n",
      "all accessor variety was computed # words = 66864\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1626\n",
      "all branching entropies was computed # words = 66930\n",
      "all accessor variety was computed # words = 66930\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1629\n",
      "all branching entropies was computed # words = 66963\n",
      "all accessor variety was computed # words = 66963\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1629\n",
      "all branching entropies was computed # words = 66963\n",
      "all accessor variety was computed # words = 66963\n",
      "'에어팟프로'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1629\n",
      "all branching entropies was computed # words = 66964\n",
      "all accessor variety was computed # words = 66964\n",
      "'짱깨폐렴'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1629\n",
      "all branching entropies was computed # words = 66978\n",
      "all accessor variety was computed # words = 66978\n",
      "'입국제한'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1633\n",
      "all branching entropies was computed # words = 67022\n",
      "all accessor variety was computed # words = 67022\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1635\n",
      "all branching entropies was computed # words = 67044\n",
      "all accessor variety was computed # words = 67044\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1635\n",
      "all branching entropies was computed # words = 67054\n",
      "all accessor variety was computed # words = 67054\n",
      "'더불당'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1636\n",
      "all branching entropies was computed # words = 67076\n",
      "all accessor variety was computed # words = 67076\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1637\n",
      "all branching entropies was computed # words = 67094\n",
      "all accessor variety was computed # words = 67094\n",
      "'이짤'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1640\n",
      "all branching entropies was computed # words = 67116\n",
      "all accessor variety was computed # words = 67116\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1640\n",
      "all branching entropies was computed # words = 67120\n",
      "all accessor variety was computed # words = 67120\n",
      "'풍선게이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1640\n",
      "all branching entropies was computed # words = 67143\n",
      "all accessor variety was computed # words = 67143\n",
      "'브렉시트'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1640\n",
      "all branching entropies was computed # words = 67149\n",
      "all accessor variety was computed # words = 67149\n",
      "'호두게이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 67209\n",
      "all accessor variety was computed # words = 67209\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 67222\n",
      "all accessor variety was computed # words = 67222\n",
      "'김치워리어'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 67264\n",
      "all accessor variety was computed # words = 67264\n",
      "'손정우'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 67289\n",
      "all accessor variety was computed # words = 67289\n",
      "'예비시댁'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 67310\n",
      "all accessor variety was computed # words = 67310\n",
      "'짝녀'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1646\n",
      "all branching entropies was computed # words = 67340\n",
      "all accessor variety was computed # words = 67340\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1646\n",
      "all branching entropies was computed # words = 67352\n",
      "all accessor variety was computed # words = 67352\n",
      "'덴탈마스크'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1646\n",
      "all branching entropies was computed # words = 67381\n",
      "all accessor variety was computed # words = 67381\n",
      "'유플러스'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1651\n",
      "all branching entropies was computed # words = 67483\n",
      "all accessor variety was computed # words = 67483\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1654\n",
      "all branching entropies was computed # words = 67495\n",
      "all accessor variety was computed # words = 67495\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1654\n",
      "all branching entropies was computed # words = 67498\n",
      "all accessor variety was computed # words = 67498\n",
      "'바른정당'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1655\n",
      "all branching entropies was computed # words = 67525\n",
      "all accessor variety was computed # words = 67525\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1657\n",
      "all branching entropies was computed # words = 67533\n",
      "all accessor variety was computed # words = 67533\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1660\n",
      "all branching entropies was computed # words = 67591\n",
      "all accessor variety was computed # words = 67591\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1669\n",
      "all branching entropies was computed # words = 67705\n",
      "all accessor variety was computed # words = 67705\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67765\n",
      "all accessor variety was computed # words = 67765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67769\n",
      "all accessor variety was computed # words = 67769\n",
      "'영만아재'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67777\n",
      "all accessor variety was computed # words = 67777\n",
      "'노건호'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67781\n",
      "all accessor variety was computed # words = 67781\n",
      "'김소혜'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67784\n",
      "all accessor variety was computed # words = 67784\n",
      "'안형섭'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1670\n",
      "all branching entropies was computed # words = 67795\n",
      "all accessor variety was computed # words = 67795\n",
      "'미투운동'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1673\n",
      "all branching entropies was computed # words = 67886\n",
      "all accessor variety was computed # words = 67886\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1673\n",
      "all branching entropies was computed # words = 67887\n",
      "all accessor variety was computed # words = 67887\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1673\n",
      "all branching entropies was computed # words = 67893\n",
      "all accessor variety was computed # words = 67893\n",
      "'백민주화'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1678\n",
      "all branching entropies was computed # words = 67918\n",
      "all accessor variety was computed # words = 67918\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1678\n",
      "all branching entropies was computed # words = 67942\n",
      "all accessor variety was computed # words = 67942\n",
      "'채동욱'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1680\n",
      "all branching entropies was computed # words = 67979\n",
      "all accessor variety was computed # words = 67979\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1680\n",
      "all branching entropies was computed # words = 67981\n",
      "all accessor variety was computed # words = 67981\n",
      "'강남대장'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1680\n",
      "all branching entropies was computed # words = 67988\n",
      "all accessor variety was computed # words = 67988\n",
      "'솔로대첩'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1680\n",
      "all branching entropies was computed # words = 68008\n",
      "all accessor variety was computed # words = 68008\n",
      "'해피머니'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1685\n",
      "all branching entropies was computed # words = 68054\n",
      "all accessor variety was computed # words = 68054\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1685\n",
      "all branching entropies was computed # words = 68082\n",
      "all accessor variety was computed # words = 68082\n",
      "'여자배구'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1687\n",
      "all branching entropies was computed # words = 68185\n",
      "all accessor variety was computed # words = 68185\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1687\n",
      "all branching entropies was computed # words = 68188\n",
      "all accessor variety was computed # words = 68188\n",
      "'팩트폭행'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1687\n",
      "all branching entropies was computed # words = 68214\n",
      "all accessor variety was computed # words = 68214\n",
      "'학력인증'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1690\n",
      "all branching entropies was computed # words = 68236\n",
      "all accessor variety was computed # words = 68236\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1695\n",
      "all branching entropies was computed # words = 68246\n",
      "all accessor variety was computed # words = 68246\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1695\n",
      "all branching entropies was computed # words = 68255\n",
      "all accessor variety was computed # words = 68255\n",
      "'탄핵청원'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1695\n",
      "all branching entropies was computed # words = 68266\n",
      "all accessor variety was computed # words = 68266\n",
      "'지소미아'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1695\n",
      "all branching entropies was computed # words = 68281\n",
      "all accessor variety was computed # words = 68281\n",
      "'응디시티'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1697\n",
      "all branching entropies was computed # words = 68339\n",
      "all accessor variety was computed # words = 68339\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1700\n",
      "all branching entropies was computed # words = 68421\n",
      "all accessor variety was computed # words = 68421\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1703\n",
      "all branching entropies was computed # words = 68431\n",
      "all accessor variety was computed # words = 68431\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1703\n",
      "all branching entropies was computed # words = 68434\n",
      "all accessor variety was computed # words = 68434\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1706\n",
      "all branching entropies was computed # words = 68442\n",
      "all accessor variety was computed # words = 68442\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1706\n",
      "all branching entropies was computed # words = 68447\n",
      "all accessor variety was computed # words = 68447\n",
      "'희귀짤'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1709\n",
      "all branching entropies was computed # words = 68456\n",
      "all accessor variety was computed # words = 68456\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1711\n",
      "all branching entropies was computed # words = 68472\n",
      "all accessor variety was computed # words = 68472\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1712\n",
      "all branching entropies was computed # words = 68487\n",
      "all accessor variety was computed # words = 68487\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1714\n",
      "all branching entropies was computed # words = 68496\n",
      "all accessor variety was computed # words = 68496\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1714\n",
      "all branching entropies was computed # words = 68500\n",
      "all accessor variety was computed # words = 68500\n",
      "'공영홈쇼핑'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1714\n",
      "all branching entropies was computed # words = 68523\n",
      "all accessor variety was computed # words = 68523\n",
      "'신도들'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1717\n",
      "all branching entropies was computed # words = 68562\n",
      "all accessor variety was computed # words = 68562\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1717\n",
      "all branching entropies was computed # words = 68882\n",
      "all accessor variety was computed # words = 68882\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1718\n",
      "all branching entropies was computed # words = 68963\n",
      "all accessor variety was computed # words = 68963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1718\n",
      "all branching entropies was computed # words = 68974\n",
      "all accessor variety was computed # words = 68974\n",
      "'게이클럽'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1720\n",
      "all branching entropies was computed # words = 69000\n",
      "all accessor variety was computed # words = 69000\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1723\n",
      "all branching entropies was computed # words = 69021\n",
      "all accessor variety was computed # words = 69021\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1725\n",
      "all branching entropies was computed # words = 69045\n",
      "all accessor variety was computed # words = 69045\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1725\n",
      "all branching entropies was computed # words = 69050\n",
      "all accessor variety was computed # words = 69050\n",
      "'또래오래'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1729\n",
      "all branching entropies was computed # words = 69115\n",
      "all accessor variety was computed # words = 69115\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1732\n",
      "all branching entropies was computed # words = 69144\n",
      "all accessor variety was computed # words = 69144\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1735\n",
      "all branching entropies was computed # words = 69172\n",
      "all accessor variety was computed # words = 69172\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1735\n",
      "all branching entropies was computed # words = 69177\n",
      "all accessor variety was computed # words = 69177\n",
      "'권아솔'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 69206\n",
      "all accessor variety was computed # words = 69206\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 69227\n",
      "all accessor variety was computed # words = 69227\n",
      "'웃음참기대회'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 69259\n",
      "all accessor variety was computed # words = 69259\n",
      "'은하철도'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 69282\n",
      "all accessor variety was computed # words = 69282\n",
      "'박병호'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1741\n",
      "all branching entropies was computed # words = 69307\n",
      "all accessor variety was computed # words = 69307\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1744\n",
      "all branching entropies was computed # words = 69315\n",
      "all accessor variety was computed # words = 69315\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1744\n",
      "all branching entropies was computed # words = 69315\n",
      "all accessor variety was computed # words = 69315\n",
      "'손동표'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1747\n",
      "all branching entropies was computed # words = 69350\n",
      "all accessor variety was computed # words = 69350\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1747\n",
      "all branching entropies was computed # words = 69362\n",
      "all accessor variety was computed # words = 69362\n",
      "'고등래퍼'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1747\n",
      "all branching entropies was computed # words = 69377\n",
      "all accessor variety was computed # words = 69377\n",
      "'명륜진사갈비'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1747\n",
      "all branching entropies was computed # words = 69378\n",
      "all accessor variety was computed # words = 69378\n",
      "'파퀴아오'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1747\n",
      "all branching entropies was computed # words = 69396\n",
      "all accessor variety was computed # words = 69396\n",
      "'인터스텔라'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1748\n",
      "all branching entropies was computed # words = 69423\n",
      "all accessor variety was computed # words = 69423\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1748\n",
      "all branching entropies was computed # words = 69433\n",
      "all accessor variety was computed # words = 69433\n",
      "'더불어민주당'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1753\n",
      "all branching entropies was computed # words = 69486\n",
      "all accessor variety was computed # words = 69486\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1755\n",
      "all branching entropies was computed # words = 69497\n",
      "all accessor variety was computed # words = 69497\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1755\n",
      "all branching entropies was computed # words = 69500\n",
      "all accessor variety was computed # words = 69500\n",
      "'인국공'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1756\n",
      "all branching entropies was computed # words = 69523\n",
      "all accessor variety was computed # words = 69523\n",
      "'여캠'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1757\n",
      "all branching entropies was computed # words = 69592\n",
      "all accessor variety was computed # words = 69592\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1757\n",
      "all branching entropies was computed # words = 69595\n",
      "all accessor variety was computed # words = 69595\n",
      "'방탄청년단'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1758\n",
      "all branching entropies was computed # words = 69646\n",
      "all accessor variety was computed # words = 69646\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1758\n",
      "all branching entropies was computed # words = 69654\n",
      "all accessor variety was computed # words = 69654\n",
      "'구의역'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1758\n",
      "all branching entropies was computed # words = 69654\n",
      "all accessor variety was computed # words = 69654\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1760\n",
      "all branching entropies was computed # words = 69665\n",
      "all accessor variety was computed # words = 69665\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69678\n",
      "all accessor variety was computed # words = 69678\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69686\n",
      "all accessor variety was computed # words = 69686\n",
      "'하트시그널'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69686\n",
      "all accessor variety was computed # words = 69686\n",
      "'함원진'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69695\n",
      "all accessor variety was computed # words = 69695\n",
      "'그룹별'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69709\n",
      "all accessor variety was computed # words = 69709\n",
      "'도칠이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1764\n",
      "all branching entropies was computed # words = 69729\n",
      "all accessor variety was computed # words = 69729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1767\n",
      "all branching entropies was computed # words = 69787\n",
      "all accessor variety was computed # words = 69787\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1771\n",
      "all branching entropies was computed # words = 69841\n",
      "all accessor variety was computed # words = 69841\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1774\n",
      "all branching entropies was computed # words = 69859\n",
      "all accessor variety was computed # words = 69859\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1774\n",
      "all branching entropies was computed # words = 69863\n",
      "all accessor variety was computed # words = 69863\n",
      "'자한당'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1775\n",
      "all branching entropies was computed # words = 69897\n",
      "all accessor variety was computed # words = 69897\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1775\n",
      "all branching entropies was computed # words = 69904\n",
      "all accessor variety was computed # words = 69904\n",
      "'팩맨'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1778\n",
      "all branching entropies was computed # words = 69929\n",
      "all accessor variety was computed # words = 69929\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1781\n",
      "all branching entropies was computed # words = 69971\n",
      "all accessor variety was computed # words = 69971\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1783\n",
      "all branching entropies was computed # words = 69977\n",
      "all accessor variety was computed # words = 69977\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1786\n",
      "all branching entropies was computed # words = 69993\n",
      "all accessor variety was computed # words = 69993\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1786\n",
      "all branching entropies was computed # words = 70010\n",
      "all accessor variety was computed # words = 70010\n",
      "'김소영'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1786\n",
      "all branching entropies was computed # words = 70022\n",
      "all accessor variety was computed # words = 70022\n",
      "'응팔'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 70027\n",
      "all accessor variety was computed # words = 70027\n",
      "'멍뭉이저장소'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 70032\n",
      "all accessor variety was computed # words = 70032\n",
      "'노쨩'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 70037\n",
      "all accessor variety was computed # words = 70037\n",
      "'컬쳐랜드'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 70040\n",
      "all accessor variety was computed # words = 70040\n",
      "'위메프오'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 70043\n",
      "all accessor variety was computed # words = 70043\n",
      "'서가대'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1789\n",
      "all branching entropies was computed # words = 70072\n",
      "all accessor variety was computed # words = 70072\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1789\n",
      "all branching entropies was computed # words = 70073\n",
      "all accessor variety was computed # words = 70073\n",
      "'많이들'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1789\n",
      "all branching entropies was computed # words = 70082\n",
      "all accessor variety was computed # words = 70082\n",
      "'해지방어'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1793\n",
      "all branching entropies was computed # words = 70127\n",
      "all accessor variety was computed # words = 70127\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1795\n",
      "all branching entropies was computed # words = 70159\n",
      "all accessor variety was computed # words = 70159\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1797\n",
      "all branching entropies was computed # words = 70172\n",
      "all accessor variety was computed # words = 70172\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1798\n",
      "all branching entropies was computed # words = 70205\n",
      "all accessor variety was computed # words = 70205\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1799\n",
      "all branching entropies was computed # words = 70215\n",
      "all accessor variety was computed # words = 70215\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1799\n",
      "all branching entropies was computed # words = 70221\n",
      "all accessor variety was computed # words = 70221\n",
      "'달빛기사단'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1804\n",
      "all branching entropies was computed # words = 70263\n",
      "all accessor variety was computed # words = 70263\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1804\n",
      "all branching entropies was computed # words = 70275\n",
      "all accessor variety was computed # words = 70275\n",
      "'두테르테'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1807\n",
      "all branching entropies was computed # words = 70288\n",
      "all accessor variety was computed # words = 70288\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1809\n",
      "all branching entropies was computed # words = 70494\n",
      "all accessor variety was computed # words = 70494\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1809\n",
      "all branching entropies was computed # words = 70509\n",
      "all accessor variety was computed # words = 70509\n",
      "'시사인'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1811\n",
      "all branching entropies was computed # words = 70523\n",
      "all accessor variety was computed # words = 70523\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1814\n",
      "all branching entropies was computed # words = 70564\n",
      "all accessor variety was computed # words = 70564\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1814\n",
      "all branching entropies was computed # words = 70573\n",
      "all accessor variety was computed # words = 70573\n",
      "'핸섬타이거즈'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1814\n",
      "all branching entropies was computed # words = 70573\n",
      "all accessor variety was computed # words = 70573\n",
      "'사전투표율'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1814\n",
      "all branching entropies was computed # words = 70574\n",
      "all accessor variety was computed # words = 70574\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1817\n",
      "all branching entropies was computed # words = 70637\n",
      "all accessor variety was computed # words = 70637\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1818\n",
      "all branching entropies was computed # words = 70650\n",
      "all accessor variety was computed # words = 70650\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1820\n",
      "all branching entropies was computed # words = 70657\n",
      "all accessor variety was computed # words = 70657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1825\n",
      "all branching entropies was computed # words = 70674\n",
      "all accessor variety was computed # words = 70674\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1825\n",
      "all branching entropies was computed # words = 70681\n",
      "all accessor variety was computed # words = 70681\n",
      "'김성준'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1826\n",
      "all branching entropies was computed # words = 70746\n",
      "all accessor variety was computed # words = 70746\n",
      "'어케'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1826\n",
      "all branching entropies was computed # words = 70755\n",
      "all accessor variety was computed # words = 70755\n",
      "'패륜집단'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1826\n",
      "all branching entropies was computed # words = 70763\n",
      "all accessor variety was computed # words = 70763\n",
      "'암사역'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1833\n",
      "all branching entropies was computed # words = 70781\n",
      "all accessor variety was computed # words = 70781\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1833\n",
      "all branching entropies was computed # words = 70786\n",
      "all accessor variety was computed # words = 70786\n",
      "'표팔만'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1836\n",
      "all branching entropies was computed # words = 70816\n",
      "all accessor variety was computed # words = 70816\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1836\n",
      "all branching entropies was computed # words = 70824\n",
      "all accessor variety was computed # words = 70824\n",
      "'푸른거탑'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1838\n",
      "all branching entropies was computed # words = 70846\n",
      "all accessor variety was computed # words = 70846\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70881\n",
      "all accessor variety was computed # words = 70881\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70882\n",
      "all accessor variety was computed # words = 70882\n",
      "'열어줘'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70887\n",
      "all accessor variety was computed # words = 70887\n",
      "'김새론'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70892\n",
      "all accessor variety was computed # words = 70892\n",
      "'플디즈'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70911\n",
      "all accessor variety was computed # words = 70911\n",
      "'예쁘고'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1840\n",
      "all branching entropies was computed # words = 70929\n",
      "all accessor variety was computed # words = 70929\n",
      "'감독상'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1843\n",
      "all branching entropies was computed # words = 71011\n",
      "all accessor variety was computed # words = 71011\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1843\n",
      "all branching entropies was computed # words = 71018\n",
      "all accessor variety was computed # words = 71018\n",
      "'빨간우의'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1843\n",
      "all branching entropies was computed # words = 71023\n",
      "all accessor variety was computed # words = 71023\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1846\n",
      "all branching entropies was computed # words = 71037\n",
      "all accessor variety was computed # words = 71037\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1850\n",
      "all branching entropies was computed # words = 71047\n",
      "all accessor variety was computed # words = 71047\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1852\n",
      "all branching entropies was computed # words = 71062\n",
      "all accessor variety was computed # words = 71062\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1855\n",
      "all branching entropies was computed # words = 71073\n",
      "all accessor variety was computed # words = 71073\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1860\n",
      "all branching entropies was computed # words = 71078\n",
      "all accessor variety was computed # words = 71078\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1860\n",
      "all branching entropies was computed # words = 71096\n",
      "all accessor variety was computed # words = 71096\n",
      "'여성모델'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1862\n",
      "all branching entropies was computed # words = 71112\n",
      "all accessor variety was computed # words = 71112\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1865\n",
      "all branching entropies was computed # words = 71210\n",
      "all accessor variety was computed # words = 71210\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1865\n",
      "all branching entropies was computed # words = 71217\n",
      "all accessor variety was computed # words = 71217\n",
      "'일산이'\n",
      "training was done. used memory 1.057 Gb1.057 Gb\n",
      "all cohesion probabilities was computed. # words = 1868\n",
      "all branching entropies was computed # words = 71444\n",
      "all accessor variety was computed # words = 71444\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1869\n",
      "all branching entropies was computed # words = 71588\n",
      "all accessor variety was computed # words = 71588\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1874\n",
      "all branching entropies was computed # words = 71604\n",
      "all accessor variety was computed # words = 71604\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1874\n",
      "all branching entropies was computed # words = 71612\n",
      "all accessor variety was computed # words = 71612\n",
      "'혜화역'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1874\n",
      "all branching entropies was computed # words = 71622\n",
      "all accessor variety was computed # words = 71622\n",
      "'미러링'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1875\n",
      "all branching entropies was computed # words = 71646\n",
      "all accessor variety was computed # words = 71646\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71817\n",
      "all accessor variety was computed # words = 71817\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71822\n",
      "all accessor variety was computed # words = 71822\n",
      "'개드립'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71827\n",
      "all accessor variety was computed # words = 71827\n",
      "'근혜찡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71831\n",
      "all accessor variety was computed # words = 71831\n",
      "'어나니머스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71847\n",
      "all accessor variety was computed # words = 71847\n",
      "'인증저장소'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71854\n",
      "all accessor variety was computed # words = 71854\n",
      "'용석찡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71871\n",
      "all accessor variety was computed # words = 71871\n",
      "'인서울'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71889\n",
      "all accessor variety was computed # words = 71889\n",
      "'쿨톤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71897\n",
      "all accessor variety was computed # words = 71897\n",
      "'강승윤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71897\n",
      "all accessor variety was computed # words = 71897\n",
      "'자가격리자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71935\n",
      "all accessor variety was computed # words = 71935\n",
      "'무료쿠폰'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71950\n",
      "all accessor variety was computed # words = 71950\n",
      "'교민들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 71957\n",
      "all accessor variety was computed # words = 71957\n",
      "'봉감독'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1877\n",
      "all branching entropies was computed # words = 71986\n",
      "all accessor variety was computed # words = 71986\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1879\n",
      "all branching entropies was computed # words = 71996\n",
      "all accessor variety was computed # words = 71996\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1880\n",
      "all branching entropies was computed # words = 72010\n",
      "all accessor variety was computed # words = 72010\n",
      "'연돈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1883\n",
      "all branching entropies was computed # words = 72056\n",
      "all accessor variety was computed # words = 72056\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1883\n",
      "all branching entropies was computed # words = 72070\n",
      "all accessor variety was computed # words = 72070\n",
      "'윤형빈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1883\n",
      "all branching entropies was computed # words = 72070\n",
      "all accessor variety was computed # words = 72070\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1884\n",
      "all branching entropies was computed # words = 72093\n",
      "all accessor variety was computed # words = 72093\n",
      "'시부터'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1889\n",
      "all branching entropies was computed # words = 72159\n",
      "all accessor variety was computed # words = 72159\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1892\n",
      "all branching entropies was computed # words = 72172\n",
      "all accessor variety was computed # words = 72172\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1895\n",
      "all branching entropies was computed # words = 72202\n",
      "all accessor variety was computed # words = 72202\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1897\n",
      "all branching entropies was computed # words = 72217\n",
      "all accessor variety was computed # words = 72217\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1897\n",
      "all branching entropies was computed # words = 72221\n",
      "all accessor variety was computed # words = 72221\n",
      "'그새끼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1899\n",
      "all branching entropies was computed # words = 72255\n",
      "all accessor variety was computed # words = 72255\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1899\n",
      "all branching entropies was computed # words = 72256\n",
      "all accessor variety was computed # words = 72256\n",
      "'김성태'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1900\n",
      "all branching entropies was computed # words = 72263\n",
      "all accessor variety was computed # words = 72263\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1901\n",
      "all branching entropies was computed # words = 72292\n",
      "all accessor variety was computed # words = 72292\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1901\n",
      "all branching entropies was computed # words = 72301\n",
      "all accessor variety was computed # words = 72301\n",
      "'지니어스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1904\n",
      "all branching entropies was computed # words = 72313\n",
      "all accessor variety was computed # words = 72313\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1906\n",
      "all branching entropies was computed # words = 72317\n",
      "all accessor variety was computed # words = 72317\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1906\n",
      "all branching entropies was computed # words = 72318\n",
      "all accessor variety was computed # words = 72318\n",
      "'나꼼수'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1911\n",
      "all branching entropies was computed # words = 72327\n",
      "all accessor variety was computed # words = 72327\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1911\n",
      "all branching entropies was computed # words = 72331\n",
      "all accessor variety was computed # words = 72331\n",
      "'디패'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1913\n",
      "all branching entropies was computed # words = 72373\n",
      "all accessor variety was computed # words = 72373\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1913\n",
      "all branching entropies was computed # words = 72380\n",
      "all accessor variety was computed # words = 72380\n",
      "'대두김용배'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1913\n",
      "all branching entropies was computed # words = 72402\n",
      "all accessor variety was computed # words = 72402\n",
      "'삼시세끼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1913\n",
      "all branching entropies was computed # words = 72411\n",
      "all accessor variety was computed # words = 72411\n",
      "'포미닛'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72431\n",
      "all accessor variety was computed # words = 72431\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72456\n",
      "all accessor variety was computed # words = 72456\n",
      "'요리들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72476\n",
      "all accessor variety was computed # words = 72476\n",
      "'레슬매니아'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72481\n",
      "all accessor variety was computed # words = 72481\n",
      "'중위소득'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72495\n",
      "all accessor variety was computed # words = 72495\n",
      "'완치자'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72501\n",
      "all accessor variety was computed # words = 72501\n",
      "'정태호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 72502\n",
      "all accessor variety was computed # words = 72502\n",
      "'갓준표'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1918\n",
      "all branching entropies was computed # words = 72540\n",
      "all accessor variety was computed # words = 72540\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1921\n",
      "all branching entropies was computed # words = 72567\n",
      "all accessor variety was computed # words = 72567\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1926\n",
      "all branching entropies was computed # words = 72637\n",
      "all accessor variety was computed # words = 72637\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1930\n",
      "all branching entropies was computed # words = 72667\n",
      "all accessor variety was computed # words = 72667\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1935\n",
      "all branching entropies was computed # words = 72673\n",
      "all accessor variety was computed # words = 72673\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1938\n",
      "all branching entropies was computed # words = 72713\n",
      "all accessor variety was computed # words = 72713\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1941\n",
      "all branching entropies was computed # words = 72740\n",
      "all accessor variety was computed # words = 72740\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1942\n",
      "all branching entropies was computed # words = 72751\n",
      "all accessor variety was computed # words = 72751\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1945\n",
      "all branching entropies was computed # words = 72799\n",
      "all accessor variety was computed # words = 72799\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1948\n",
      "all branching entropies was computed # words = 72808\n",
      "all accessor variety was computed # words = 72808\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1948\n",
      "all branching entropies was computed # words = 72817\n",
      "all accessor variety was computed # words = 72817\n",
      "'주예지'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1951\n",
      "all branching entropies was computed # words = 72830\n",
      "all accessor variety was computed # words = 72830\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1954\n",
      "all branching entropies was computed # words = 72837\n",
      "all accessor variety was computed # words = 72837\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1955\n",
      "all branching entropies was computed # words = 72898\n",
      "all accessor variety was computed # words = 72898\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1956\n",
      "all branching entropies was computed # words = 72915\n",
      "all accessor variety was computed # words = 72915\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73070\n",
      "all accessor variety was computed # words = 73070\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73072\n",
      "all accessor variety was computed # words = 73072\n",
      "'홍자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73073\n",
      "all accessor variety was computed # words = 73073\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73076\n",
      "all accessor variety was computed # words = 73076\n",
      "'지지선언'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73093\n",
      "all accessor variety was computed # words = 73093\n",
      "'슈틸리케'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 73099\n",
      "all accessor variety was computed # words = 73099\n",
      "'김영란법'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1959\n",
      "all branching entropies was computed # words = 73110\n",
      "all accessor variety was computed # words = 73110\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1959\n",
      "all branching entropies was computed # words = 73123\n",
      "all accessor variety was computed # words = 73123\n",
      "'권은희'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1959\n",
      "all branching entropies was computed # words = 73138\n",
      "all accessor variety was computed # words = 73138\n",
      "'히든싱어'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1959\n",
      "all branching entropies was computed # words = 73171\n",
      "all accessor variety was computed # words = 73171\n",
      "'가보았다'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73228\n",
      "all accessor variety was computed # words = 73228\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73237\n",
      "all accessor variety was computed # words = 73237\n",
      "'연애혁명'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73240\n",
      "all accessor variety was computed # words = 73240\n",
      "'연습생들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73240\n",
      "all accessor variety was computed # words = 73240\n",
      "'경기지역화폐'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73249\n",
      "all accessor variety was computed # words = 73249\n",
      "'트레이더스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73269\n",
      "all accessor variety was computed # words = 73269\n",
      "'주민센터'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73272\n",
      "all accessor variety was computed # words = 73272\n",
      "'아에르'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73278\n",
      "all accessor variety was computed # words = 73278\n",
      "'김다미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73281\n",
      "all accessor variety was computed # words = 73281\n",
      "'번환자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73294\n",
      "all accessor variety was computed # words = 73294\n",
      "'아임뚜렛'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1960\n",
      "all branching entropies was computed # words = 73306\n",
      "all accessor variety was computed # words = 73306\n",
      "'네이버성님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1963\n",
      "all branching entropies was computed # words = 73353\n",
      "all accessor variety was computed # words = 73353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1965\n",
      "all branching entropies was computed # words = 73383\n",
      "all accessor variety was computed # words = 73383\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1969\n",
      "all branching entropies was computed # words = 73388\n",
      "all accessor variety was computed # words = 73388\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1974\n",
      "all branching entropies was computed # words = 73426\n",
      "all accessor variety was computed # words = 73426\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1974\n",
      "all branching entropies was computed # words = 73432\n",
      "all accessor variety was computed # words = 73432\n",
      "'빅타이거'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1976\n",
      "all branching entropies was computed # words = 73476\n",
      "all accessor variety was computed # words = 73476\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1976\n",
      "all branching entropies was computed # words = 73479\n",
      "all accessor variety was computed # words = 73479\n",
      "'원종건'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1979\n",
      "all branching entropies was computed # words = 73514\n",
      "all accessor variety was computed # words = 73514\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1979\n",
      "all branching entropies was computed # words = 73516\n",
      "all accessor variety was computed # words = 73516\n",
      "'안나경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73541\n",
      "all accessor variety was computed # words = 73541\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73548\n",
      "all accessor variety was computed # words = 73548\n",
      "'이방카'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73551\n",
      "all accessor variety was computed # words = 73551\n",
      "'흥진호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73559\n",
      "all accessor variety was computed # words = 73559\n",
      "'청년수당'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73564\n",
      "all accessor variety was computed # words = 73564\n",
      "'맨시티'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1982\n",
      "all branching entropies was computed # words = 73566\n",
      "all accessor variety was computed # words = 73566\n",
      "'예정화'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1985\n",
      "all branching entropies was computed # words = 73752\n",
      "all accessor variety was computed # words = 73752\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1985\n",
      "all branching entropies was computed # words = 73772\n",
      "all accessor variety was computed # words = 73772\n",
      "'고무현'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1985\n",
      "all branching entropies was computed # words = 73779\n",
      "all accessor variety was computed # words = 73779\n",
      "'고승덕'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1985\n",
      "all branching entropies was computed # words = 73783\n",
      "all accessor variety was computed # words = 73783\n",
      "'로이킴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1985\n",
      "all branching entropies was computed # words = 73790\n",
      "all accessor variety was computed # words = 73790\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1988\n",
      "all branching entropies was computed # words = 73819\n",
      "all accessor variety was computed # words = 73819\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1988\n",
      "all branching entropies was computed # words = 73844\n",
      "all accessor variety was computed # words = 73844\n",
      "'학원강사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1988\n",
      "all branching entropies was computed # words = 73846\n",
      "all accessor variety was computed # words = 73846\n",
      "'정재현'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73882\n",
      "all accessor variety was computed # words = 73882\n",
      "'있긴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73891\n",
      "all accessor variety was computed # words = 73891\n",
      "'예비시어머니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73897\n",
      "all accessor variety was computed # words = 73897\n",
      "'믹스나인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73903\n",
      "all accessor variety was computed # words = 73903\n",
      "'반박글'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73917\n",
      "all accessor variety was computed # words = 73917\n",
      "'임시완'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73927\n",
      "all accessor variety was computed # words = 73927\n",
      "'스마일캐시'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73934\n",
      "all accessor variety was computed # words = 73934\n",
      "'펠리세이드'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73937\n",
      "all accessor variety was computed # words = 73937\n",
      "'강기정'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73952\n",
      "all accessor variety was computed # words = 73952\n",
      "'챔스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 73965\n",
      "all accessor variety was computed # words = 73965\n",
      "'난박이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 74001\n",
      "all accessor variety was computed # words = 74001\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 74028\n",
      "all accessor variety was computed # words = 74028\n",
      "'아기고양이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 74028\n",
      "all accessor variety was computed # words = 74028\n",
      "'우한바이러스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 74031\n",
      "all accessor variety was computed # words = 74031\n",
      "'진워렌버핏'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1992\n",
      "all branching entropies was computed # words = 74036\n",
      "all accessor variety was computed # words = 74036\n",
      "'개돼지들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1997\n",
      "all branching entropies was computed # words = 74063\n",
      "all accessor variety was computed # words = 74063\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 1999\n",
      "all branching entropies was computed # words = 74103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 74103\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2006\n",
      "all branching entropies was computed # words = 74399\n",
      "all accessor variety was computed # words = 74399\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2006\n",
      "all branching entropies was computed # words = 74399\n",
      "all accessor variety was computed # words = 74399\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74436\n",
      "all accessor variety was computed # words = 74436\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74440\n",
      "all accessor variety was computed # words = 74440\n",
      "'손봄향'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74448\n",
      "all accessor variety was computed # words = 74448\n",
      "'뱅모'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74462\n",
      "all accessor variety was computed # words = 74462\n",
      "'대출갤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74463\n",
      "all accessor variety was computed # words = 74463\n",
      "'형욱신'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74474\n",
      "all accessor variety was computed # words = 74474\n",
      "'황병서'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2008\n",
      "all branching entropies was computed # words = 74481\n",
      "all accessor variety was computed # words = 74481\n",
      "'재기성님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2011\n",
      "all branching entropies was computed # words = 74506\n",
      "all accessor variety was computed # words = 74506\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74571\n",
      "all accessor variety was computed # words = 74571\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74581\n",
      "all accessor variety was computed # words = 74581\n",
      "'쿠팡이츠'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74584\n",
      "all accessor variety was computed # words = 74584\n",
      "'간호학과'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74599\n",
      "all accessor variety was computed # words = 74599\n",
      "'입어도'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74609\n",
      "all accessor variety was computed # words = 74609\n",
      "'화장법'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74609\n",
      "all accessor variety was computed # words = 74609\n",
      "'차준호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74613\n",
      "all accessor variety was computed # words = 74613\n",
      "'그룹들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74621\n",
      "all accessor variety was computed # words = 74621\n",
      "'랩몬'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2014\n",
      "all branching entropies was computed # words = 74626\n",
      "all accessor variety was computed # words = 74626\n",
      "'의사생활'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74634\n",
      "all accessor variety was computed # words = 74634\n",
      "'사진있음'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74647\n",
      "all accessor variety was computed # words = 74647\n",
      "'제로페이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74678\n",
      "all accessor variety was computed # words = 74678\n",
      "'받아가세요'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74678\n",
      "all accessor variety was computed # words = 74678\n",
      "'확진환자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74687\n",
      "all accessor variety was computed # words = 74687\n",
      "'오열사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74699\n",
      "all accessor variety was computed # words = 74699\n",
      "'한남충'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74700\n",
      "all accessor variety was computed # words = 74700\n",
      "'준표형'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74709\n",
      "all accessor variety was computed # words = 74709\n",
      "'아이돌학교'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2016\n",
      "all branching entropies was computed # words = 74722\n",
      "all accessor variety was computed # words = 74722\n",
      "'비원에이포'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2020\n",
      "all branching entropies was computed # words = 74741\n",
      "all accessor variety was computed # words = 74741\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2022\n",
      "all branching entropies was computed # words = 74763\n",
      "all accessor variety was computed # words = 74763\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2024\n",
      "all branching entropies was computed # words = 74774\n",
      "all accessor variety was computed # words = 74774\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2029\n",
      "all branching entropies was computed # words = 74840\n",
      "all accessor variety was computed # words = 74840\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2029\n",
      "all branching entropies was computed # words = 74842\n",
      "all accessor variety was computed # words = 74842\n",
      "'좆국'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2030\n",
      "all branching entropies was computed # words = 74884\n",
      "all accessor variety was computed # words = 74884\n",
      "'추노'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2031\n",
      "all branching entropies was computed # words = 74900\n",
      "all accessor variety was computed # words = 74900\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2036\n",
      "all branching entropies was computed # words = 74936\n",
      "all accessor variety was computed # words = 74936\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2036\n",
      "all branching entropies was computed # words = 74944\n",
      "all accessor variety was computed # words = 74944\n",
      "'껍데기집'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2036\n",
      "all branching entropies was computed # words = 74946\n",
      "all accessor variety was computed # words = 74946\n",
      "'윾갈비'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2042\n",
      "all branching entropies was computed # words = 74968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 74968\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2045\n",
      "all branching entropies was computed # words = 75001\n",
      "all accessor variety was computed # words = 75001\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2045\n",
      "all branching entropies was computed # words = 75009\n",
      "all accessor variety was computed # words = 75009\n",
      "'캣맘'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2045\n",
      "all branching entropies was computed # words = 75012\n",
      "all accessor variety was computed # words = 75012\n",
      "'정사갤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2045\n",
      "all branching entropies was computed # words = 75019\n",
      "all accessor variety was computed # words = 75019\n",
      "'합필갤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2048\n",
      "all branching entropies was computed # words = 75054\n",
      "all accessor variety was computed # words = 75054\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2051\n",
      "all branching entropies was computed # words = 75152\n",
      "all accessor variety was computed # words = 75152\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2051\n",
      "all branching entropies was computed # words = 75164\n",
      "all accessor variety was computed # words = 75164\n",
      "'놀면뭐하니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75185\n",
      "all accessor variety was computed # words = 75185\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75189\n",
      "all accessor variety was computed # words = 75189\n",
      "'역조공'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75189\n",
      "all accessor variety was computed # words = 75189\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75193\n",
      "all accessor variety was computed # words = 75193\n",
      "'서은광'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75199\n",
      "all accessor variety was computed # words = 75199\n",
      "'병크'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 75202\n",
      "all accessor variety was computed # words = 75202\n",
      "'강두기'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2053\n",
      "all branching entropies was computed # words = 75234\n",
      "all accessor variety was computed # words = 75234\n",
      "'보밍'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2053\n",
      "all branching entropies was computed # words = 75238\n",
      "all accessor variety was computed # words = 75238\n",
      "'컵밥'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2054\n",
      "all branching entropies was computed # words = 75272\n",
      "all accessor variety was computed # words = 75272\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2054\n",
      "all branching entropies was computed # words = 75292\n",
      "all accessor variety was computed # words = 75292\n",
      "'히트곡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2054\n",
      "all branching entropies was computed # words = 75298\n",
      "all accessor variety was computed # words = 75298\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2056\n",
      "all branching entropies was computed # words = 75301\n",
      "all accessor variety was computed # words = 75301\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 75352\n",
      "all accessor variety was computed # words = 75352\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 75362\n",
      "all accessor variety was computed # words = 75362\n",
      "'방진마스크'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 75371\n",
      "all accessor variety was computed # words = 75371\n",
      "'재난문자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2060\n",
      "all branching entropies was computed # words = 75392\n",
      "all accessor variety was computed # words = 75392\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2060\n",
      "all branching entropies was computed # words = 75401\n",
      "all accessor variety was computed # words = 75401\n",
      "'나로호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2060\n",
      "all branching entropies was computed # words = 75409\n",
      "all accessor variety was computed # words = 75409\n",
      "'데뷔조'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2063\n",
      "all branching entropies was computed # words = 75423\n",
      "all accessor variety was computed # words = 75423\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2064\n",
      "all branching entropies was computed # words = 75431\n",
      "all accessor variety was computed # words = 75431\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2064\n",
      "all branching entropies was computed # words = 75431\n",
      "all accessor variety was computed # words = 75431\n",
      "'이태원클라스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2067\n",
      "all branching entropies was computed # words = 75454\n",
      "all accessor variety was computed # words = 75454\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2067\n",
      "all branching entropies was computed # words = 75470\n",
      "all accessor variety was computed # words = 75470\n",
      "'틀무새'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2069\n",
      "all branching entropies was computed # words = 75482\n",
      "all accessor variety was computed # words = 75482\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2072\n",
      "all branching entropies was computed # words = 75507\n",
      "all accessor variety was computed # words = 75507\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2077\n",
      "all branching entropies was computed # words = 75513\n",
      "all accessor variety was computed # words = 75513\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2077\n",
      "all branching entropies was computed # words = 75523\n",
      "all accessor variety was computed # words = 75523\n",
      "'레진탈퇴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2077\n",
      "all branching entropies was computed # words = 75527\n",
      "all accessor variety was computed # words = 75527\n",
      "'너굴맨'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2079\n",
      "all branching entropies was computed # words = 75603\n",
      "all accessor variety was computed # words = 75603\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2084\n",
      "all branching entropies was computed # words = 75608\n",
      "all accessor variety was computed # words = 75608\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2084\n",
      "all branching entropies was computed # words = 75610\n",
      "all accessor variety was computed # words = 75610\n",
      "'봉하반점'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2084\n",
      "all branching entropies was computed # words = 75614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 75614\n",
      "'김치맨'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2084\n",
      "all branching entropies was computed # words = 75616\n",
      "all accessor variety was computed # words = 75616\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2088\n",
      "all branching entropies was computed # words = 75661\n",
      "all accessor variety was computed # words = 75661\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2088\n",
      "all branching entropies was computed # words = 75670\n",
      "all accessor variety was computed # words = 75670\n",
      "'송유빈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2090\n",
      "all branching entropies was computed # words = 75699\n",
      "all accessor variety was computed # words = 75699\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2090\n",
      "all branching entropies was computed # words = 75702\n",
      "all accessor variety was computed # words = 75702\n",
      "'장문복'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2092\n",
      "all branching entropies was computed # words = 75715\n",
      "all accessor variety was computed # words = 75715\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2092\n",
      "all branching entropies was computed # words = 75727\n",
      "all accessor variety was computed # words = 75727\n",
      "'집단면역'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2092\n",
      "all branching entropies was computed # words = 75740\n",
      "all accessor variety was computed # words = 75740\n",
      "'신종플루'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2092\n",
      "all branching entropies was computed # words = 75750\n",
      "all accessor variety was computed # words = 75750\n",
      "'김사부'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2092\n",
      "all branching entropies was computed # words = 75756\n",
      "all accessor variety was computed # words = 75756\n",
      "'추가시어머니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2093\n",
      "all branching entropies was computed # words = 75812\n",
      "all accessor variety was computed # words = 75812\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2096\n",
      "all branching entropies was computed # words = 75894\n",
      "all accessor variety was computed # words = 75894\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2096\n",
      "all branching entropies was computed # words = 75902\n",
      "all accessor variety was computed # words = 75902\n",
      "'수꼴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2096\n",
      "all branching entropies was computed # words = 75905\n",
      "all accessor variety was computed # words = 75905\n",
      "'송가인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2098\n",
      "all branching entropies was computed # words = 75922\n",
      "all accessor variety was computed # words = 75922\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2101\n",
      "all branching entropies was computed # words = 75935\n",
      "all accessor variety was computed # words = 75935\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2108\n",
      "all branching entropies was computed # words = 75971\n",
      "all accessor variety was computed # words = 75971\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2108\n",
      "all branching entropies was computed # words = 75973\n",
      "all accessor variety was computed # words = 75973\n",
      "'법무부장관'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2110\n",
      "all branching entropies was computed # words = 75991\n",
      "all accessor variety was computed # words = 75991\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2115\n",
      "all branching entropies was computed # words = 76005\n",
      "all accessor variety was computed # words = 76005\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2118\n",
      "all branching entropies was computed # words = 76035\n",
      "all accessor variety was computed # words = 76035\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2121\n",
      "all branching entropies was computed # words = 76088\n",
      "all accessor variety was computed # words = 76088\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2124\n",
      "all branching entropies was computed # words = 76120\n",
      "all accessor variety was computed # words = 76120\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2127\n",
      "all branching entropies was computed # words = 76163\n",
      "all accessor variety was computed # words = 76163\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2131\n",
      "all branching entropies was computed # words = 76221\n",
      "all accessor variety was computed # words = 76221\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2131\n",
      "all branching entropies was computed # words = 76226\n",
      "all accessor variety was computed # words = 76226\n",
      "'양팡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2134\n",
      "all branching entropies was computed # words = 76248\n",
      "all accessor variety was computed # words = 76248\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76276\n",
      "all accessor variety was computed # words = 76276\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76281\n",
      "all accessor variety was computed # words = 76281\n",
      "'강정마을'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76296\n",
      "all accessor variety was computed # words = 76296\n",
      "'호남사람'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76304\n",
      "all accessor variety was computed # words = 76304\n",
      "'윾재석'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76313\n",
      "all accessor variety was computed # words = 76313\n",
      "'주갤러'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76316\n",
      "all accessor variety was computed # words = 76316\n",
      "'특조위'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76319\n",
      "all accessor variety was computed # words = 76319\n",
      "'메구리'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76323\n",
      "all accessor variety was computed # words = 76323\n",
      "'코노야로'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76330\n",
      "all accessor variety was computed # words = 76330\n",
      "'발치몽'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76330\n",
      "all accessor variety was computed # words = 76330\n",
      "'유승옥'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76344\n",
      "all accessor variety was computed # words = 76344\n",
      "'렛미인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 76348\n",
      "'몽준이형'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76348\n",
      "all accessor variety was computed # words = 76348\n",
      "'호두게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 76352\n",
      "all accessor variety was computed # words = 76352\n",
      "'해외유입'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2141\n",
      "all branching entropies was computed # words = 76363\n",
      "all accessor variety was computed # words = 76363\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2141\n",
      "all branching entropies was computed # words = 76367\n",
      "all accessor variety was computed # words = 76367\n",
      "'가요대축제'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2141\n",
      "all branching entropies was computed # words = 76382\n",
      "all accessor variety was computed # words = 76382\n",
      "'방탈죄송'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2141\n",
      "all branching entropies was computed # words = 76386\n",
      "all accessor variety was computed # words = 76386\n",
      "'신세경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76417\n",
      "all accessor variety was computed # words = 76417\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76419\n",
      "all accessor variety was computed # words = 76419\n",
      "'치인트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76423\n",
      "all accessor variety was computed # words = 76423\n",
      "'유인나'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76426\n",
      "all accessor variety was computed # words = 76426\n",
      "'각본상'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76432\n",
      "all accessor variety was computed # words = 76432\n",
      "'교회들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2144\n",
      "all branching entropies was computed # words = 76447\n",
      "all accessor variety was computed # words = 76447\n",
      "'재승인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2149\n",
      "all branching entropies was computed # words = 76473\n",
      "all accessor variety was computed # words = 76473\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 76515\n",
      "all accessor variety was computed # words = 76515\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 76538\n",
      "all accessor variety was computed # words = 76538\n",
      "'상간녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 76548\n",
      "all accessor variety was computed # words = 76548\n",
      "'마라탕'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 76551\n",
      "all accessor variety was computed # words = 76551\n",
      "'림종석'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2152\n",
      "all branching entropies was computed # words = 76563\n",
      "all accessor variety was computed # words = 76563\n",
      "'문빠'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2155\n",
      "all branching entropies was computed # words = 76577\n",
      "all accessor variety was computed # words = 76577\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2155\n",
      "all branching entropies was computed # words = 76578\n",
      "all accessor variety was computed # words = 76578\n",
      "'여아이돌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2160\n",
      "all branching entropies was computed # words = 76700\n",
      "all accessor variety was computed # words = 76700\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2160\n",
      "all branching entropies was computed # words = 76706\n",
      "all accessor variety was computed # words = 76706\n",
      "'일본제품'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2160\n",
      "all branching entropies was computed # words = 76709\n",
      "all accessor variety was computed # words = 76709\n",
      "'조보아'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2160\n",
      "all branching entropies was computed # words = 76716\n",
      "all accessor variety was computed # words = 76716\n",
      "'박능후'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2161\n",
      "all branching entropies was computed # words = 76759\n",
      "all accessor variety was computed # words = 76759\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 76802\n",
      "all accessor variety was computed # words = 76802\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76812\n",
      "all accessor variety was computed # words = 76812\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76821\n",
      "all accessor variety was computed # words = 76821\n",
      "'보내주라'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76830\n",
      "all accessor variety was computed # words = 76830\n",
      "'상폐녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76830\n",
      "all accessor variety was computed # words = 76830\n",
      "'갓교안'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76841\n",
      "all accessor variety was computed # words = 76841\n",
      "'슨타크'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76841\n",
      "all accessor variety was computed # words = 76841\n",
      "'이희은'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76846\n",
      "all accessor variety was computed # words = 76846\n",
      "'두부외상'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76851\n",
      "all accessor variety was computed # words = 76851\n",
      "'노근혜'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76856\n",
      "all accessor variety was computed # words = 76856\n",
      "'패미게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76856\n",
      "all accessor variety was computed # words = 76856\n",
      "'멸공의횃불'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76870\n",
      "all accessor variety was computed # words = 76870\n",
      "'강소라'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2166\n",
      "all branching entropies was computed # words = 76876\n",
      "all accessor variety was computed # words = 76876\n",
      "'이민정'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 76915\n",
      "all accessor variety was computed # words = 76915\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 76922\n",
      "all accessor variety was computed # words = 76922\n",
      "'피디님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 76930\n",
      "all accessor variety was computed # words = 76930\n",
      "'소울드레서'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 76947\n",
      "all accessor variety was computed # words = 76947\n",
      "'롯데시네마'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 76947\n",
      "all accessor variety was computed # words = 76947\n",
      "'미친거아님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 76969\n",
      "all accessor variety was computed # words = 76969\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 76976\n",
      "all accessor variety was computed # words = 76976\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 76984\n",
      "all accessor variety was computed # words = 76984\n",
      "'최숙현'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 76993\n",
      "all accessor variety was computed # words = 76993\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 77000\n",
      "all accessor variety was computed # words = 77000\n",
      "'엽떡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 77000\n",
      "all accessor variety was computed # words = 77000\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2178\n",
      "all branching entropies was computed # words = 77014\n",
      "all accessor variety was computed # words = 77014\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77040\n",
      "all accessor variety was computed # words = 77040\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77040\n",
      "all accessor variety was computed # words = 77040\n",
      "'정부재난지원금'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77051\n",
      "all accessor variety was computed # words = 77051\n",
      "'혐오주의'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77054\n",
      "all accessor variety was computed # words = 77054\n",
      "'허가윤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77058\n",
      "all accessor variety was computed # words = 77058\n",
      "'한선화'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77074\n",
      "all accessor variety was computed # words = 77074\n",
      "'썸녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77083\n",
      "all accessor variety was computed # words = 77083\n",
      "'만원대'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77085\n",
      "all accessor variety was computed # words = 77085\n",
      "'구원파'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2180\n",
      "all branching entropies was computed # words = 77093\n",
      "all accessor variety was computed # words = 77093\n",
      "'법게이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2182\n",
      "all branching entropies was computed # words = 77120\n",
      "all accessor variety was computed # words = 77120\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2185\n",
      "all branching entropies was computed # words = 77156\n",
      "all accessor variety was computed # words = 77156\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2191\n",
      "all branching entropies was computed # words = 77184\n",
      "all accessor variety was computed # words = 77184\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2196\n",
      "all branching entropies was computed # words = 77234\n",
      "all accessor variety was computed # words = 77234\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 77279\n",
      "all accessor variety was computed # words = 77279\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 77290\n",
      "all accessor variety was computed # words = 77290\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 77294\n",
      "all accessor variety was computed # words = 77294\n",
      "'여친인증'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 77299\n",
      "all accessor variety was computed # words = 77299\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 77303\n",
      "all accessor variety was computed # words = 77303\n",
      "'정찬희'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 77312\n",
      "all accessor variety was computed # words = 77312\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 77314\n",
      "all accessor variety was computed # words = 77314\n",
      "'표창원이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2209\n",
      "all branching entropies was computed # words = 77335\n",
      "all accessor variety was computed # words = 77335\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2213\n",
      "all branching entropies was computed # words = 77465\n",
      "all accessor variety was computed # words = 77465\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2215\n",
      "all branching entropies was computed # words = 77499\n",
      "all accessor variety was computed # words = 77499\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2215\n",
      "all branching entropies was computed # words = 77505\n",
      "all accessor variety was computed # words = 77505\n",
      "'명이게이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 77511\n",
      "all accessor variety was computed # words = 77511\n",
      "'그려봄'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 77518\n",
      "all accessor variety was computed # words = 77518\n",
      "'강지영'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2218\n",
      "all branching entropies was computed # words = 77532\n",
      "all accessor variety was computed # words = 77532\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 77536\n",
      "all accessor variety was computed # words = 77536\n",
      "'빅토르안'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2218\n",
      "all branching entropies was computed # words = 77536\n",
      "all accessor variety was computed # words = 77536\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2218\n",
      "all branching entropies was computed # words = 77557\n",
      "all accessor variety was computed # words = 77557\n",
      "'안나게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2218\n",
      "all branching entropies was computed # words = 77562\n",
      "all accessor variety was computed # words = 77562\n",
      "'호성성님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2223\n",
      "all branching entropies was computed # words = 77593\n",
      "all accessor variety was computed # words = 77593\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2223\n",
      "all branching entropies was computed # words = 77598\n",
      "all accessor variety was computed # words = 77598\n",
      "'국정원녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 77625\n",
      "all accessor variety was computed # words = 77625\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 77635\n",
      "all accessor variety was computed # words = 77635\n",
      "'정국이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 77644\n",
      "all accessor variety was computed # words = 77644\n",
      "'박나래'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 77678\n",
      "all accessor variety was computed # words = 77678\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 77685\n",
      "all accessor variety was computed # words = 77685\n",
      "'싸강'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 77697\n",
      "all accessor variety was computed # words = 77697\n",
      "'맥날'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 77697\n",
      "all accessor variety was computed # words = 77697\n",
      "'자가격리중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 77699\n",
      "all accessor variety was computed # words = 77699\n",
      "'유증상자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2230\n",
      "all branching entropies was computed # words = 77699\n",
      "all accessor variety was computed # words = 77699\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 77711\n",
      "all accessor variety was computed # words = 77711\n",
      "'남잔'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 77712\n",
      "all accessor variety was computed # words = 77712\n",
      "'브금주'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 77729\n",
      "all accessor variety was computed # words = 77729\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 77742\n",
      "all accessor variety was computed # words = 77742\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 77742\n",
      "all accessor variety was computed # words = 77742\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 77742\n",
      "all accessor variety was computed # words = 77742\n",
      "'확진판정'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 77744\n",
      "all accessor variety was computed # words = 77744\n",
      "'조작선거'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 77818\n",
      "all accessor variety was computed # words = 77818\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 77827\n",
      "all accessor variety was computed # words = 77827\n",
      "'유학생들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 77838\n",
      "all accessor variety was computed # words = 77838\n",
      "'무리뉴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2243\n",
      "all branching entropies was computed # words = 77895\n",
      "all accessor variety was computed # words = 77895\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2243\n",
      "all branching entropies was computed # words = 77900\n",
      "all accessor variety was computed # words = 77900\n",
      "'블랙하우스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2243\n",
      "all branching entropies was computed # words = 77901\n",
      "all accessor variety was computed # words = 77901\n",
      "'제천화재'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2243\n",
      "all branching entropies was computed # words = 77907\n",
      "all accessor variety was computed # words = 77907\n",
      "'핑크코끼리'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 77923\n",
      "all accessor variety was computed # words = 77923\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 77927\n",
      "all accessor variety was computed # words = 77927\n",
      "'최태민'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 77931\n",
      "all accessor variety was computed # words = 77931\n",
      "'이진욱'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78009\n",
      "all accessor variety was computed # words = 78009\n",
      "'일밍'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78012\n",
      "all accessor variety was computed # words = 78012\n",
      "'일베기자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78016\n",
      "all accessor variety was computed # words = 78016\n",
      "'천재소녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78023\n",
      "all accessor variety was computed # words = 78023\n",
      "'재업저격'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78026\n",
      "all accessor variety was computed # words = 78026\n",
      "'새밑년'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78026\n",
      "all accessor variety was computed # words = 78026\n",
      "'쉰김치년'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78037\n",
      "all accessor variety was computed # words = 78037\n",
      "'김유식'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78038\n",
      "all accessor variety was computed # words = 78038\n",
      "'영훈이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78038\n",
      "all accessor variety was computed # words = 78038\n",
      "'진중궈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78047\n",
      "all accessor variety was computed # words = 78047\n",
      "'존박'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78056\n",
      "all accessor variety was computed # words = 78056\n",
      "'아키에이지'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78060\n",
      "all accessor variety was computed # words = 78060\n",
      "'땅크성님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2244\n",
      "all branching entropies was computed # words = 78083\n",
      "all accessor variety was computed # words = 78083\n",
      "'딸바보'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2249\n",
      "all branching entropies was computed # words = 78114\n",
      "all accessor variety was computed # words = 78114\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78179\n",
      "all accessor variety was computed # words = 78179\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78184\n",
      "all accessor variety was computed # words = 78184\n",
      "'생기부'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78191\n",
      "all accessor variety was computed # words = 78191\n",
      "'이나은'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78191\n",
      "all accessor variety was computed # words = 78191\n",
      "'실물짤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78201\n",
      "all accessor variety was computed # words = 78201\n",
      "'좋긴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78204\n",
      "all accessor variety was computed # words = 78204\n",
      "'브랜뉴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78204\n",
      "all accessor variety was computed # words = 78204\n",
      "'이태원클럽'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78214\n",
      "all accessor variety was computed # words = 78214\n",
      "'세계일주'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78227\n",
      "all accessor variety was computed # words = 78227\n",
      "'여다경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78230\n",
      "all accessor variety was computed # words = 78230\n",
      "'저희동네'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78230\n",
      "all accessor variety was computed # words = 78230\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78233\n",
      "all accessor variety was computed # words = 78233\n",
      "'되팔이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78236\n",
      "all accessor variety was computed # words = 78236\n",
      "'보실분'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78256\n",
      "all accessor variety was computed # words = 78256\n",
      "'여경이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78258\n",
      "all accessor variety was computed # words = 78258\n",
      "'컴퓨존'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 78266\n",
      "all accessor variety was computed # words = 78266\n",
      "'황선미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2254\n",
      "all branching entropies was computed # words = 78305\n",
      "all accessor variety was computed # words = 78305\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2254\n",
      "all branching entropies was computed # words = 78305\n",
      "all accessor variety was computed # words = 78305\n",
      "'쓰시는분들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2257\n",
      "all branching entropies was computed # words = 78323\n",
      "all accessor variety was computed # words = 78323\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2260\n",
      "all branching entropies was computed # words = 78333\n",
      "all accessor variety was computed # words = 78333\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2260\n",
      "all branching entropies was computed # words = 78340\n",
      "all accessor variety was computed # words = 78340\n",
      "'팬톡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2260\n",
      "all branching entropies was computed # words = 78342\n",
      "all accessor variety was computed # words = 78342\n",
      "'탈김치녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2260\n",
      "all branching entropies was computed # words = 78342\n",
      "all accessor variety was computed # words = 78342\n",
      "'원픽'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2263\n",
      "all branching entropies was computed # words = 78392\n",
      "all accessor variety was computed # words = 78392\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2263\n",
      "all branching entropies was computed # words = 78392\n",
      "all accessor variety was computed # words = 78392\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2281\n",
      "all branching entropies was computed # words = 79230\n",
      "all accessor variety was computed # words = 79230\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2281\n",
      "all branching entropies was computed # words = 79236\n",
      "all accessor variety was computed # words = 79236\n",
      "'청원인증'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2281\n",
      "all branching entropies was computed # words = 79237\n",
      "all accessor variety was computed # words = 79237\n",
      "'보람튜브'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2289\n",
      "all branching entropies was computed # words = 79607\n",
      "all accessor variety was computed # words = 79607\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2289\n",
      "all branching entropies was computed # words = 79632\n",
      "all accessor variety was computed # words = 79632\n",
      "'주작질'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2291\n",
      "all branching entropies was computed # words = 79714\n",
      "all accessor variety was computed # words = 79714\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2291\n",
      "all branching entropies was computed # words = 79723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 79723\n",
      "'황철순'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2293\n",
      "all branching entropies was computed # words = 79754\n",
      "all accessor variety was computed # words = 79754\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2294\n",
      "all branching entropies was computed # words = 79825\n",
      "all accessor variety was computed # words = 79825\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2294\n",
      "all branching entropies was computed # words = 79857\n",
      "all accessor variety was computed # words = 79857\n",
      "'재팬'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2294\n",
      "all branching entropies was computed # words = 79881\n",
      "all accessor variety was computed # words = 79881\n",
      "'인증대란'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2296\n",
      "all branching entropies was computed # words = 79891\n",
      "all accessor variety was computed # words = 79891\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2296\n",
      "all branching entropies was computed # words = 79892\n",
      "all accessor variety was computed # words = 79892\n",
      "'권한대행'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2298\n",
      "all branching entropies was computed # words = 79900\n",
      "all accessor variety was computed # words = 79900\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2298\n",
      "all branching entropies was computed # words = 79904\n",
      "all accessor variety was computed # words = 79904\n",
      "'쉑쉑버거'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2298\n",
      "all branching entropies was computed # words = 79913\n",
      "all accessor variety was computed # words = 79913\n",
      "'샌더스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2298\n",
      "all branching entropies was computed # words = 79930\n",
      "all accessor variety was computed # words = 79930\n",
      "'김연경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2299\n",
      "all branching entropies was computed # words = 79949\n",
      "all accessor variety was computed # words = 79949\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2303\n",
      "all branching entropies was computed # words = 80012\n",
      "all accessor variety was computed # words = 80012\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2303\n",
      "all branching entropies was computed # words = 80014\n",
      "all accessor variety was computed # words = 80014\n",
      "'엠씨몽'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2303\n",
      "all branching entropies was computed # words = 80014\n",
      "all accessor variety was computed # words = 80014\n",
      "'다크나이트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2303\n",
      "all branching entropies was computed # words = 80017\n",
      "all accessor variety was computed # words = 80017\n",
      "'짭게이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2303\n",
      "all branching entropies was computed # words = 80023\n",
      "all accessor variety was computed # words = 80023\n",
      "'뿌링클'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2305\n",
      "all branching entropies was computed # words = 80041\n",
      "all accessor variety was computed # words = 80041\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2305\n",
      "all branching entropies was computed # words = 80048\n",
      "all accessor variety was computed # words = 80048\n",
      "'한문철'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80054\n",
      "all accessor variety was computed # words = 80054\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80055\n",
      "all accessor variety was computed # words = 80055\n",
      "'슈화'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80063\n",
      "all accessor variety was computed # words = 80063\n",
      "'데이트비용'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80065\n",
      "all accessor variety was computed # words = 80065\n",
      "'육지담'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80069\n",
      "all accessor variety was computed # words = 80069\n",
      "'한효주'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80071\n",
      "all accessor variety was computed # words = 80071\n",
      "'대구지역'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80081\n",
      "all accessor variety was computed # words = 80081\n",
      "'의료붕괴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2308\n",
      "all branching entropies was computed # words = 80085\n",
      "all accessor variety was computed # words = 80085\n",
      "'검사수'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2311\n",
      "all branching entropies was computed # words = 80113\n",
      "all accessor variety was computed # words = 80113\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2311\n",
      "all branching entropies was computed # words = 80116\n",
      "all accessor variety was computed # words = 80116\n",
      "'웃기노'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2314\n",
      "all branching entropies was computed # words = 80139\n",
      "all accessor variety was computed # words = 80139\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2318\n",
      "all branching entropies was computed # words = 80147\n",
      "all accessor variety was computed # words = 80147\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2320\n",
      "all branching entropies was computed # words = 80155\n",
      "all accessor variety was computed # words = 80155\n",
      "'기분좋다'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2320\n",
      "all branching entropies was computed # words = 80166\n",
      "all accessor variety was computed # words = 80166\n",
      "'머글이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2320\n",
      "all branching entropies was computed # words = 80173\n",
      "all accessor variety was computed # words = 80173\n",
      "'명절때'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2320\n",
      "all branching entropies was computed # words = 80187\n",
      "all accessor variety was computed # words = 80187\n",
      "'아이스버킷'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2323\n",
      "all branching entropies was computed # words = 80234\n",
      "all accessor variety was computed # words = 80234\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2323\n",
      "all branching entropies was computed # words = 80240\n",
      "all accessor variety was computed # words = 80240\n",
      "'종결자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2323\n",
      "all branching entropies was computed # words = 80240\n",
      "all accessor variety was computed # words = 80240\n",
      "'갭차이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2323\n",
      "all branching entropies was computed # words = 80247\n",
      "all accessor variety was computed # words = 80247\n",
      "'원글지킴이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 80272\n",
      "all accessor variety was computed # words = 80272\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2326\n",
      "all branching entropies was computed # words = 80285\n",
      "all accessor variety was computed # words = 80285\n",
      "'갓양남'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2326\n",
      "all branching entropies was computed # words = 80294\n",
      "all accessor variety was computed # words = 80294\n",
      "'코로나사태'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2327\n",
      "all branching entropies was computed # words = 80310\n",
      "all accessor variety was computed # words = 80310\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2327\n",
      "all branching entropies was computed # words = 80316\n",
      "all accessor variety was computed # words = 80316\n",
      "'중국폐렴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2327\n",
      "all branching entropies was computed # words = 80322\n",
      "all accessor variety was computed # words = 80322\n",
      "'투표조작'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2331\n",
      "all branching entropies was computed # words = 80610\n",
      "all accessor variety was computed # words = 80610\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2331\n",
      "all branching entropies was computed # words = 80617\n",
      "all accessor variety was computed # words = 80617\n",
      "'메갈들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2333\n",
      "all branching entropies was computed # words = 80630\n",
      "all accessor variety was computed # words = 80630\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2333\n",
      "all branching entropies was computed # words = 80634\n",
      "all accessor variety was computed # words = 80634\n",
      "'고소녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2334\n",
      "all branching entropies was computed # words = 80668\n",
      "all accessor variety was computed # words = 80668\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2337\n",
      "all branching entropies was computed # words = 80672\n",
      "all accessor variety was computed # words = 80672\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2337\n",
      "all branching entropies was computed # words = 80675\n",
      "all accessor variety was computed # words = 80675\n",
      "'은하선'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80694\n",
      "all accessor variety was computed # words = 80694\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80710\n",
      "all accessor variety was computed # words = 80710\n",
      "'존버'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80710\n",
      "all accessor variety was computed # words = 80710\n",
      "'창조경제'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80720\n",
      "all accessor variety was computed # words = 80720\n",
      "'국가들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80721\n",
      "all accessor variety was computed # words = 80721\n",
      "'최설화'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2339\n",
      "all branching entropies was computed # words = 80724\n",
      "all accessor variety was computed # words = 80724\n",
      "'강난희'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80796\n",
      "all accessor variety was computed # words = 80796\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80803\n",
      "all accessor variety was computed # words = 80803\n",
      "'선비님들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80804\n",
      "all accessor variety was computed # words = 80804\n",
      "'오의사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80813\n",
      "all accessor variety was computed # words = 80813\n",
      "'싸이월드'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80824\n",
      "all accessor variety was computed # words = 80824\n",
      "'대표님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80832\n",
      "all accessor variety was computed # words = 80832\n",
      "'윤일병'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80836\n",
      "all accessor variety was computed # words = 80836\n",
      "'오오미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80843\n",
      "all accessor variety was computed # words = 80843\n",
      "'오동통면'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80848\n",
      "all accessor variety was computed # words = 80848\n",
      "'해본사람'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80855\n",
      "all accessor variety was computed # words = 80855\n",
      "'빽다방'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80887\n",
      "all accessor variety was computed # words = 80887\n",
      "'살여자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80891\n",
      "all accessor variety was computed # words = 80891\n",
      "'정일훈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80893\n",
      "all accessor variety was computed # words = 80893\n",
      "'강미나'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80894\n",
      "all accessor variety was computed # words = 80894\n",
      "'양홍원'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80894\n",
      "all accessor variety was computed # words = 80894\n",
      "'버논'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80907\n",
      "all accessor variety was computed # words = 80907\n",
      "'선별진료소'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80917\n",
      "all accessor variety was computed # words = 80917\n",
      "'메르스때'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80923\n",
      "all accessor variety was computed # words = 80923\n",
      "'손세정제'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2341\n",
      "all branching entropies was computed # words = 80924\n",
      "all accessor variety was computed # words = 80924\n",
      "'웰킵스몰'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2343\n",
      "all branching entropies was computed # words = 80944\n",
      "all accessor variety was computed # words = 80944\n",
      "'해보신분'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2343\n",
      "all branching entropies was computed # words = 80956\n",
      "all accessor variety was computed # words = 80956\n",
      "'팬데믹'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2343\n",
      "all branching entropies was computed # words = 80965\n",
      "all accessor variety was computed # words = 80965\n",
      "'뻘글'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2346\n",
      "all branching entropies was computed # words = 81007\n",
      "all accessor variety was computed # words = 81007\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2350\n",
      "all branching entropies was computed # words = 81035\n",
      "all accessor variety was computed # words = 81035\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2350\n",
      "all branching entropies was computed # words = 81048\n",
      "all accessor variety was computed # words = 81048\n",
      "'진라면'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2350\n",
      "all branching entropies was computed # words = 81055\n",
      "all accessor variety was computed # words = 81055\n",
      "'한남패치'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2353\n",
      "all branching entropies was computed # words = 81077\n",
      "all accessor variety was computed # words = 81077\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2353\n",
      "all branching entropies was computed # words = 81088\n",
      "all accessor variety was computed # words = 81088\n",
      "'긴급생활비'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2353\n",
      "all branching entropies was computed # words = 81089\n",
      "all accessor variety was computed # words = 81089\n",
      "'류지혜'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2355\n",
      "all branching entropies was computed # words = 81116\n",
      "all accessor variety was computed # words = 81116\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2355\n",
      "all branching entropies was computed # words = 81120\n",
      "all accessor variety was computed # words = 81120\n",
      "'박초롱'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2356\n",
      "all branching entropies was computed # words = 81130\n",
      "all accessor variety was computed # words = 81130\n",
      "'뽐뻐'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2359\n",
      "all branching entropies was computed # words = 81162\n",
      "all accessor variety was computed # words = 81162\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2362\n",
      "all branching entropies was computed # words = 81203\n",
      "all accessor variety was computed # words = 81203\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2362\n",
      "all branching entropies was computed # words = 81210\n",
      "all accessor variety was computed # words = 81210\n",
      "'장성규'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2365\n",
      "all branching entropies was computed # words = 81242\n",
      "all accessor variety was computed # words = 81242\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2365\n",
      "all branching entropies was computed # words = 81261\n",
      "all accessor variety was computed # words = 81261\n",
      "'살차이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2365\n",
      "all branching entropies was computed # words = 81274\n",
      "all accessor variety was computed # words = 81274\n",
      "'퀴어축제'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2365\n",
      "all branching entropies was computed # words = 81287\n",
      "all accessor variety was computed # words = 81287\n",
      "'결시친'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2366\n",
      "all branching entropies was computed # words = 81326\n",
      "all accessor variety was computed # words = 81326\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2366\n",
      "all branching entropies was computed # words = 81340\n",
      "all accessor variety was computed # words = 81340\n",
      "'네일아트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2367\n",
      "all branching entropies was computed # words = 81414\n",
      "all accessor variety was computed # words = 81414\n",
      "'본사람'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2371\n",
      "all branching entropies was computed # words = 81436\n",
      "all accessor variety was computed # words = 81436\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2373\n",
      "all branching entropies was computed # words = 81456\n",
      "all accessor variety was computed # words = 81456\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2373\n",
      "all branching entropies was computed # words = 81460\n",
      "all accessor variety was computed # words = 81460\n",
      "'밝혀졌다'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2375\n",
      "all branching entropies was computed # words = 81468\n",
      "all accessor variety was computed # words = 81468\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2378\n",
      "all branching entropies was computed # words = 81485\n",
      "all accessor variety was computed # words = 81485\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2385\n",
      "all branching entropies was computed # words = 81638\n",
      "all accessor variety was computed # words = 81638\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2385\n",
      "all branching entropies was computed # words = 81638\n",
      "all accessor variety was computed # words = 81638\n",
      "'최종훈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2385\n",
      "all branching entropies was computed # words = 81644\n",
      "all accessor variety was computed # words = 81644\n",
      "'뇌피셜'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2385\n",
      "all branching entropies was computed # words = 81647\n",
      "all accessor variety was computed # words = 81647\n",
      "'오또맘'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2385\n",
      "all branching entropies was computed # words = 81649\n",
      "all accessor variety was computed # words = 81649\n",
      "'좌리앙'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2387\n",
      "all branching entropies was computed # words = 81655\n",
      "all accessor variety was computed # words = 81655\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2388\n",
      "all branching entropies was computed # words = 81687\n",
      "all accessor variety was computed # words = 81687\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2389\n",
      "all branching entropies was computed # words = 81707\n",
      "all accessor variety was computed # words = 81707\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2394\n",
      "all branching entropies was computed # words = 81722\n",
      "all accessor variety was computed # words = 81722\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2394\n",
      "all branching entropies was computed # words = 81722\n",
      "all accessor variety was computed # words = 81722\n",
      "'유세현장'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2394\n",
      "all branching entropies was computed # words = 81728\n",
      "all accessor variety was computed # words = 81728\n",
      "'미북회담'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2394\n",
      "all branching entropies was computed # words = 81728\n",
      "all accessor variety was computed # words = 81728\n",
      "'유세차량'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81748\n",
      "all accessor variety was computed # words = 81748\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81757\n",
      "all accessor variety was computed # words = 81757\n",
      "'저번주'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81761\n",
      "all accessor variety was computed # words = 81761\n",
      "'전인범'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81785\n",
      "all accessor variety was computed # words = 81785\n",
      "'정찬성'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81786\n",
      "all accessor variety was computed # words = 81786\n",
      "'윤성빈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81795\n",
      "all accessor variety was computed # words = 81795\n",
      "'빨간우비'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81795\n",
      "all accessor variety was computed # words = 81795\n",
      "'박원숭이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2397\n",
      "all branching entropies was computed # words = 81805\n",
      "all accessor variety was computed # words = 81805\n",
      "'최두호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2399\n",
      "all branching entropies was computed # words = 81813\n",
      "all accessor variety was computed # words = 81813\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2399\n",
      "all branching entropies was computed # words = 81820\n",
      "all accessor variety was computed # words = 81820\n",
      "'리재명'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81864\n",
      "all accessor variety was computed # words = 81864\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81866\n",
      "all accessor variety was computed # words = 81866\n",
      "'입갤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81870\n",
      "all accessor variety was computed # words = 81870\n",
      "'새민년'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81874\n",
      "all accessor variety was computed # words = 81874\n",
      "'유민이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81881\n",
      "all accessor variety was computed # words = 81881\n",
      "'국산과자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81897\n",
      "all accessor variety was computed # words = 81897\n",
      "'이정도는'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81898\n",
      "all accessor variety was computed # words = 81898\n",
      "'홍팍성님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81911\n",
      "all accessor variety was computed # words = 81911\n",
      "'일부심'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81935\n",
      "all accessor variety was computed # words = 81935\n",
      "'고인드립'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81954\n",
      "all accessor variety was computed # words = 81954\n",
      "'강아지들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81958\n",
      "all accessor variety was computed # words = 81958\n",
      "'시험기간'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81961\n",
      "all accessor variety was computed # words = 81961\n",
      "'바이나인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81961\n",
      "all accessor variety was computed # words = 81961\n",
      "'박서준'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81967\n",
      "all accessor variety was computed # words = 81967\n",
      "'나이차'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81972\n",
      "all accessor variety was computed # words = 81972\n",
      "'아이돌중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81985\n",
      "all accessor variety was computed # words = 81985\n",
      "'추가결혼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 81991\n",
      "all accessor variety was computed # words = 81991\n",
      "'예쁘게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82000\n",
      "all accessor variety was computed # words = 82000\n",
      "'추가내'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82000\n",
      "all accessor variety was computed # words = 82000\n",
      "'주결경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82002\n",
      "all accessor variety was computed # words = 82002\n",
      "'서강준'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82004\n",
      "all accessor variety was computed # words = 82004\n",
      "'남아이돌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82006\n",
      "all accessor variety was computed # words = 82006\n",
      "'일회용마스크'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82010\n",
      "all accessor variety was computed # words = 82010\n",
      "'감염자수'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82014\n",
      "all accessor variety was computed # words = 82014\n",
      "'감염경로'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82018\n",
      "all accessor variety was computed # words = 82018\n",
      "'손정의'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82019\n",
      "all accessor variety was computed # words = 82019\n",
      "'박시영'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82044\n",
      "all accessor variety was computed # words = 82044\n",
      "'흑인이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2401\n",
      "all branching entropies was computed # words = 82064\n",
      "all accessor variety was computed # words = 82064\n",
      "'민원넣었다'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 82089\n",
      "all accessor variety was computed # words = 82089\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2405\n",
      "all branching entropies was computed # words = 82108\n",
      "all accessor variety was computed # words = 82108\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2405\n",
      "all branching entropies was computed # words = 82111\n",
      "all accessor variety was computed # words = 82111\n",
      "'레노버'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2405\n",
      "all branching entropies was computed # words = 82125\n",
      "all accessor variety was computed # words = 82125\n",
      "'재미로'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2405\n",
      "all branching entropies was computed # words = 82126\n",
      "all accessor variety was computed # words = 82126\n",
      "'걸캅스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2408\n",
      "all branching entropies was computed # words = 82146\n",
      "all accessor variety was computed # words = 82146\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2408\n",
      "all branching entropies was computed # words = 82151\n",
      "all accessor variety was computed # words = 82151\n",
      "'달샤벳'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2408\n",
      "all branching entropies was computed # words = 82151\n",
      "all accessor variety was computed # words = 82151\n",
      "'오이수'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2408\n",
      "all branching entropies was computed # words = 82157\n",
      "all accessor variety was computed # words = 82157\n",
      "'한아름송이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2411\n",
      "all branching entropies was computed # words = 82160\n",
      "all accessor variety was computed # words = 82160\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2411\n",
      "all branching entropies was computed # words = 82185\n",
      "all accessor variety was computed # words = 82185\n",
      "'랜쳇'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2411\n",
      "all branching entropies was computed # words = 82189\n",
      "all accessor variety was computed # words = 82189\n",
      "'우주소녀'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2411\n",
      "all branching entropies was computed # words = 82189\n",
      "all accessor variety was computed # words = 82189\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2411\n",
      "all branching entropies was computed # words = 82197\n",
      "all accessor variety was computed # words = 82197\n",
      "'유툽'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2416\n",
      "all branching entropies was computed # words = 82235\n",
      "all accessor variety was computed # words = 82235\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2416\n",
      "all branching entropies was computed # words = 82240\n",
      "all accessor variety was computed # words = 82240\n",
      "'로린이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2419\n",
      "all branching entropies was computed # words = 82272\n",
      "all accessor variety was computed # words = 82272\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2419\n",
      "all branching entropies was computed # words = 82278\n",
      "all accessor variety was computed # words = 82278\n",
      "'과거사진'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2419\n",
      "all branching entropies was computed # words = 82280\n",
      "all accessor variety was computed # words = 82280\n",
      "'배달의민족'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2425\n",
      "all branching entropies was computed # words = 82373\n",
      "all accessor variety was computed # words = 82373\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2425\n",
      "all branching entropies was computed # words = 82377\n",
      "all accessor variety was computed # words = 82377\n",
      "'반일선동'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2428\n",
      "all branching entropies was computed # words = 82386\n",
      "all accessor variety was computed # words = 82386\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2428\n",
      "all branching entropies was computed # words = 82388\n",
      "all accessor variety was computed # words = 82388\n",
      "'죽이려고'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2428\n",
      "all branching entropies was computed # words = 82388\n",
      "all accessor variety was computed # words = 82388\n",
      "'여초반응'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2431\n",
      "all branching entropies was computed # words = 82416\n",
      "all accessor variety was computed # words = 82416\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2434\n",
      "all branching entropies was computed # words = 82435\n",
      "all accessor variety was computed # words = 82435\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2434\n",
      "all branching entropies was computed # words = 82436\n",
      "all accessor variety was computed # words = 82436\n",
      "'양준일'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2434\n",
      "all branching entropies was computed # words = 82439\n",
      "all accessor variety was computed # words = 82439\n",
      "'윤석렬'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2437\n",
      "all branching entropies was computed # words = 82490\n",
      "all accessor variety was computed # words = 82490\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2437\n",
      "all branching entropies was computed # words = 82490\n",
      "all accessor variety was computed # words = 82490\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2437\n",
      "all branching entropies was computed # words = 82498\n",
      "all accessor variety was computed # words = 82498\n",
      "'생기는일'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2437\n",
      "all branching entropies was computed # words = 82501\n",
      "all accessor variety was computed # words = 82501\n",
      "'전희경'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2437\n",
      "all branching entropies was computed # words = 82503\n",
      "all accessor variety was computed # words = 82503\n",
      "'레이양'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2439\n",
      "all branching entropies was computed # words = 82504\n",
      "all accessor variety was computed # words = 82504\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2442\n",
      "all branching entropies was computed # words = 82531\n",
      "all accessor variety was computed # words = 82531\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2442\n",
      "all branching entropies was computed # words = 82548\n",
      "all accessor variety was computed # words = 82548\n",
      "'쾌도난마'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2442\n",
      "all branching entropies was computed # words = 82557\n",
      "all accessor variety was computed # words = 82557\n",
      "'차유람'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2442\n",
      "all branching entropies was computed # words = 82563\n",
      "all accessor variety was computed # words = 82563\n",
      "'예원이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2442\n",
      "all branching entropies was computed # words = 82574\n",
      "all accessor variety was computed # words = 82574\n",
      "'미란다커'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 82583\n",
      "all accessor variety was computed # words = 82583\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82586\n",
      "all accessor variety was computed # words = 82586\n",
      "'일베돌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82594\n",
      "all accessor variety was computed # words = 82594\n",
      "'오유새끼들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82615\n",
      "all accessor variety was computed # words = 82615\n",
      "'스카이림'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82619\n",
      "all accessor variety was computed # words = 82619\n",
      "'법조게이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82621\n",
      "all accessor variety was computed # words = 82621\n",
      "'재익이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2444\n",
      "all branching entropies was computed # words = 82621\n",
      "all accessor variety was computed # words = 82621\n",
      "'브이로그'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82644\n",
      "all accessor variety was computed # words = 82644\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82644\n",
      "all accessor variety was computed # words = 82644\n",
      "'이진우'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82645\n",
      "all accessor variety was computed # words = 82645\n",
      "'백예린'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82654\n",
      "all accessor variety was computed # words = 82654\n",
      "'첵스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82665\n",
      "all accessor variety was computed # words = 82665\n",
      "'예쁨'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82669\n",
      "all accessor variety was computed # words = 82669\n",
      "'행사장'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82674\n",
      "all accessor variety was computed # words = 82674\n",
      "'성경험'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82704\n",
      "all accessor variety was computed # words = 82704\n",
      "'스키니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82704\n",
      "all accessor variety was computed # words = 82704\n",
      "'용준형'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82715\n",
      "all accessor variety was computed # words = 82715\n",
      "'설레게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82735\n",
      "all accessor variety was computed # words = 82735\n",
      "'궁금한이야기'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82742\n",
      "all accessor variety was computed # words = 82742\n",
      "'드라이브스루'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82745\n",
      "all accessor variety was computed # words = 82745\n",
      "'대구신천지'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82760\n",
      "all accessor variety was computed # words = 82760\n",
      "'나라들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82768\n",
      "all accessor variety was computed # words = 82768\n",
      "'살려고'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82771\n",
      "all accessor variety was computed # words = 82771\n",
      "'남규리'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2446\n",
      "all branching entropies was computed # words = 82775\n",
      "all accessor variety was computed # words = 82775\n",
      "'널널'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2450\n",
      "all branching entropies was computed # words = 82784\n",
      "all accessor variety was computed # words = 82784\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2450\n",
      "all branching entropies was computed # words = 82789\n",
      "all accessor variety was computed # words = 82789\n",
      "'테블릿'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2450\n",
      "all branching entropies was computed # words = 82791\n",
      "all accessor variety was computed # words = 82791\n",
      "'있는사람'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2452\n",
      "all branching entropies was computed # words = 82810\n",
      "all accessor variety was computed # words = 82810\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2454\n",
      "all branching entropies was computed # words = 82839\n",
      "all accessor variety was computed # words = 82839\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2454\n",
      "all branching entropies was computed # words = 82839\n",
      "all accessor variety was computed # words = 82839\n",
      "'일베논란'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2455\n",
      "all branching entropies was computed # words = 82872\n",
      "all accessor variety was computed # words = 82872\n",
      "'좆도'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2455\n",
      "all branching entropies was computed # words = 82889\n",
      "all accessor variety was computed # words = 82889\n",
      "'사이프리드'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2456\n",
      "all branching entropies was computed # words = 82899\n",
      "all accessor variety was computed # words = 82899\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2456\n",
      "all branching entropies was computed # words = 82911\n",
      "all accessor variety was computed # words = 82911\n",
      "'우한교민'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2459\n",
      "all branching entropies was computed # words = 82928\n",
      "all accessor variety was computed # words = 82928\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2459\n",
      "all branching entropies was computed # words = 82933\n",
      "all accessor variety was computed # words = 82933\n",
      "'여잔데'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2459\n",
      "all branching entropies was computed # words = 82935\n",
      "all accessor variety was computed # words = 82935\n",
      "'여진구'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2462\n",
      "all branching entropies was computed # words = 82935\n",
      "all accessor variety was computed # words = 82935\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2465\n",
      "all branching entropies was computed # words = 82964\n",
      "all accessor variety was computed # words = 82964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2466\n",
      "all branching entropies was computed # words = 82985\n",
      "all accessor variety was computed # words = 82985\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2466\n",
      "all branching entropies was computed # words = 82985\n",
      "all accessor variety was computed # words = 82985\n",
      "'대형기획사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2469\n",
      "all branching entropies was computed # words = 83090\n",
      "all accessor variety was computed # words = 83090\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2469\n",
      "all branching entropies was computed # words = 83094\n",
      "all accessor variety was computed # words = 83094\n",
      "'혜령이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2469\n",
      "all branching entropies was computed # words = 83122\n",
      "all accessor variety was computed # words = 83122\n",
      "'키스후기'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2472\n",
      "all branching entropies was computed # words = 83138\n",
      "all accessor variety was computed # words = 83138\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2475\n",
      "all branching entropies was computed # words = 83260\n",
      "all accessor variety was computed # words = 83260\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2478\n",
      "all branching entropies was computed # words = 83298\n",
      "all accessor variety was computed # words = 83298\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2481\n",
      "all branching entropies was computed # words = 83314\n",
      "all accessor variety was computed # words = 83314\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2481\n",
      "all branching entropies was computed # words = 83314\n",
      "all accessor variety was computed # words = 83314\n",
      "'안유진'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2481\n",
      "all branching entropies was computed # words = 83323\n",
      "all accessor variety was computed # words = 83323\n",
      "'웃긴짤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2481\n",
      "all branching entropies was computed # words = 83331\n",
      "all accessor variety was computed # words = 83331\n",
      "'보혐충전'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2481\n",
      "all branching entropies was computed # words = 83339\n",
      "all accessor variety was computed # words = 83339\n",
      "'급식들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2483\n",
      "all branching entropies was computed # words = 83379\n",
      "all accessor variety was computed # words = 83379\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2483\n",
      "all branching entropies was computed # words = 83381\n",
      "all accessor variety was computed # words = 83381\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2487\n",
      "all branching entropies was computed # words = 83410\n",
      "all accessor variety was computed # words = 83410\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2487\n",
      "all branching entropies was computed # words = 83410\n",
      "all accessor variety was computed # words = 83410\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2491\n",
      "all branching entropies was computed # words = 83424\n",
      "all accessor variety was computed # words = 83424\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2491\n",
      "all branching entropies was computed # words = 83431\n",
      "all accessor variety was computed # words = 83431\n",
      "'국회청원'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2491\n",
      "all branching entropies was computed # words = 83434\n",
      "all accessor variety was computed # words = 83434\n",
      "'소신발언'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2494\n",
      "all branching entropies was computed # words = 83474\n",
      "all accessor variety was computed # words = 83474\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2497\n",
      "all branching entropies was computed # words = 83509\n",
      "all accessor variety was computed # words = 83509\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2500\n",
      "all branching entropies was computed # words = 83523\n",
      "all accessor variety was computed # words = 83523\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2502\n",
      "all branching entropies was computed # words = 83542\n",
      "all accessor variety was computed # words = 83542\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2502\n",
      "all branching entropies was computed # words = 83542\n",
      "all accessor variety was computed # words = 83542\n",
      "'한녀들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2502\n",
      "all branching entropies was computed # words = 83560\n",
      "all accessor variety was computed # words = 83560\n",
      "'중국발'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83571\n",
      "all accessor variety was computed # words = 83571\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83572\n",
      "all accessor variety was computed # words = 83572\n",
      "'아담사우나'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83577\n",
      "all accessor variety was computed # words = 83577\n",
      "'무역전쟁'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83579\n",
      "all accessor variety was computed # words = 83579\n",
      "'평광옥'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83593\n",
      "all accessor variety was computed # words = 83593\n",
      "'장제원'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83601\n",
      "all accessor variety was computed # words = 83601\n",
      "'문정인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2503\n",
      "all branching entropies was computed # words = 83604\n",
      "all accessor variety was computed # words = 83604\n",
      "'탁현민'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2506\n",
      "all branching entropies was computed # words = 83617\n",
      "all accessor variety was computed # words = 83617\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2508\n",
      "all branching entropies was computed # words = 83649\n",
      "all accessor variety was computed # words = 83649\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2508\n",
      "all branching entropies was computed # words = 83654\n",
      "all accessor variety was computed # words = 83654\n",
      "'양성평등연대'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2515\n",
      "all branching entropies was computed # words = 83688\n",
      "all accessor variety was computed # words = 83688\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2515\n",
      "all branching entropies was computed # words = 83691\n",
      "all accessor variety was computed # words = 83691\n",
      "'장시호'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83716\n",
      "all accessor variety was computed # words = 83716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83718\n",
      "all accessor variety was computed # words = 83718\n",
      "'은수미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83721\n",
      "all accessor variety was computed # words = 83721\n",
      "'더민당'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83724\n",
      "all accessor variety was computed # words = 83724\n",
      "'노양심'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83734\n",
      "all accessor variety was computed # words = 83734\n",
      "'특별전형'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83735\n",
      "all accessor variety was computed # words = 83735\n",
      "'역저격'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83744\n",
      "all accessor variety was computed # words = 83744\n",
      "'막아야'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2518\n",
      "all branching entropies was computed # words = 83747\n",
      "all accessor variety was computed # words = 83747\n",
      "'유경근'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83750\n",
      "all accessor variety was computed # words = 83750\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83756\n",
      "all accessor variety was computed # words = 83756\n",
      "'일베간게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83760\n",
      "all accessor variety was computed # words = 83760\n",
      "'수아레즈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83778\n",
      "all accessor variety was computed # words = 83778\n",
      "'재업일베'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83786\n",
      "all accessor variety was computed # words = 83786\n",
      "'공룡풍선'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83788\n",
      "all accessor variety was computed # words = 83788\n",
      "'유대균'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83791\n",
      "all accessor variety was computed # words = 83791\n",
      "'홍띵보'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83791\n",
      "all accessor variety was computed # words = 83791\n",
      "'낸시랭이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83795\n",
      "all accessor variety was computed # words = 83795\n",
      "'패륜충'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83799\n",
      "all accessor variety was computed # words = 83799\n",
      "'파리바게트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83802\n",
      "all accessor variety was computed # words = 83802\n",
      "'골라주라'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83810\n",
      "all accessor variety was computed # words = 83810\n",
      "'에코백'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83812\n",
      "all accessor variety was computed # words = 83812\n",
      "'보는중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83823\n",
      "all accessor variety was computed # words = 83823\n",
      "'왓챠'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83823\n",
      "all accessor variety was computed # words = 83823\n",
      "'랩몬스터'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83834\n",
      "all accessor variety was computed # words = 83834\n",
      "'화류계'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83836\n",
      "all accessor variety was computed # words = 83836\n",
      "'백퍼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83843\n",
      "all accessor variety was computed # words = 83843\n",
      "'좋아하는거'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83851\n",
      "all accessor variety was computed # words = 83851\n",
      "'에픽게임즈'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83853\n",
      "all accessor variety was computed # words = 83853\n",
      "'셀카들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83863\n",
      "all accessor variety was computed # words = 83863\n",
      "'다른나라'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83870\n",
      "all accessor variety was computed # words = 83870\n",
      "'긴급사태'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83883\n",
      "all accessor variety was computed # words = 83883\n",
      "'차이나게이트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83884\n",
      "all accessor variety was computed # words = 83884\n",
      "'갑수목장'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2519\n",
      "all branching entropies was computed # words = 83889\n",
      "all accessor variety was computed # words = 83889\n",
      "'설현이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2522\n",
      "all branching entropies was computed # words = 83911\n",
      "all accessor variety was computed # words = 83911\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n",
      "all branching entropies was computed # words = 83947\n",
      "all accessor variety was computed # words = 83947\n",
      "'게이네'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n",
      "all branching entropies was computed # words = 83947\n",
      "all accessor variety was computed # words = 83947\n",
      "'쓰까국'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n",
      "all branching entropies was computed # words = 83947\n",
      "all accessor variety was computed # words = 83947\n",
      "'주간아이돌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n",
      "all branching entropies was computed # words = 83948\n",
      "all accessor variety was computed # words = 83948\n",
      "'워너원이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n",
      "all branching entropies was computed # words = 83950\n",
      "all accessor variety was computed # words = 83950\n",
      "'여신강림'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 83958\n",
      "all accessor variety was computed # words = 83958\n",
      "'예쁜애들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2530\n",
      "all branching entropies was computed # words = 83971\n",
      "all accessor variety was computed # words = 83971\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2530\n",
      "all branching entropies was computed # words = 83998\n",
      "all accessor variety was computed # words = 83998\n",
      "'임신중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2533\n",
      "all branching entropies was computed # words = 84024\n",
      "all accessor variety was computed # words = 84024\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2533\n",
      "all branching entropies was computed # words = 84034\n",
      "all accessor variety was computed # words = 84034\n",
      "'마스크값'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2533\n",
      "all branching entropies was computed # words = 84034\n",
      "all accessor variety was computed # words = 84034\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2533\n",
      "all branching entropies was computed # words = 84034\n",
      "all accessor variety was computed # words = 84034\n",
      "'한나식당'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2534\n",
      "all branching entropies was computed # words = 84067\n",
      "all accessor variety was computed # words = 84067\n",
      "'일째'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2534\n",
      "all branching entropies was computed # words = 84067\n",
      "all accessor variety was computed # words = 84067\n",
      "'영업글'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2534\n",
      "all branching entropies was computed # words = 84069\n",
      "all accessor variety was computed # words = 84069\n",
      "'신천지교회'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2534\n",
      "all branching entropies was computed # words = 84076\n",
      "all accessor variety was computed # words = 84076\n",
      "'추수꾼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2536\n",
      "all branching entropies was computed # words = 84089\n",
      "all accessor variety was computed # words = 84089\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2536\n",
      "all branching entropies was computed # words = 84095\n",
      "all accessor variety was computed # words = 84095\n",
      "'앰창인생'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2538\n",
      "all branching entropies was computed # words = 84117\n",
      "all accessor variety was computed # words = 84117\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2538\n",
      "all branching entropies was computed # words = 84120\n",
      "all accessor variety was computed # words = 84120\n",
      "'북한석탄'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2538\n",
      "all branching entropies was computed # words = 84123\n",
      "all accessor variety was computed # words = 84123\n",
      "'환풍구'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2538\n",
      "all branching entropies was computed # words = 84124\n",
      "all accessor variety was computed # words = 84124\n",
      "'재확진'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2541\n",
      "all branching entropies was computed # words = 84140\n",
      "all accessor variety was computed # words = 84140\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2541\n",
      "all branching entropies was computed # words = 84144\n",
      "all accessor variety was computed # words = 84144\n",
      "'문세먼지'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2544\n",
      "all branching entropies was computed # words = 84178\n",
      "all accessor variety was computed # words = 84178\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2547\n",
      "all branching entropies was computed # words = 84209\n",
      "all accessor variety was computed # words = 84209\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2549\n",
      "all branching entropies was computed # words = 84225\n",
      "all accessor variety was computed # words = 84225\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2552\n",
      "all branching entropies was computed # words = 84274\n",
      "all accessor variety was computed # words = 84274\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2552\n",
      "all branching entropies was computed # words = 84278\n",
      "all accessor variety was computed # words = 84278\n",
      "'똥남아'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2555\n",
      "all branching entropies was computed # words = 84284\n",
      "all accessor variety was computed # words = 84284\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2556\n",
      "all branching entropies was computed # words = 84310\n",
      "all accessor variety was computed # words = 84310\n",
      "'씹창'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2556\n",
      "all branching entropies was computed # words = 84316\n",
      "all accessor variety was computed # words = 84316\n",
      "'최군'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2557\n",
      "all branching entropies was computed # words = 84341\n",
      "all accessor variety was computed # words = 84341\n",
      "'현피'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2557\n",
      "all branching entropies was computed # words = 84345\n",
      "all accessor variety was computed # words = 84345\n",
      "'헬반도'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2560\n",
      "all branching entropies was computed # words = 84365\n",
      "all accessor variety was computed # words = 84365\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2560\n",
      "all branching entropies was computed # words = 84372\n",
      "all accessor variety was computed # words = 84372\n",
      "'박철상'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2562\n",
      "all branching entropies was computed # words = 84403\n",
      "all accessor variety was computed # words = 84403\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2562\n",
      "all branching entropies was computed # words = 84405\n",
      "all accessor variety was computed # words = 84405\n",
      "'학생증'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2562\n",
      "all branching entropies was computed # words = 84405\n",
      "all accessor variety was computed # words = 84405\n",
      "'울이니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2562\n",
      "all branching entropies was computed # words = 84413\n",
      "all accessor variety was computed # words = 84413\n",
      "'애게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2562\n",
      "all branching entropies was computed # words = 84418\n",
      "all accessor variety was computed # words = 84418\n",
      "'흐엉'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84480\n",
      "all accessor variety was computed # words = 84480\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84497\n",
      "all accessor variety was computed # words = 84497\n",
      "'원미경찰서'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84497\n",
      "all accessor variety was computed # words = 84497\n",
      "'마카롱게이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 84500\n",
      "all accessor variety was computed # words = 84500\n",
      "'힐링캠프'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84506\n",
      "all accessor variety was computed # words = 84506\n",
      "'단룡인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84506\n",
      "all accessor variety was computed # words = 84506\n",
      "'블랙넛'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84510\n",
      "all accessor variety was computed # words = 84510\n",
      "'딸통법'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2565\n",
      "all branching entropies was computed # words = 84511\n",
      "all accessor variety was computed # words = 84511\n",
      "'최룡해'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2567\n",
      "all branching entropies was computed # words = 84516\n",
      "all accessor variety was computed # words = 84516\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2569\n",
      "all branching entropies was computed # words = 84530\n",
      "all accessor variety was computed # words = 84530\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2572\n",
      "all branching entropies was computed # words = 84551\n",
      "all accessor variety was computed # words = 84551\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2572\n",
      "all branching entropies was computed # words = 84556\n",
      "all accessor variety was computed # words = 84556\n",
      "'과대포장'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2576\n",
      "all branching entropies was computed # words = 84565\n",
      "all accessor variety was computed # words = 84565\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2576\n",
      "all branching entropies was computed # words = 84577\n",
      "all accessor variety was computed # words = 84577\n",
      "'주인님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84600\n",
      "all accessor variety was computed # words = 84600\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84604\n",
      "all accessor variety was computed # words = 84604\n",
      "'마지막회'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84605\n",
      "all accessor variety was computed # words = 84605\n",
      "'스압정보'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84609\n",
      "all accessor variety was computed # words = 84609\n",
      "'샘해밍턴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84609\n",
      "all accessor variety was computed # words = 84609\n",
      "'여돌들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84610\n",
      "all accessor variety was computed # words = 84610\n",
      "'김국헌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84621\n",
      "all accessor variety was computed # words = 84621\n",
      "'좋아하는여자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84631\n",
      "all accessor variety was computed # words = 84631\n",
      "'반반결혼'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84646\n",
      "all accessor variety was computed # words = 84646\n",
      "'챙겨야'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84663\n",
      "all accessor variety was computed # words = 84663\n",
      "'결혼안'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84668\n",
      "all accessor variety was computed # words = 84668\n",
      "'남친짤'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84670\n",
      "all accessor variety was computed # words = 84670\n",
      "'윤보미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84677\n",
      "all accessor variety was computed # words = 84677\n",
      "'이럴때'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84692\n",
      "all accessor variety was computed # words = 84692\n",
      "'중딩얼짱'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84698\n",
      "all accessor variety was computed # words = 84698\n",
      "'한글이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84699\n",
      "all accessor variety was computed # words = 84699\n",
      "'선톡'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84702\n",
      "all accessor variety was computed # words = 84702\n",
      "'마스크가격'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84711\n",
      "all accessor variety was computed # words = 84711\n",
      "'격리시설'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84718\n",
      "all accessor variety was computed # words = 84718\n",
      "'프로포폴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84722\n",
      "all accessor variety was computed # words = 84722\n",
      "'핑크당'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84758\n",
      "all accessor variety was computed # words = 84758\n",
      "'격리중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84760\n",
      "all accessor variety was computed # words = 84760\n",
      "'네일베'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2577\n",
      "all branching entropies was computed # words = 84764\n",
      "all accessor variety was computed # words = 84764\n",
      "'재밌게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2581\n",
      "all branching entropies was computed # words = 84795\n",
      "all accessor variety was computed # words = 84795\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2584\n",
      "all branching entropies was computed # words = 84808\n",
      "all accessor variety was computed # words = 84808\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2584\n",
      "all branching entropies was computed # words = 84820\n",
      "all accessor variety was computed # words = 84820\n",
      "'속보노무'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2585\n",
      "all branching entropies was computed # words = 84913\n",
      "all accessor variety was computed # words = 84913\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2585\n",
      "all branching entropies was computed # words = 84926\n",
      "all accessor variety was computed # words = 84926\n",
      "'김남국'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2585\n",
      "all branching entropies was computed # words = 84931\n",
      "all accessor variety was computed # words = 84931\n",
      "'지오영'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2585\n",
      "all branching entropies was computed # words = 84937\n",
      "all accessor variety was computed # words = 84937\n",
      "'부장판사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2590\n",
      "all branching entropies was computed # words = 84966\n",
      "all accessor variety was computed # words = 84966\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2590\n",
      "all branching entropies was computed # words = 84969\n",
      "all accessor variety was computed # words = 84969\n",
      "'장훈이형'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85004\n",
      "all accessor variety was computed # words = 85004\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85010\n",
      "all accessor variety was computed # words = 85010\n",
      "'친정부모님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85016\n",
      "all accessor variety was computed # words = 85016\n",
      "'오또'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85016\n",
      "all accessor variety was computed # words = 85016\n",
      "'브금주의'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85022\n",
      "all accessor variety was computed # words = 85022\n",
      "'종현이'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85030\n",
      "all accessor variety was computed # words = 85030\n",
      "'개념상실'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2592\n",
      "all branching entropies was computed # words = 85043\n",
      "all accessor variety was computed # words = 85043\n",
      "'사이코패스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2594\n",
      "all branching entropies was computed # words = 85048\n",
      "all accessor variety was computed # words = 85048\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2594\n",
      "all branching entropies was computed # words = 85056\n",
      "all accessor variety was computed # words = 85056\n",
      "'온천교회'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2595\n",
      "all branching entropies was computed # words = 85144\n",
      "all accessor variety was computed # words = 85144\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2595\n",
      "all branching entropies was computed # words = 85155\n",
      "all accessor variety was computed # words = 85155\n",
      "'신메뉴'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2599\n",
      "all branching entropies was computed # words = 85179\n",
      "all accessor variety was computed # words = 85179\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2599\n",
      "all branching entropies was computed # words = 85183\n",
      "all accessor variety was computed # words = 85183\n",
      "'동물의숲'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2602\n",
      "all branching entropies was computed # words = 85197\n",
      "all accessor variety was computed # words = 85197\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2607\n",
      "all branching entropies was computed # words = 85220\n",
      "all accessor variety was computed # words = 85220\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2612\n",
      "all branching entropies was computed # words = 85239\n",
      "all accessor variety was computed # words = 85239\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2612\n",
      "all branching entropies was computed # words = 85245\n",
      "all accessor variety was computed # words = 85245\n",
      "'김겨쿨'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2615\n",
      "all branching entropies was computed # words = 85284\n",
      "all accessor variety was computed # words = 85284\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2615\n",
      "all branching entropies was computed # words = 85293\n",
      "all accessor variety was computed # words = 85293\n",
      "'신라젠'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2618\n",
      "all branching entropies was computed # words = 85313\n",
      "all accessor variety was computed # words = 85313\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2619\n",
      "all branching entropies was computed # words = 85351\n",
      "all accessor variety was computed # words = 85351\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2619\n",
      "all branching entropies was computed # words = 85367\n",
      "all accessor variety was computed # words = 85367\n",
      "'무고죄'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85384\n",
      "all accessor variety was computed # words = 85384\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85389\n",
      "all accessor variety was computed # words = 85389\n",
      "'문꿀오소리'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85390\n",
      "all accessor variety was computed # words = 85390\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85390\n",
      "all accessor variety was computed # words = 85390\n",
      "'급식충들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85391\n",
      "all accessor variety was computed # words = 85391\n",
      "'댕댕이들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85391\n",
      "all accessor variety was computed # words = 85391\n",
      "'치매설'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2621\n",
      "all branching entropies was computed # words = 85397\n",
      "all accessor variety was computed # words = 85397\n",
      "'누나들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85407\n",
      "all accessor variety was computed # words = 85407\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85412\n",
      "all accessor variety was computed # words = 85412\n",
      "'이정미'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85420\n",
      "all accessor variety was computed # words = 85420\n",
      "'황희찬'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85432\n",
      "all accessor variety was computed # words = 85432\n",
      "'의원들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85438\n",
      "all accessor variety was computed # words = 85438\n",
      "'한중일'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2623\n",
      "all branching entropies was computed # words = 85449\n",
      "all accessor variety was computed # words = 85449\n",
      "'여성전용칸'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85457\n",
      "all accessor variety was computed # words = 85457\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85461\n",
      "all accessor variety was computed # words = 85461\n",
      "'박해진'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85485\n",
      "all accessor variety was computed # words = 85485\n",
      "'회장님'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85494\n",
      "all accessor variety was computed # words = 85494\n",
      "'허니버터칩'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85495\n",
      "all accessor variety was computed # words = 85495\n",
      "'레이디제인'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2626\n",
      "all branching entropies was computed # words = 85508\n",
      "all accessor variety was computed # words = 85508\n",
      "'랜덤채팅'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2630\n",
      "all branching entropies was computed # words = 85525\n",
      "all accessor variety was computed # words = 85525\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2630\n",
      "all branching entropies was computed # words = 85535\n",
      "all accessor variety was computed # words = 85535\n",
      "'담배값'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2630\n",
      "all branching entropies was computed # words = 85553\n",
      "all accessor variety was computed # words = 85553\n",
      "'김형식'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85576\n",
      "all accessor variety was computed # words = 85576\n",
      "'불펌'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85583\n",
      "all accessor variety was computed # words = 85583\n",
      "'버스커버스커'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85593\n",
      "all accessor variety was computed # words = 85593\n",
      "'아사다마오'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85602\n",
      "all accessor variety was computed # words = 85602\n",
      "'방사능국'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85605\n",
      "all accessor variety was computed # words = 85605\n",
      "'새벽감성'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85609\n",
      "all accessor variety was computed # words = 85609\n",
      "'설국열차'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85615\n",
      "all accessor variety was computed # words = 85615\n",
      "'신고완료'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85618\n",
      "all accessor variety was computed # words = 85618\n",
      "'자랑하고'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2631\n",
      "all branching entropies was computed # words = 85621\n",
      "all accessor variety was computed # words = 85621\n",
      "'나재민'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85684\n",
      "all accessor variety was computed # words = 85684\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85685\n",
      "all accessor variety was computed # words = 85685\n",
      "'채령'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85695\n",
      "all accessor variety was computed # words = 85695\n",
      "'예비새언니'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85698\n",
      "all accessor variety was computed # words = 85698\n",
      "'남자들한테'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85712\n",
      "all accessor variety was computed # words = 85712\n",
      "'시댁식구들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85721\n",
      "all accessor variety was computed # words = 85721\n",
      "'다른여자'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85735\n",
      "all accessor variety was computed # words = 85735\n",
      "'음원차트'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85735\n",
      "all accessor variety was computed # words = 85735\n",
      "'카톡프사'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85745\n",
      "all accessor variety was computed # words = 85745\n",
      "'이쁜애들'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85757\n",
      "all accessor variety was computed # words = 85757\n",
      "'프로미스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85763\n",
      "all accessor variety was computed # words = 85763\n",
      "'원더걸스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85764\n",
      "all accessor variety was computed # words = 85764\n",
      "'제왑'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85796\n",
      "all accessor variety was computed # words = 85796\n",
      "'않게'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85804\n",
      "all accessor variety was computed # words = 85804\n",
      "'알리익스프레스'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85808\n",
      "all accessor variety was computed # words = 85808\n",
      "'보고있는데'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85812\n",
      "all accessor variety was computed # words = 85812\n",
      "'나을까요'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85834\n",
      "all accessor variety was computed # words = 85834\n",
      "'대기중'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85841\n",
      "all accessor variety was computed # words = 85841\n",
      "'인가족'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2633\n",
      "all branching entropies was computed # words = 85843\n",
      "all accessor variety was computed # words = 85843\n",
      "'빅맥'\n",
      "training was done. used memory 1.052 Gb1.052 Gb\n",
      "all cohesion probabilities was computed. # words = 2635\n",
      "all branching entropies was computed # words = 85897\n",
      "all accessor variety was computed # words = 85897\n",
      "'거죠'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 87108\n",
      "all accessor variety was computed # words = 87108\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2668\n",
      "all branching entropies was computed # words = 87132\n",
      "all accessor variety was computed # words = 87132\n",
      "'먹었는데'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2668\n",
      "all branching entropies was computed # words = 87133\n",
      "all accessor variety was computed # words = 87133\n",
      "'가요대제전'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87156\n",
      "all accessor variety was computed # words = 87156\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87158\n",
      "all accessor variety was computed # words = 87158\n",
      "'대깨문들이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87158\n",
      "all accessor variety was computed # words = 87158\n",
      "'악플후기'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87223\n",
      "all accessor variety was computed # words = 87223\n",
      "'두원공대'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87228\n",
      "all accessor variety was computed # words = 87228\n",
      "'틸러슨'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87230\n",
      "all accessor variety was computed # words = 87230\n",
      "'전정국'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87230\n",
      "all accessor variety was computed # words = 87230\n",
      "'전여친이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87240\n",
      "all accessor variety was computed # words = 87240\n",
      "'나쁜년'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87243\n",
      "all accessor variety was computed # words = 87243\n",
      "'우주여신'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87243\n",
      "all accessor variety was computed # words = 87243\n",
      "'남재준'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87251\n",
      "all accessor variety was computed # words = 87251\n",
      "'하이바이마마'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2670\n",
      "all branching entropies was computed # words = 87266\n",
      "all accessor variety was computed # words = 87266\n",
      "'못사는'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2671\n",
      "all branching entropies was computed # words = 87278\n",
      "all accessor variety was computed # words = 87278\n",
      "'좆목'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2673\n",
      "all branching entropies was computed # words = 87301\n",
      "all accessor variety was computed # words = 87301\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2673\n",
      "all branching entropies was computed # words = 87306\n",
      "all accessor variety was computed # words = 87306\n",
      "'황금연휴'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2675\n",
      "all branching entropies was computed # words = 87338\n",
      "all accessor variety was computed # words = 87338\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2678\n",
      "all branching entropies was computed # words = 87347\n",
      "all accessor variety was computed # words = 87347\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2678\n",
      "all branching entropies was computed # words = 87354\n",
      "all accessor variety was computed # words = 87354\n",
      "'애국집회'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2679\n",
      "all branching entropies was computed # words = 87370\n",
      "all accessor variety was computed # words = 87370\n",
      "'좆문'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2680\n",
      "all branching entropies was computed # words = 87391\n",
      "all accessor variety was computed # words = 87391\n",
      "'될듯'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2680\n",
      "all branching entropies was computed # words = 87406\n",
      "all accessor variety was computed # words = 87406\n",
      "'이쁘지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2682\n",
      "all branching entropies was computed # words = 87414\n",
      "all accessor variety was computed # words = 87414\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2682\n",
      "all branching entropies was computed # words = 87437\n",
      "all accessor variety was computed # words = 87437\n",
      "'번출구'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2685\n",
      "all branching entropies was computed # words = 87449\n",
      "all accessor variety was computed # words = 87449\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87464\n",
      "all accessor variety was computed # words = 87464\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87464\n",
      "all accessor variety was computed # words = 87464\n",
      "'장자연'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87469\n",
      "all accessor variety was computed # words = 87469\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87474\n",
      "all accessor variety was computed # words = 87474\n",
      "'병신년'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87480\n",
      "all accessor variety was computed # words = 87480\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87480\n",
      "all accessor variety was computed # words = 87480\n",
      "'문빠들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2690\n",
      "all branching entropies was computed # words = 87483\n",
      "all accessor variety was computed # words = 87483\n",
      "'건강보험료'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2695\n",
      "all branching entropies was computed # words = 87506\n",
      "all accessor variety was computed # words = 87506\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2698\n",
      "all branching entropies was computed # words = 87515\n",
      "all accessor variety was computed # words = 87515\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2698\n",
      "all branching entropies was computed # words = 87516\n",
      "all accessor variety was computed # words = 87516\n",
      "'사모펀드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2698\n",
      "all branching entropies was computed # words = 87529\n",
      "all accessor variety was computed # words = 87529\n",
      "'한국년들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2701\n",
      "all branching entropies was computed # words = 87549\n",
      "all accessor variety was computed # words = 87549\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 87558\n",
      "all accessor variety was computed # words = 87558\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2703\n",
      "all branching entropies was computed # words = 87562\n",
      "all accessor variety was computed # words = 87562\n",
      "'조현병'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2706\n",
      "all branching entropies was computed # words = 87615\n",
      "all accessor variety was computed # words = 87615\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2710\n",
      "all branching entropies was computed # words = 87645\n",
      "all accessor variety was computed # words = 87645\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2712\n",
      "all branching entropies was computed # words = 87674\n",
      "all accessor variety was computed # words = 87674\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2715\n",
      "all branching entropies was computed # words = 87692\n",
      "all accessor variety was computed # words = 87692\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2717\n",
      "all branching entropies was computed # words = 87711\n",
      "all accessor variety was computed # words = 87711\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2717\n",
      "all branching entropies was computed # words = 87714\n",
      "all accessor variety was computed # words = 87714\n",
      "'의전원'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2718\n",
      "all branching entropies was computed # words = 87758\n",
      "all accessor variety was computed # words = 87758\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2718\n",
      "all branching entropies was computed # words = 87763\n",
      "all accessor variety was computed # words = 87763\n",
      "'치트키'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2719\n",
      "all branching entropies was computed # words = 87815\n",
      "all accessor variety was computed # words = 87815\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2719\n",
      "all branching entropies was computed # words = 87819\n",
      "all accessor variety was computed # words = 87819\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2719\n",
      "all branching entropies was computed # words = 87822\n",
      "all accessor variety was computed # words = 87822\n",
      "'달창년들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2719\n",
      "all branching entropies was computed # words = 87830\n",
      "all accessor variety was computed # words = 87830\n",
      "'탄핵기각'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2722\n",
      "all branching entropies was computed # words = 87854\n",
      "all accessor variety was computed # words = 87854\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2722\n",
      "all branching entropies was computed # words = 87855\n",
      "all accessor variety was computed # words = 87855\n",
      "'문죄앙'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2724\n",
      "all branching entropies was computed # words = 87867\n",
      "all accessor variety was computed # words = 87867\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2725\n",
      "all branching entropies was computed # words = 87904\n",
      "all accessor variety was computed # words = 87904\n",
      "'보혐'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2725\n",
      "all branching entropies was computed # words = 87920\n",
      "all accessor variety was computed # words = 87920\n",
      "'어버이연합'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2725\n",
      "all branching entropies was computed # words = 87923\n",
      "all accessor variety was computed # words = 87923\n",
      "'김무성이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2727\n",
      "all branching entropies was computed # words = 87959\n",
      "all accessor variety was computed # words = 87959\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 87978\n",
      "all accessor variety was computed # words = 87978\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 87982\n",
      "all accessor variety was computed # words = 87982\n",
      "'박상영'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 87990\n",
      "all accessor variety was computed # words = 87990\n",
      "'좌제동'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 87999\n",
      "all accessor variety was computed # words = 87999\n",
      "'하연수'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88012\n",
      "all accessor variety was computed # words = 88012\n",
      "'머중이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88013\n",
      "all accessor variety was computed # words = 88013\n",
      "'커제'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88017\n",
      "all accessor variety was computed # words = 88017\n",
      "'대북방송'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88023\n",
      "all accessor variety was computed # words = 88023\n",
      "'매드맥스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88023\n",
      "all accessor variety was computed # words = 88023\n",
      "'김영오씨'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88023\n",
      "all accessor variety was computed # words = 88023\n",
      "'좆목질'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2729\n",
      "all branching entropies was computed # words = 88032\n",
      "all accessor variety was computed # words = 88032\n",
      "'문채원'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2732\n",
      "all branching entropies was computed # words = 88049\n",
      "all accessor variety was computed # words = 88049\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2732\n",
      "all branching entropies was computed # words = 88074\n",
      "all accessor variety was computed # words = 88074\n",
      "'소트니코바'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2732\n",
      "all branching entropies was computed # words = 88078\n",
      "all accessor variety was computed # words = 88078\n",
      "'으리성님'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88085\n",
      "all accessor variety was computed # words = 88085\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88095\n",
      "all accessor variety was computed # words = 88095\n",
      "'내란음모'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88110\n",
      "all accessor variety was computed # words = 88110\n",
      "'현자타'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88118\n",
      "all accessor variety was computed # words = 88118\n",
      "'정보게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 88119\n",
      "all accessor variety was computed # words = 88119\n",
      "'동엽신'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88128\n",
      "all accessor variety was computed # words = 88128\n",
      "'미니홈피'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2734\n",
      "all branching entropies was computed # words = 88130\n",
      "all accessor variety was computed # words = 88130\n",
      "'선비들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2737\n",
      "all branching entropies was computed # words = 88133\n",
      "all accessor variety was computed # words = 88133\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2737\n",
      "all branching entropies was computed # words = 88138\n",
      "all accessor variety was computed # words = 88138\n",
      "'해밍턴'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2737\n",
      "all branching entropies was computed # words = 88142\n",
      "all accessor variety was computed # words = 88142\n",
      "'장도연'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2737\n",
      "all branching entropies was computed # words = 88147\n",
      "all accessor variety was computed # words = 88147\n",
      "'스터디카페'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2741\n",
      "all branching entropies was computed # words = 88161\n",
      "all accessor variety was computed # words = 88161\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2741\n",
      "all branching entropies was computed # words = 88172\n",
      "all accessor variety was computed # words = 88172\n",
      "'결혼할때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2745\n",
      "all branching entropies was computed # words = 88185\n",
      "all accessor variety was computed # words = 88185\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2745\n",
      "all branching entropies was computed # words = 88205\n",
      "all accessor variety was computed # words = 88205\n",
      "'집순이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2746\n",
      "all branching entropies was computed # words = 88222\n",
      "all accessor variety was computed # words = 88222\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2747\n",
      "all branching entropies was computed # words = 88250\n",
      "all accessor variety was computed # words = 88250\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2747\n",
      "all branching entropies was computed # words = 88256\n",
      "all accessor variety was computed # words = 88256\n",
      "'키즈카페'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2747\n",
      "all branching entropies was computed # words = 88260\n",
      "all accessor variety was computed # words = 88260\n",
      "'팬사인회'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2747\n",
      "all branching entropies was computed # words = 88264\n",
      "all accessor variety was computed # words = 88264\n",
      "'내용추가'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2747\n",
      "all branching entropies was computed # words = 88287\n",
      "all accessor variety was computed # words = 88287\n",
      "'친엄마'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88312\n",
      "all accessor variety was computed # words = 88312\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88323\n",
      "all accessor variety was computed # words = 88323\n",
      "'다른느낌'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88350\n",
      "all accessor variety was computed # words = 88350\n",
      "'동거중'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88352\n",
      "all accessor variety was computed # words = 88352\n",
      "'답정너'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88352\n",
      "all accessor variety was computed # words = 88352\n",
      "'송지은'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88378\n",
      "all accessor variety was computed # words = 88378\n",
      "'쿤이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88380\n",
      "all accessor variety was computed # words = 88380\n",
      "'버즈플러스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88382\n",
      "all accessor variety was computed # words = 88382\n",
      "'신세계네요'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88384\n",
      "all accessor variety was computed # words = 88384\n",
      "'사전예약'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88387\n",
      "all accessor variety was computed # words = 88387\n",
      "'새로고침'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88395\n",
      "all accessor variety was computed # words = 88395\n",
      "'유럽증시'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88401\n",
      "all accessor variety was computed # words = 88401\n",
      "'형보수지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88404\n",
      "all accessor variety was computed # words = 88404\n",
      "'돼지들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2749\n",
      "all branching entropies was computed # words = 88410\n",
      "all accessor variety was computed # words = 88410\n",
      "'개훌륭'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2752\n",
      "all branching entropies was computed # words = 88423\n",
      "all accessor variety was computed # words = 88423\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2752\n",
      "all branching entropies was computed # words = 88425\n",
      "all accessor variety was computed # words = 88425\n",
      "'이민혁'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2752\n",
      "all branching entropies was computed # words = 88435\n",
      "all accessor variety was computed # words = 88435\n",
      "'우한서'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2752\n",
      "all branching entropies was computed # words = 88448\n",
      "all accessor variety was computed # words = 88448\n",
      "'온도차'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88499\n",
      "all accessor variety was computed # words = 88499\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88499\n",
      "all accessor variety was computed # words = 88499\n",
      "'추가확진자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88506\n",
      "all accessor variety was computed # words = 88506\n",
      "'조이서'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88513\n",
      "all accessor variety was computed # words = 88513\n",
      "'나냉'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88527\n",
      "all accessor variety was computed # words = 88527\n",
      "'추가제'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88536\n",
      "all accessor variety was computed # words = 88536\n",
      "'청원동의'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88557\n",
      "all accessor variety was computed # words = 88557\n",
      "'야옹이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2755\n",
      "all branching entropies was computed # words = 88569\n",
      "all accessor variety was computed # words = 88569\n",
      "'판빙빙'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2756\n",
      "all branching entropies was computed # words = 88598\n",
      "all accessor variety was computed # words = 88598\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2757\n",
      "all branching entropies was computed # words = 88653\n",
      "all accessor variety was computed # words = 88653\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2760\n",
      "all branching entropies was computed # words = 88695\n",
      "all accessor variety was computed # words = 88695\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2762\n",
      "all branching entropies was computed # words = 88714\n",
      "all accessor variety was computed # words = 88714\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2762\n",
      "all branching entropies was computed # words = 88717\n",
      "all accessor variety was computed # words = 88717\n",
      "'자급제'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2762\n",
      "all branching entropies was computed # words = 88720\n",
      "all accessor variety was computed # words = 88720\n",
      "'사용중'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2762\n",
      "all branching entropies was computed # words = 88721\n",
      "all accessor variety was computed # words = 88721\n",
      "'광화문광장'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2762\n",
      "all branching entropies was computed # words = 88724\n",
      "all accessor variety was computed # words = 88724\n",
      "'궁금한점'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2767\n",
      "all branching entropies was computed # words = 88739\n",
      "all accessor variety was computed # words = 88739\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2769\n",
      "all branching entropies was computed # words = 88762\n",
      "all accessor variety was computed # words = 88762\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2769\n",
      "all branching entropies was computed # words = 88762\n",
      "all accessor variety was computed # words = 88762\n",
      "'씹선비들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2769\n",
      "all branching entropies was computed # words = 88780\n",
      "all accessor variety was computed # words = 88780\n",
      "'렙새끼'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2769\n",
      "all branching entropies was computed # words = 88780\n",
      "all accessor variety was computed # words = 88780\n",
      "'연예병사'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2769\n",
      "all branching entropies was computed # words = 88786\n",
      "all accessor variety was computed # words = 88786\n",
      "'죽여버리고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2772\n",
      "all branching entropies was computed # words = 88810\n",
      "all accessor variety was computed # words = 88810\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88816\n",
      "all accessor variety was computed # words = 88816\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88820\n",
      "all accessor variety was computed # words = 88820\n",
      "'보여준다'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88820\n",
      "all accessor variety was computed # words = 88820\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88820\n",
      "all accessor variety was computed # words = 88820\n",
      "'좆같은거'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88832\n",
      "all accessor variety was computed # words = 88832\n",
      "'실업급여'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88849\n",
      "all accessor variety was computed # words = 88849\n",
      "'성접대'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88849\n",
      "all accessor variety was computed # words = 88849\n",
      "'후전드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88849\n",
      "all accessor variety was computed # words = 88849\n",
      "'지디게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2775\n",
      "all branching entropies was computed # words = 88862\n",
      "all accessor variety was computed # words = 88862\n",
      "'다음달'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2778\n",
      "all branching entropies was computed # words = 88874\n",
      "all accessor variety was computed # words = 88874\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2781\n",
      "all branching entropies was computed # words = 88922\n",
      "all accessor variety was computed # words = 88922\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2781\n",
      "all branching entropies was computed # words = 88924\n",
      "all accessor variety was computed # words = 88924\n",
      "'우갤러'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89001\n",
      "all accessor variety was computed # words = 89001\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89001\n",
      "all accessor variety was computed # words = 89001\n",
      "'유승민이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89001\n",
      "all accessor variety was computed # words = 89001\n",
      "'사드보복'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89007\n",
      "all accessor variety was computed # words = 89007\n",
      "'호뽑뽑요'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89010\n",
      "all accessor variety was computed # words = 89010\n",
      "'대북제재'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89017\n",
      "all accessor variety was computed # words = 89017\n",
      "'투블럭'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89017\n",
      "all accessor variety was computed # words = 89017\n",
      "'심석희'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2783\n",
      "all branching entropies was computed # words = 89018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 89018\n",
      "'조금전'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2785\n",
      "all branching entropies was computed # words = 89027\n",
      "all accessor variety was computed # words = 89027\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2785\n",
      "all branching entropies was computed # words = 89031\n",
      "all accessor variety was computed # words = 89031\n",
      "'양진호'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2788\n",
      "all branching entropies was computed # words = 89089\n",
      "all accessor variety was computed # words = 89089\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89129\n",
      "all accessor variety was computed # words = 89129\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89134\n",
      "all accessor variety was computed # words = 89134\n",
      "'명짤'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89136\n",
      "all accessor variety was computed # words = 89136\n",
      "'황교안총리'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89142\n",
      "all accessor variety was computed # words = 89142\n",
      "'추성훈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89145\n",
      "all accessor variety was computed # words = 89145\n",
      "'박원슨'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89151\n",
      "all accessor variety was computed # words = 89151\n",
      "'이겼다'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2791\n",
      "all branching entropies was computed # words = 89154\n",
      "all accessor variety was computed # words = 89154\n",
      "'무상복지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2793\n",
      "all branching entropies was computed # words = 89170\n",
      "all accessor variety was computed # words = 89170\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2795\n",
      "all branching entropies was computed # words = 89191\n",
      "all accessor variety was computed # words = 89191\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2795\n",
      "all branching entropies was computed # words = 89200\n",
      "all accessor variety was computed # words = 89200\n",
      "'똥송'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2795\n",
      "all branching entropies was computed # words = 89204\n",
      "all accessor variety was computed # words = 89204\n",
      "'팩트티비'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2799\n",
      "all branching entropies was computed # words = 89211\n",
      "all accessor variety was computed # words = 89211\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2799\n",
      "all branching entropies was computed # words = 89211\n",
      "all accessor variety was computed # words = 89211\n",
      "'불광역'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2799\n",
      "all branching entropies was computed # words = 89211\n",
      "all accessor variety was computed # words = 89211\n",
      "'머전머'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89256\n",
      "all accessor variety was computed # words = 89256\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89260\n",
      "all accessor variety was computed # words = 89260\n",
      "'언딘'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89271\n",
      "all accessor variety was computed # words = 89271\n",
      "'미디어워치'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89271\n",
      "all accessor variety was computed # words = 89271\n",
      "'일베새끼들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89271\n",
      "all accessor variety was computed # words = 89271\n",
      "'보밍아웃'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89272\n",
      "all accessor variety was computed # words = 89272\n",
      "'김종훈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89324\n",
      "all accessor variety was computed # words = 89324\n",
      "'워해머'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89327\n",
      "all accessor variety was computed # words = 89327\n",
      "'아이돌그룹'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89335\n",
      "all accessor variety was computed # words = 89335\n",
      "'폴딩박스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89342\n",
      "all accessor variety was computed # words = 89342\n",
      "'직장생활'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89345\n",
      "all accessor variety was computed # words = 89345\n",
      "'쓰는데'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89348\n",
      "all accessor variety was computed # words = 89348\n",
      "'짤털이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89348\n",
      "all accessor variety was computed # words = 89348\n",
      "'화제성'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2802\n",
      "all branching entropies was computed # words = 89353\n",
      "all accessor variety was computed # words = 89353\n",
      "'식약처'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2805\n",
      "all branching entropies was computed # words = 89426\n",
      "all accessor variety was computed # words = 89426\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2806\n",
      "all branching entropies was computed # words = 89438\n",
      "all accessor variety was computed # words = 89438\n",
      "'최애'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2806\n",
      "all branching entropies was computed # words = 89442\n",
      "all accessor variety was computed # words = 89442\n",
      "'롬앤'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2808\n",
      "all branching entropies was computed # words = 89447\n",
      "all accessor variety was computed # words = 89447\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2808\n",
      "all branching entropies was computed # words = 89447\n",
      "all accessor variety was computed # words = 89447\n",
      "'프듀때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2808\n",
      "all branching entropies was computed # words = 89453\n",
      "all accessor variety was computed # words = 89453\n",
      "'나연이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2808\n",
      "all branching entropies was computed # words = 89459\n",
      "all accessor variety was computed # words = 89459\n",
      "'대후반'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 89495\n",
      "all accessor variety was computed # words = 89495\n",
      "'봤을때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89496\n",
      "all accessor variety was computed # words = 89496\n",
      "'조유리'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89497\n",
      "all accessor variety was computed # words = 89497\n",
      "'김진우'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89499\n",
      "all accessor variety was computed # words = 89499\n",
      "'스마일클럽'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89509\n",
      "all accessor variety was computed # words = 89509\n",
      "'악동뮤지션'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89511\n",
      "all accessor variety was computed # words = 89511\n",
      "'터졌네요'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89511\n",
      "all accessor variety was computed # words = 89511\n",
      "'박형식'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89524\n",
      "all accessor variety was computed # words = 89524\n",
      "'흔남'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89528\n",
      "all accessor variety was computed # words = 89528\n",
      "'봉준호감독'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89535\n",
      "all accessor variety was computed # words = 89535\n",
      "'사이비종교'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89538\n",
      "all accessor variety was computed # words = 89538\n",
      "'대리구매'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89551\n",
      "all accessor variety was computed # words = 89551\n",
      "'은혜의강'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89557\n",
      "all accessor variety was computed # words = 89557\n",
      "'미국증시'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89568\n",
      "all accessor variety was computed # words = 89568\n",
      "'가려고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89569\n",
      "all accessor variety was computed # words = 89569\n",
      "'이시언'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2809\n",
      "all branching entropies was computed # words = 89574\n",
      "all accessor variety was computed # words = 89574\n",
      "'백승수'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2810\n",
      "all branching entropies was computed # words = 89613\n",
      "all accessor variety was computed # words = 89613\n",
      "'준다고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2810\n",
      "all branching entropies was computed # words = 89637\n",
      "all accessor variety was computed # words = 89637\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2810\n",
      "all branching entropies was computed # words = 89639\n",
      "all accessor variety was computed # words = 89639\n",
      "'간경화'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2810\n",
      "all branching entropies was computed # words = 89642\n",
      "all accessor variety was computed # words = 89642\n",
      "'무적권'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2810\n",
      "all branching entropies was computed # words = 89654\n",
      "all accessor variety was computed # words = 89654\n",
      "'전사자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89719\n",
      "all accessor variety was computed # words = 89719\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89719\n",
      "all accessor variety was computed # words = 89719\n",
      "'길가다가'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89723\n",
      "all accessor variety was computed # words = 89723\n",
      "'들어와서'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89728\n",
      "all accessor variety was computed # words = 89728\n",
      "'백파더'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89729\n",
      "all accessor variety was computed # words = 89729\n",
      "'스트레이키즈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89732\n",
      "all accessor variety was computed # words = 89732\n",
      "'부직포마스크'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89746\n",
      "all accessor variety was computed # words = 89746\n",
      "'업자들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89756\n",
      "all accessor variety was computed # words = 89756\n",
      "'몽주니어'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89770\n",
      "all accessor variety was computed # words = 89770\n",
      "'강제정모'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89777\n",
      "all accessor variety was computed # words = 89777\n",
      "'저격방금'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89781\n",
      "all accessor variety was computed # words = 89781\n",
      "'라이브방송'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89785\n",
      "all accessor variety was computed # words = 89785\n",
      "'민생당'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89786\n",
      "all accessor variety was computed # words = 89786\n",
      "'웹툰작'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89787\n",
      "all accessor variety was computed # words = 89787\n",
      "'김영만'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89789\n",
      "all accessor variety was computed # words = 89789\n",
      "'도경수'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89799\n",
      "all accessor variety was computed # words = 89799\n",
      "'사촌오빠'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2812\n",
      "all branching entropies was computed # words = 89804\n",
      "all accessor variety was computed # words = 89804\n",
      "'발암주'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2813\n",
      "all branching entropies was computed # words = 89832\n",
      "all accessor variety was computed # words = 89832\n",
      "'성괴'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2813\n",
      "all branching entropies was computed # words = 89841\n",
      "all accessor variety was computed # words = 89841\n",
      "'노력충'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2814\n",
      "all branching entropies was computed # words = 89860\n",
      "all accessor variety was computed # words = 89860\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2814\n",
      "all branching entropies was computed # words = 89869\n",
      "all accessor variety was computed # words = 89869\n",
      "'예쁘지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2814\n",
      "all branching entropies was computed # words = 89871\n",
      "all accessor variety was computed # words = 89871\n",
      "'컨담'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2820\n",
      "all branching entropies was computed # words = 89910\n",
      "all accessor variety was computed # words = 89910\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2820\n",
      "all branching entropies was computed # words = 89921\n",
      "all accessor variety was computed # words = 89921\n",
      "'폭풍성장'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2820\n",
      "all branching entropies was computed # words = 89923\n",
      "all accessor variety was computed # words = 89923\n",
      "'발퀄'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2821\n",
      "all branching entropies was computed # words = 89935\n",
      "all accessor variety was computed # words = 89935\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2822\n",
      "all branching entropies was computed # words = 89975\n",
      "all accessor variety was computed # words = 89975\n",
      "'일뽕'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2822\n",
      "all branching entropies was computed # words = 89978\n",
      "all accessor variety was computed # words = 89978\n",
      "'그짝'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90016\n",
      "all accessor variety was computed # words = 90016\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90020\n",
      "all accessor variety was computed # words = 90020\n",
      "'신천지들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90020\n",
      "all accessor variety was computed # words = 90020\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90026\n",
      "all accessor variety was computed # words = 90026\n",
      "'깨시민'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90026\n",
      "all accessor variety was computed # words = 90026\n",
      "'무개념녀'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90047\n",
      "all accessor variety was computed # words = 90047\n",
      "'무한리필'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90047\n",
      "all accessor variety was computed # words = 90047\n",
      "'목격짤'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90050\n",
      "all accessor variety was computed # words = 90050\n",
      "'미래한국당'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90053\n",
      "all accessor variety was computed # words = 90053\n",
      "'우한코로나'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90062\n",
      "all accessor variety was computed # words = 90062\n",
      "'카트라이더'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90065\n",
      "all accessor variety was computed # words = 90065\n",
      "'오픈채팅'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2824\n",
      "all branching entropies was computed # words = 90085\n",
      "all accessor variety was computed # words = 90085\n",
      "'으리미엄'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2833\n",
      "all branching entropies was computed # words = 90119\n",
      "all accessor variety was computed # words = 90119\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2837\n",
      "all branching entropies was computed # words = 90128\n",
      "all accessor variety was computed # words = 90128\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2840\n",
      "all branching entropies was computed # words = 90146\n",
      "all accessor variety was computed # words = 90146\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2840\n",
      "all branching entropies was computed # words = 90151\n",
      "all accessor variety was computed # words = 90151\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2840\n",
      "all branching entropies was computed # words = 90154\n",
      "all accessor variety was computed # words = 90154\n",
      "'백수게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2843\n",
      "all branching entropies was computed # words = 90203\n",
      "all accessor variety was computed # words = 90203\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2843\n",
      "all branching entropies was computed # words = 90207\n",
      "all accessor variety was computed # words = 90207\n",
      "'숙노'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2843\n",
      "all branching entropies was computed # words = 90207\n",
      "all accessor variety was computed # words = 90207\n",
      "'김세의기자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2843\n",
      "all branching entropies was computed # words = 90216\n",
      "all accessor variety was computed # words = 90216\n",
      "'만경봉호'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2843\n",
      "all branching entropies was computed # words = 90224\n",
      "all accessor variety was computed # words = 90224\n",
      "'개새끼인'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2848\n",
      "all branching entropies was computed # words = 90280\n",
      "all accessor variety was computed # words = 90280\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2850\n",
      "all branching entropies was computed # words = 90285\n",
      "all accessor variety was computed # words = 90285\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2850\n",
      "all branching entropies was computed # words = 90289\n",
      "all accessor variety was computed # words = 90289\n",
      "'문쩝쩝'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2850\n",
      "all branching entropies was computed # words = 90296\n",
      "all accessor variety was computed # words = 90296\n",
      "'백마들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2851\n",
      "all branching entropies was computed # words = 90313\n",
      "all accessor variety was computed # words = 90313\n",
      "'남혐'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2857\n",
      "all branching entropies was computed # words = 90327\n",
      "all accessor variety was computed # words = 90327\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2857\n",
      "all branching entropies was computed # words = 90329\n",
      "all accessor variety was computed # words = 90329\n",
      "'김머중'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2857\n",
      "all branching entropies was computed # words = 90335\n",
      "all accessor variety was computed # words = 90335\n",
      "'댕청'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2857\n",
      "all branching entropies was computed # words = 90340\n",
      "all accessor variety was computed # words = 90340\n",
      "'제롯데월드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90383\n",
      "all accessor variety was computed # words = 90383\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90391\n",
      "all accessor variety was computed # words = 90391\n",
      "'네네치킨'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90394\n",
      "all accessor variety was computed # words = 90394\n",
      "'갈베년들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90394\n",
      "all accessor variety was computed # words = 90394\n",
      "'뽐거지들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90408\n",
      "all accessor variety was computed # words = 90408\n",
      "'테이큰'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90412\n",
      "all accessor variety was computed # words = 90412\n",
      "'전승절'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90413\n",
      "all accessor variety was computed # words = 90413\n",
      "'유족들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90413\n",
      "all accessor variety was computed # words = 90413\n",
      "'박종진'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90416\n",
      "all accessor variety was computed # words = 90416\n",
      "'물총게이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90417\n",
      "all accessor variety was computed # words = 90417\n",
      "'육회게이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90424\n",
      "all accessor variety was computed # words = 90424\n",
      "'이혼게이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90427\n",
      "all accessor variety was computed # words = 90427\n",
      "'체고조넘'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90427\n",
      "all accessor variety was computed # words = 90427\n",
      "'스토리를'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90429\n",
      "all accessor variety was computed # words = 90429\n",
      "'미현이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90441\n",
      "all accessor variety was computed # words = 90441\n",
      "'감독님'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90445\n",
      "all accessor variety was computed # words = 90445\n",
      "'원채준'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90448\n",
      "all accessor variety was computed # words = 90448\n",
      "'세로드립'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90457\n",
      "all accessor variety was computed # words = 90457\n",
      "'튜토리얼'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90458\n",
      "all accessor variety was computed # words = 90458\n",
      "'소련여자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90467\n",
      "all accessor variety was computed # words = 90467\n",
      "'강예빈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90478\n",
      "all accessor variety was computed # words = 90478\n",
      "'노짱이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90481\n",
      "all accessor variety was computed # words = 90481\n",
      "'토렌트'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2862\n",
      "all branching entropies was computed # words = 90482\n",
      "all accessor variety was computed # words = 90482\n",
      "'최효종'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90502\n",
      "all accessor variety was computed # words = 90502\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90511\n",
      "all accessor variety was computed # words = 90511\n",
      "'정규직화'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90517\n",
      "all accessor variety was computed # words = 90517\n",
      "'추천부탁'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90523\n",
      "all accessor variety was computed # words = 90523\n",
      "'연금복권'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90526\n",
      "all accessor variety was computed # words = 90526\n",
      "'로투킹'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90530\n",
      "all accessor variety was computed # words = 90530\n",
      "'최신글'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90532\n",
      "all accessor variety was computed # words = 90532\n",
      "'연생들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2865\n",
      "all branching entropies was computed # words = 90535\n",
      "all accessor variety was computed # words = 90535\n",
      "'조두팔'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90600\n",
      "all accessor variety was computed # words = 90600\n",
      "'남들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90620\n",
      "all accessor variety was computed # words = 90620\n",
      "'사기결혼'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90633\n",
      "all accessor variety was computed # words = 90633\n",
      "'우리오빠'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90635\n",
      "all accessor variety was computed # words = 90635\n",
      "'주간아'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90651\n",
      "all accessor variety was computed # words = 90651\n",
      "'에릭남'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 90652\n",
      "all accessor variety was computed # words = 90652\n",
      "'공식색'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90693\n",
      "all accessor variety was computed # words = 90693\n",
      "'예랑이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90706\n",
      "all accessor variety was computed # words = 90706\n",
      "'기럭지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90706\n",
      "all accessor variety was computed # words = 90706\n",
      "'김태리'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90707\n",
      "all accessor variety was computed # words = 90707\n",
      "'첸백시'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90708\n",
      "all accessor variety was computed # words = 90708\n",
      "'춤선'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90709\n",
      "all accessor variety was computed # words = 90709\n",
      "'억뷰'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90714\n",
      "all accessor variety was computed # words = 90714\n",
      "'미쓰에이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2866\n",
      "all branching entropies was computed # words = 90722\n",
      "all accessor variety was computed # words = 90722\n",
      "'선불카드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90792\n",
      "all accessor variety was computed # words = 90792\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90794\n",
      "all accessor variety was computed # words = 90794\n",
      "'이보영'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90794\n",
      "all accessor variety was computed # words = 90794\n",
      "'유연석'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90796\n",
      "all accessor variety was computed # words = 90796\n",
      "'장윤주'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90800\n",
      "all accessor variety was computed # words = 90800\n",
      "'퇴원자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90806\n",
      "all accessor variety was computed # words = 90806\n",
      "'패션플러스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90813\n",
      "all accessor variety was computed # words = 90813\n",
      "'종이의집'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90813\n",
      "all accessor variety was computed # words = 90813\n",
      "'첫주문'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90833\n",
      "all accessor variety was computed # words = 90833\n",
      "'바꿔야'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90858\n",
      "all accessor variety was computed # words = 90858\n",
      "'될것'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90862\n",
      "all accessor variety was computed # words = 90862\n",
      "'유민상'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90862\n",
      "all accessor variety was computed # words = 90862\n",
      "'글올렸다'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2867\n",
      "all branching entropies was computed # words = 90868\n",
      "all accessor variety was computed # words = 90868\n",
      "'빠순이들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2869\n",
      "all branching entropies was computed # words = 90876\n",
      "all accessor variety was computed # words = 90876\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2869\n",
      "all branching entropies was computed # words = 90877\n",
      "all accessor variety was computed # words = 90877\n",
      "'이성경'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2869\n",
      "all branching entropies was computed # words = 90877\n",
      "all accessor variety was computed # words = 90877\n",
      "'코로나확진자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2869\n",
      "all branching entropies was computed # words = 90882\n",
      "all accessor variety was computed # words = 90882\n",
      "'비싸게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90939\n",
      "all accessor variety was computed # words = 90939\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90945\n",
      "all accessor variety was computed # words = 90945\n",
      "'샨샤댐'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90946\n",
      "all accessor variety was computed # words = 90946\n",
      "'임종석이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90949\n",
      "all accessor variety was computed # words = 90949\n",
      "'송영선'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90961\n",
      "all accessor variety was computed # words = 90961\n",
      "'검정고무신'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90965\n",
      "all accessor variety was computed # words = 90965\n",
      "'뭐지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2874\n",
      "all branching entropies was computed # words = 90971\n",
      "all accessor variety was computed # words = 90971\n",
      "'배경화면'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2877\n",
      "all branching entropies was computed # words = 90976\n",
      "all accessor variety was computed # words = 90976\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2879\n",
      "all branching entropies was computed # words = 90995\n",
      "all accessor variety was computed # words = 90995\n",
      "'되는건'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2879\n",
      "all branching entropies was computed # words = 91008\n",
      "all accessor variety was computed # words = 91008\n",
      "'어떤여자'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2879\n",
      "all branching entropies was computed # words = 91024\n",
      "all accessor variety was computed # words = 91024\n",
      "'이쁘게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2879\n",
      "all branching entropies was computed # words = 91024\n",
      "all accessor variety was computed # words = 91024\n",
      "'이불밖'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2881\n",
      "all branching entropies was computed # words = 91052\n",
      "all accessor variety was computed # words = 91052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2882\n",
      "all branching entropies was computed # words = 91093\n",
      "all accessor variety was computed # words = 91093\n",
      "'현타'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2884\n",
      "all branching entropies was computed # words = 91097\n",
      "all accessor variety was computed # words = 91097\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2884\n",
      "all branching entropies was computed # words = 91099\n",
      "all accessor variety was computed # words = 91099\n",
      "'장현승'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2884\n",
      "all branching entropies was computed # words = 91105\n",
      "all accessor variety was computed # words = 91105\n",
      "'올블랙'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2884\n",
      "all branching entropies was computed # words = 91108\n",
      "all accessor variety was computed # words = 91108\n",
      "'탄핵반대'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2884\n",
      "all branching entropies was computed # words = 91109\n",
      "all accessor variety was computed # words = 91109\n",
      "'고두림'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2885\n",
      "all branching entropies was computed # words = 91129\n",
      "all accessor variety was computed # words = 91129\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2885\n",
      "all branching entropies was computed # words = 91129\n",
      "all accessor variety was computed # words = 91129\n",
      "'추가함'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2886\n",
      "all branching entropies was computed # words = 91162\n",
      "all accessor variety was computed # words = 91162\n",
      "'살때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2886\n",
      "all branching entropies was computed # words = 91166\n",
      "all accessor variety was computed # words = 91166\n",
      "'속보북'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2886\n",
      "all branching entropies was computed # words = 91173\n",
      "all accessor variety was computed # words = 91173\n",
      "'취준생'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91182\n",
      "all accessor variety was computed # words = 91182\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91182\n",
      "all accessor variety was computed # words = 91182\n",
      "'바이럴'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91188\n",
      "all accessor variety was computed # words = 91188\n",
      "'자전거충'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91188\n",
      "all accessor variety was computed # words = 91188\n",
      "'좆고딩들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91193\n",
      "all accessor variety was computed # words = 91193\n",
      "'예쁜거'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2887\n",
      "all branching entropies was computed # words = 91196\n",
      "all accessor variety was computed # words = 91196\n",
      "'한마음아파트'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91222\n",
      "all accessor variety was computed # words = 91222\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91235\n",
      "all accessor variety was computed # words = 91235\n",
      "'페메'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91247\n",
      "all accessor variety was computed # words = 91247\n",
      "'냥찡'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91254\n",
      "all accessor variety was computed # words = 91254\n",
      "'애기엄마'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91259\n",
      "all accessor variety was computed # words = 91259\n",
      "'띵곡'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91262\n",
      "all accessor variety was computed # words = 91262\n",
      "'슬의'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2889\n",
      "all branching entropies was computed # words = 91271\n",
      "all accessor variety was computed # words = 91271\n",
      "'사귀는거'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2894\n",
      "all branching entropies was computed # words = 91299\n",
      "all accessor variety was computed # words = 91299\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2899\n",
      "all branching entropies was computed # words = 91318\n",
      "all accessor variety was computed # words = 91318\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2899\n",
      "all branching entropies was computed # words = 91321\n",
      "all accessor variety was computed # words = 91321\n",
      "'일본맥주'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2899\n",
      "all branching entropies was computed # words = 91324\n",
      "all accessor variety was computed # words = 91324\n",
      "'몽골게이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2899\n",
      "all branching entropies was computed # words = 91326\n",
      "all accessor variety was computed # words = 91326\n",
      "'댓글부대'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2899\n",
      "all branching entropies was computed # words = 91332\n",
      "all accessor variety was computed # words = 91332\n",
      "'맘까페'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2902\n",
      "all branching entropies was computed # words = 91375\n",
      "all accessor variety was computed # words = 91375\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2902\n",
      "all branching entropies was computed # words = 91381\n",
      "all accessor variety was computed # words = 91381\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2903\n",
      "all branching entropies was computed # words = 91391\n",
      "all accessor variety was computed # words = 91391\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2905\n",
      "all branching entropies was computed # words = 91402\n",
      "all accessor variety was computed # words = 91402\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2905\n",
      "all branching entropies was computed # words = 91405\n",
      "all accessor variety was computed # words = 91405\n",
      "'정숙이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2905\n",
      "all branching entropies was computed # words = 91408\n",
      "all accessor variety was computed # words = 91408\n",
      "'나오고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2908\n",
      "all branching entropies was computed # words = 91443\n",
      "all accessor variety was computed # words = 91443\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2908\n",
      "all branching entropies was computed # words = 91460\n",
      "all accessor variety was computed # words = 91460\n",
      "'안마방'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2908\n",
      "all branching entropies was computed # words = 91464\n",
      "all accessor variety was computed # words = 91464\n",
      "'우한시'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2911\n",
      "all branching entropies was computed # words = 91493\n",
      "all accessor variety was computed # words = 91493\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2911\n",
      "all branching entropies was computed # words = 91501\n",
      "all accessor variety was computed # words = 91501\n",
      "'헬갤러'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2915\n",
      "all branching entropies was computed # words = 91527\n",
      "all accessor variety was computed # words = 91527\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2915\n",
      "all branching entropies was computed # words = 91527\n",
      "all accessor variety was computed # words = 91527\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2915\n",
      "all branching entropies was computed # words = 91532\n",
      "all accessor variety was computed # words = 91532\n",
      "'임나연'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2915\n",
      "all branching entropies was computed # words = 91535\n",
      "all accessor variety was computed # words = 91535\n",
      "'차명진'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2915\n",
      "all branching entropies was computed # words = 91535\n",
      "all accessor variety was computed # words = 91535\n",
      "'잘가라'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91606\n",
      "all accessor variety was computed # words = 91606\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91612\n",
      "all accessor variety was computed # words = 91612\n",
      "'배틀그라운드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91617\n",
      "all accessor variety was computed # words = 91617\n",
      "'토지공개념'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91620\n",
      "all accessor variety was computed # words = 91620\n",
      "'성상품화'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91629\n",
      "all accessor variety was computed # words = 91629\n",
      "'한끼줍쇼'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91630\n",
      "all accessor variety was computed # words = 91630\n",
      "'좆불집회'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91636\n",
      "all accessor variety was computed # words = 91636\n",
      "'기자님'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91647\n",
      "all accessor variety was computed # words = 91647\n",
      "'좆본'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91648\n",
      "all accessor variety was computed # words = 91648\n",
      "'인랑'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91648\n",
      "all accessor variety was computed # words = 91648\n",
      "'국회의원들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2917\n",
      "all branching entropies was computed # words = 91663\n",
      "all accessor variety was computed # words = 91663\n",
      "'장기매매'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91676\n",
      "all accessor variety was computed # words = 91676\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91676\n",
      "all accessor variety was computed # words = 91676\n",
      "'시체팔이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91681\n",
      "all accessor variety was computed # words = 91681\n",
      "'헛저격'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91682\n",
      "all accessor variety was computed # words = 91682\n",
      "'손인증'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91682\n",
      "all accessor variety was computed # words = 91682\n",
      "'서하준'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91684\n",
      "all accessor variety was computed # words = 91684\n",
      "'황총리'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91689\n",
      "all accessor variety was computed # words = 91689\n",
      "'전여옥'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91693\n",
      "all accessor variety was computed # words = 91693\n",
      "'미슐랭'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91703\n",
      "all accessor variety was computed # words = 91703\n",
      "'통합진보당'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91707\n",
      "all accessor variety was computed # words = 91707\n",
      "'치킨게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91710\n",
      "all accessor variety was computed # words = 91710\n",
      "'의느님'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91710\n",
      "all accessor variety was computed # words = 91710\n",
      "'갓동민'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91711\n",
      "all accessor variety was computed # words = 91711\n",
      "'갓치녀'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91711\n",
      "all accessor variety was computed # words = 91711\n",
      "'림수경'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91711\n",
      "all accessor variety was computed # words = 91711\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91718\n",
      "all accessor variety was computed # words = 91718\n",
      "'슴가'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2919\n",
      "all branching entropies was computed # words = 91719\n",
      "all accessor variety was computed # words = 91719\n",
      "'통수용팝'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91743\n",
      "all accessor variety was computed # words = 91743\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91758\n",
      "all accessor variety was computed # words = 91758\n",
      "'일베송'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91760\n",
      "all accessor variety was computed # words = 91760\n",
      "'고추걸'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 91773\n",
      "'중딩때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91775\n",
      "all accessor variety was computed # words = 91775\n",
      "'문용린'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91778\n",
      "all accessor variety was computed # words = 91778\n",
      "'토르부인'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91782\n",
      "all accessor variety was computed # words = 91782\n",
      "'보슬아치'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91783\n",
      "all accessor variety was computed # words = 91783\n",
      "'뜬금포'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2921\n",
      "all branching entropies was computed # words = 91791\n",
      "all accessor variety was computed # words = 91791\n",
      "'오원춘'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2922\n",
      "all branching entropies was computed # words = 91806\n",
      "all accessor variety was computed # words = 91806\n",
      "'냥이들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91839\n",
      "all accessor variety was computed # words = 91839\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91839\n",
      "all accessor variety was computed # words = 91839\n",
      "'신전떡볶이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91839\n",
      "all accessor variety was computed # words = 91839\n",
      "'올리브영'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91852\n",
      "all accessor variety was computed # words = 91852\n",
      "'혼전임신'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91853\n",
      "all accessor variety was computed # words = 91853\n",
      "'요리스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91858\n",
      "all accessor variety was computed # words = 91858\n",
      "'이동욱'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91858\n",
      "all accessor variety was computed # words = 91858\n",
      "'병찬이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91871\n",
      "all accessor variety was computed # words = 91871\n",
      "'재밌어'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2925\n",
      "all branching entropies was computed # words = 91877\n",
      "all accessor variety was computed # words = 91877\n",
      "'배달비'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2926\n",
      "all branching entropies was computed # words = 91891\n",
      "all accessor variety was computed # words = 91891\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91913\n",
      "all accessor variety was computed # words = 91913\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91926\n",
      "all accessor variety was computed # words = 91926\n",
      "'팬싸인회'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91934\n",
      "all accessor variety was computed # words = 91934\n",
      "'다른사람'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91950\n",
      "all accessor variety was computed # words = 91950\n",
      "'연애할때'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91962\n",
      "all accessor variety was computed # words = 91962\n",
      "'사촌언니'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91962\n",
      "all accessor variety was computed # words = 91962\n",
      "'대기획사'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91967\n",
      "all accessor variety was computed # words = 91967\n",
      "'종방연'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91977\n",
      "all accessor variety was computed # words = 91977\n",
      "'이런말'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91991\n",
      "all accessor variety was computed # words = 91991\n",
      "'개월째'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91993\n",
      "all accessor variety was computed # words = 91993\n",
      "'루키즈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 91999\n",
      "all accessor variety was computed # words = 91999\n",
      "'강남미'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92000\n",
      "all accessor variety was computed # words = 92000\n",
      "'사기캐'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92001\n",
      "all accessor variety was computed # words = 92001\n",
      "'망가녀'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92003\n",
      "all accessor variety was computed # words = 92003\n",
      "'느낀게'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92020\n",
      "all accessor variety was computed # words = 92020\n",
      "'폭발한'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92023\n",
      "all accessor variety was computed # words = 92023\n",
      "'유연정'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92054\n",
      "all accessor variety was computed # words = 92054\n",
      "'집밥'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92071\n",
      "all accessor variety was computed # words = 92071\n",
      "'스코티쉬폴드'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92077\n",
      "all accessor variety was computed # words = 92077\n",
      "'전남자친구'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92080\n",
      "all accessor variety was computed # words = 92080\n",
      "'양요섭'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92082\n",
      "all accessor variety was computed # words = 92082\n",
      "'엠블랙'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92097\n",
      "all accessor variety was computed # words = 92097\n",
      "'일찐'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92098\n",
      "all accessor variety was computed # words = 92098\n",
      "'빠던'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92098\n",
      "all accessor variety was computed # words = 92098\n",
      "'청도대남병원'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92109\n",
      "all accessor variety was computed # words = 92109\n",
      "'렘데시비르'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92114\n",
      "all accessor variety was computed # words = 92114\n",
      "'일본정부'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92117\n",
      "all accessor variety was computed # words = 92117\n",
      "'미국주식'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92117\n",
      "all accessor variety was computed # words = 92117\n",
      "'코로나맵'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92121\n",
      "all accessor variety was computed # words = 92121\n",
      "'개학연기'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92125\n",
      "all accessor variety was computed # words = 92125\n",
      "'멸균우유'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92129\n",
      "all accessor variety was computed # words = 92129\n",
      "'바꼈네요'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92131\n",
      "all accessor variety was computed # words = 92131\n",
      "'종양일보'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92134\n",
      "all accessor variety was computed # words = 92134\n",
      "'중대본'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92134\n",
      "all accessor variety was computed # words = 92134\n",
      "'명추가'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92140\n",
      "all accessor variety was computed # words = 92140\n",
      "'집근처'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92150\n",
      "all accessor variety was computed # words = 92150\n",
      "'업체들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92153\n",
      "all accessor variety was computed # words = 92153\n",
      "'개독들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92160\n",
      "all accessor variety was computed # words = 92160\n",
      "'아비간'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92164\n",
      "all accessor variety was computed # words = 92164\n",
      "'맞다고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2927\n",
      "all branching entropies was computed # words = 92164\n",
      "all accessor variety was computed # words = 92164\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2931\n",
      "all branching entropies was computed # words = 92194\n",
      "all accessor variety was computed # words = 92194\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2931\n",
      "all branching entropies was computed # words = 92196\n",
      "all accessor variety was computed # words = 92196\n",
      "'노영희'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2935\n",
      "all branching entropies was computed # words = 92220\n",
      "all accessor variety was computed # words = 92220\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2935\n",
      "all branching entropies was computed # words = 92225\n",
      "all accessor variety was computed # words = 92225\n",
      "'난민들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2935\n",
      "all branching entropies was computed # words = 92225\n",
      "all accessor variety was computed # words = 92225\n",
      "'사이버포뮬러'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2935\n",
      "all branching entropies was computed # words = 92226\n",
      "all accessor variety was computed # words = 92226\n",
      "'한남들'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2935\n",
      "all branching entropies was computed # words = 92226\n",
      "all accessor variety was computed # words = 92226\n",
      "'확통'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2938\n",
      "all branching entropies was computed # words = 92229\n",
      "all accessor variety was computed # words = 92229\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2938\n",
      "all branching entropies was computed # words = 92234\n",
      "all accessor variety was computed # words = 92234\n",
      "'제탄'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2939\n",
      "all branching entropies was computed # words = 92245\n",
      "all accessor variety was computed # words = 92245\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2939\n",
      "all branching entropies was computed # words = 92257\n",
      "all accessor variety was computed # words = 92257\n",
      "'마인크래프트'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2939\n",
      "all branching entropies was computed # words = 92257\n",
      "all accessor variety was computed # words = 92257\n",
      "'스무디킹'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2939\n",
      "all branching entropies was computed # words = 92257\n",
      "all accessor variety was computed # words = 92257\n",
      "'위버스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2939\n",
      "all branching entropies was computed # words = 92264\n",
      "all accessor variety was computed # words = 92264\n",
      "'많지'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92267\n",
      "all accessor variety was computed # words = 92267\n",
      "'재명이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92268\n",
      "all accessor variety was computed # words = 92268\n",
      "'현피게이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92273\n",
      "all accessor variety was computed # words = 92273\n",
      "'마재윤'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92283\n",
      "all accessor variety was computed # words = 92283\n",
      "'소독용'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92290\n",
      "all accessor variety was computed # words = 92290\n",
      "'아빠어디'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2941\n",
      "all branching entropies was computed # words = 92292\n",
      "all accessor variety was computed # words = 92292\n",
      "'아다뗐다'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2942\n",
      "all branching entropies was computed # words = 92312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 92312\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2942\n",
      "all branching entropies was computed # words = 92315\n",
      "all accessor variety was computed # words = 92315\n",
      "'내동생'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2942\n",
      "all branching entropies was computed # words = 92326\n",
      "all accessor variety was computed # words = 92326\n",
      "'짜파게티'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92339\n",
      "all accessor variety was computed # words = 92339\n",
      "'한짤'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92357\n",
      "all accessor variety was computed # words = 92357\n",
      "'비비고'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92364\n",
      "all accessor variety was computed # words = 92364\n",
      "'연락두절'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92383\n",
      "all accessor variety was computed # words = 92383\n",
      "'강형욱'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92383\n",
      "all accessor variety was computed # words = 92383\n",
      "'채널에이'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92383\n",
      "all accessor variety was computed # words = 92383\n",
      "'정보약스압'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92392\n",
      "all accessor variety was computed # words = 92392\n",
      "'데뷔초'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92396\n",
      "all accessor variety was computed # words = 92396\n",
      "'쪽국'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92400\n",
      "all accessor variety was computed # words = 92400\n",
      "'브금스압'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92413\n",
      "all accessor variety was computed # words = 92413\n",
      "'일베간놈'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2947\n",
      "all branching entropies was computed # words = 92414\n",
      "all accessor variety was computed # words = 92414\n",
      "'시뉴스'\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2949\n",
      "all branching entropies was computed # words = 92423\n",
      "all accessor variety was computed # words = 92423\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2951\n",
      "all branching entropies was computed # words = 92453\n",
      "all accessor variety was computed # words = 92453\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2954\n",
      "all branching entropies was computed # words = 92459\n",
      "all accessor variety was computed # words = 92459\n",
      "training was done. used memory 1.067 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2962\n",
      "all branching entropies was computed # words = 92506\n",
      "all accessor variety was computed # words = 92506\n",
      "training was done. used memory 1.096 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2962\n",
      "all branching entropies was computed # words = 92522\n",
      "all accessor variety was computed # words = 92522\n",
      "'여성가족부'\n",
      "training was done. used memory 1.087 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2962\n",
      "all branching entropies was computed # words = 92522\n",
      "all accessor variety was computed # words = 92522\n",
      "'속보문재인'\n",
      "training was done. used memory 1.096 Gb1.087 Gb\n",
      "all cohesion probabilities was computed. # words = 2962\n",
      "all branching entropies was computed # words = 92531\n",
      "all accessor variety was computed # words = 92531\n",
      "'물류창고'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2962\n",
      "all branching entropies was computed # words = 92537\n",
      "all accessor variety was computed # words = 92537\n",
      "'후원인증'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2965\n",
      "all branching entropies was computed # words = 92569\n",
      "all accessor variety was computed # words = 92569\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2965\n",
      "all branching entropies was computed # words = 92571\n",
      "all accessor variety was computed # words = 92571\n",
      "'착짱죽짱'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2965\n",
      "all branching entropies was computed # words = 92588\n",
      "all accessor variety was computed # words = 92588\n",
      "'문대통령'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2967\n",
      "all branching entropies was computed # words = 92648\n",
      "all accessor variety was computed # words = 92648\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2967\n",
      "all branching entropies was computed # words = 92648\n",
      "all accessor variety was computed # words = 92648\n",
      "'쓰리섬'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2969\n",
      "all branching entropies was computed # words = 92651\n",
      "all accessor variety was computed # words = 92651\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2969\n",
      "all branching entropies was computed # words = 92663\n",
      "all accessor variety was computed # words = 92663\n",
      "'일본차'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2969\n",
      "all branching entropies was computed # words = 92672\n",
      "all accessor variety was computed # words = 92672\n",
      "'흑사병'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2969\n",
      "all branching entropies was computed # words = 92673\n",
      "all accessor variety was computed # words = 92673\n",
      "'빤스런'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2972\n",
      "all branching entropies was computed # words = 92680\n",
      "all accessor variety was computed # words = 92680\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2972\n",
      "all branching entropies was computed # words = 92682\n",
      "all accessor variety was computed # words = 92682\n",
      "'조적조'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2974\n",
      "all branching entropies was computed # words = 92695\n",
      "all accessor variety was computed # words = 92695\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2974\n",
      "all branching entropies was computed # words = 92699\n",
      "all accessor variety was computed # words = 92699\n",
      "'재검표'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2974\n",
      "all branching entropies was computed # words = 92701\n",
      "all accessor variety was computed # words = 92701\n",
      "'김미나'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2974\n",
      "all branching entropies was computed # words = 92705\n",
      "all accessor variety was computed # words = 92705\n",
      "'황운하'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2974\n",
      "all branching entropies was computed # words = 92712\n",
      "all accessor variety was computed # words = 92712\n",
      "'달창들'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2977\n",
      "all branching entropies was computed # words = 92740\n",
      "all accessor variety was computed # words = 92740\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 92747\n",
      "all accessor variety was computed # words = 92747\n",
      "'유병재'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2977\n",
      "all branching entropies was computed # words = 92747\n",
      "all accessor variety was computed # words = 92747\n",
      "'우갤'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2978\n",
      "all branching entropies was computed # words = 92785\n",
      "all accessor variety was computed # words = 92785\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2979\n",
      "all branching entropies was computed # words = 92822\n",
      "all accessor variety was computed # words = 92822\n",
      "'부랄'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2980\n",
      "all branching entropies was computed # words = 92843\n",
      "all accessor variety was computed # words = 92843\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2980\n",
      "all branching entropies was computed # words = 92849\n",
      "all accessor variety was computed # words = 92849\n",
      "'달빛창녀단'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2980\n",
      "all branching entropies was computed # words = 92849\n",
      "all accessor variety was computed # words = 92849\n",
      "'탈북자들'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2980\n",
      "all branching entropies was computed # words = 92853\n",
      "all accessor variety was computed # words = 92853\n",
      "'청산규리'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2982\n",
      "all branching entropies was computed # words = 92862\n",
      "all accessor variety was computed # words = 92862\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2982\n",
      "all branching entropies was computed # words = 92865\n",
      "all accessor variety was computed # words = 92865\n",
      "'양드립'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2982\n",
      "all branching entropies was computed # words = 92868\n",
      "all accessor variety was computed # words = 92868\n",
      "'칼빈슨'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2982\n",
      "all branching entropies was computed # words = 92877\n",
      "all accessor variety was computed # words = 92877\n",
      "'미국산'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2984\n",
      "all branching entropies was computed # words = 92894\n",
      "all accessor variety was computed # words = 92894\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92899\n",
      "all accessor variety was computed # words = 92899\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92909\n",
      "all accessor variety was computed # words = 92909\n",
      "'접속자'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92927\n",
      "all accessor variety was computed # words = 92927\n",
      "'좆밥'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92929\n",
      "all accessor variety was computed # words = 92929\n",
      "'좃망'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92944\n",
      "all accessor variety was computed # words = 92944\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2986\n",
      "all branching entropies was computed # words = 92949\n",
      "all accessor variety was computed # words = 92949\n",
      "'리우올림픽'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2992\n",
      "all branching entropies was computed # words = 92975\n",
      "all accessor variety was computed # words = 92975\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2992\n",
      "all branching entropies was computed # words = 92979\n",
      "all accessor variety was computed # words = 92979\n",
      "'러브라이브'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2992\n",
      "all branching entropies was computed # words = 92983\n",
      "all accessor variety was computed # words = 92983\n",
      "'밤샘토론'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2992\n",
      "all branching entropies was computed # words = 92985\n",
      "all accessor variety was computed # words = 92985\n",
      "'선동중'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2994\n",
      "all branching entropies was computed # words = 92999\n",
      "all accessor variety was computed # words = 92999\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2994\n",
      "all branching entropies was computed # words = 93004\n",
      "all accessor variety was computed # words = 93004\n",
      "'한라봉'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2994\n",
      "all branching entropies was computed # words = 93004\n",
      "all accessor variety was computed # words = 93004\n",
      "'차은택'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2994\n",
      "all branching entropies was computed # words = 93006\n",
      "all accessor variety was computed # words = 93006\n",
      "'안민석'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2995\n",
      "all branching entropies was computed # words = 93033\n",
      "all accessor variety was computed # words = 93033\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2995\n",
      "all branching entropies was computed # words = 93033\n",
      "all accessor variety was computed # words = 93033\n",
      "'슨타크게이'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2995\n",
      "all branching entropies was computed # words = 93036\n",
      "all accessor variety was computed # words = 93036\n",
      "'김현의원'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2995\n",
      "all branching entropies was computed # words = 93041\n",
      "all accessor variety was computed # words = 93041\n",
      "'소방게이'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2997\n",
      "all branching entropies was computed # words = 93047\n",
      "all accessor variety was computed # words = 93047\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2997\n",
      "all branching entropies was computed # words = 93054\n",
      "all accessor variety was computed # words = 93054\n",
      "'두체통'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2997\n",
      "all branching entropies was computed # words = 93055\n",
      "all accessor variety was computed # words = 93055\n",
      "'관악을'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2997\n",
      "all branching entropies was computed # words = 93059\n",
      "all accessor variety was computed # words = 93059\n",
      "'극딜중'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93089\n",
      "all accessor variety was computed # words = 93089\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93091\n",
      "all accessor variety was computed # words = 93091\n",
      "'파혼게'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93095\n",
      "all accessor variety was computed # words = 93095\n",
      "'이은결'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93099\n",
      "all accessor variety was computed # words = 93099\n",
      "'소치올림픽'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93101\n",
      "all accessor variety was computed # words = 93101\n",
      "'소개팅게이'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93101\n",
      "all accessor variety was computed # words = 93101\n",
      "'베츙이몰'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93105\n",
      "all accessor variety was computed # words = 93105\n",
      "'클로징'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93105\n",
      "all accessor variety was computed # words = 93105\n",
      "'갓몽준'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 2999\n",
      "all branching entropies was computed # words = 93105\n",
      "all accessor variety was computed # words = 93105\n",
      "'좀전'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3000\n",
      "all branching entropies was computed # words = 93109\n",
      "all accessor variety was computed # words = 93109\n",
      "'판년'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3003\n",
      "all branching entropies was computed # words = 93165\n",
      "all accessor variety was computed # words = 93165\n",
      "'규재'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3003\n",
      "all branching entropies was computed # words = 93179\n",
      "all accessor variety was computed # words = 93179\n",
      "'사타부언'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3003\n",
      "all branching entropies was computed # words = 93189\n",
      "all accessor variety was computed # words = 93189\n",
      "'군가산점'\n",
      "training was done. used memory 1.062 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3003\n",
      "all branching entropies was computed # words = 93195\n",
      "all accessor variety was computed # words = 93195\n",
      "'일베광고'\n",
      "training was done. used memory 1.096 Gb1.048 Gb\n",
      "all cohesion probabilities was computed. # words = 3003\n",
      "all branching entropies was computed # words = 93198\n",
      "all accessor variety was computed # words = 93198\n",
      "'족발집'\n",
      "training was done. used memory 1.106 Gb1.067 Gb\n",
      "all cohesion probabilities was computed. # words = 3006\n",
      "all branching entropies was computed # words = 93968\n",
      "all accessor variety was computed # words = 93968\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3006\n",
      "all branching entropies was computed # words = 93970\n",
      "all accessor variety was computed # words = 93970\n",
      "'효성찡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3006\n",
      "all branching entropies was computed # words = 93973\n",
      "all accessor variety was computed # words = 93973\n",
      "'와갤러'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3006\n",
      "all branching entropies was computed # words = 93973\n",
      "all accessor variety was computed # words = 93973\n",
      "'나얼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3006\n",
      "all branching entropies was computed # words = 93982\n",
      "all accessor variety was computed # words = 93982\n",
      "'이디야'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3007\n",
      "all branching entropies was computed # words = 94001\n",
      "all accessor variety was computed # words = 94001\n",
      "'비빔면'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94044\n",
      "all accessor variety was computed # words = 94044\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94061\n",
      "all accessor variety was computed # words = 94061\n",
      "'이해못'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94062\n",
      "all accessor variety was computed # words = 94062\n",
      "'유가릿'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94066\n",
      "all accessor variety was computed # words = 94066\n",
      "'상윤쓰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94069\n",
      "all accessor variety was computed # words = 94069\n",
      "'부산행'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94072\n",
      "all accessor variety was computed # words = 94072\n",
      "'우지윤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3009\n",
      "all branching entropies was computed # words = 94079\n",
      "all accessor variety was computed # words = 94079\n",
      "'방시혁'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94092\n",
      "all accessor variety was computed # words = 94092\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94093\n",
      "all accessor variety was computed # words = 94093\n",
      "'앵콜'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94097\n",
      "all accessor variety was computed # words = 94097\n",
      "'배그'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94106\n",
      "all accessor variety was computed # words = 94106\n",
      "'생일파티'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94115\n",
      "all accessor variety was computed # words = 94115\n",
      "'레드카펫'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94121\n",
      "all accessor variety was computed # words = 94121\n",
      "'자연미인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94124\n",
      "all accessor variety was computed # words = 94124\n",
      "'쇼케이스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94137\n",
      "all accessor variety was computed # words = 94137\n",
      "'연락안'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94150\n",
      "all accessor variety was computed # words = 94150\n",
      "'찬사람'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94155\n",
      "all accessor variety was computed # words = 94155\n",
      "'예쁜애'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94159\n",
      "all accessor variety was computed # words = 94159\n",
      "'김이브'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94176\n",
      "all accessor variety was computed # words = 94176\n",
      "'일하고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94179\n",
      "all accessor variety was computed # words = 94179\n",
      "'강다녤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94197\n",
      "all accessor variety was computed # words = 94197\n",
      "'말티즈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 94214\n",
      "'심쿵'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94228\n",
      "all accessor variety was computed # words = 94228\n",
      "'공인인증서'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94230\n",
      "all accessor variety was computed # words = 94230\n",
      "'로봇청소기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94237\n",
      "all accessor variety was computed # words = 94237\n",
      "'여자사람'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94240\n",
      "all accessor variety was computed # words = 94240\n",
      "'트위치'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94249\n",
      "all accessor variety was computed # words = 94249\n",
      "'대방출'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94266\n",
      "all accessor variety was computed # words = 94266\n",
      "'만원권'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94282\n",
      "all accessor variety was computed # words = 94282\n",
      "'키우고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94283\n",
      "all accessor variety was computed # words = 94283\n",
      "'지선우'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94287\n",
      "all accessor variety was computed # words = 94287\n",
      "'이수혁'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94291\n",
      "all accessor variety was computed # words = 94291\n",
      "'만민중앙교회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94295\n",
      "all accessor variety was computed # words = 94295\n",
      "'온라인개학'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94312\n",
      "all accessor variety was computed # words = 94312\n",
      "'통화스와프'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94312\n",
      "all accessor variety was computed # words = 94312\n",
      "'삼성페이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94317\n",
      "all accessor variety was computed # words = 94317\n",
      "'검사키트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94327\n",
      "all accessor variety was computed # words = 94327\n",
      "'부분이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94332\n",
      "all accessor variety was computed # words = 94332\n",
      "'최저가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3010\n",
      "all branching entropies was computed # words = 94337\n",
      "all accessor variety was computed # words = 94337\n",
      "'쓸만'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3013\n",
      "all branching entropies was computed # words = 94340\n",
      "all accessor variety was computed # words = 94340\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3013\n",
      "all branching entropies was computed # words = 94342\n",
      "all accessor variety was computed # words = 94342\n",
      "'로또게이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3013\n",
      "all branching entropies was computed # words = 94343\n",
      "all accessor variety was computed # words = 94343\n",
      "'똥푸산'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3013\n",
      "all branching entropies was computed # words = 94348\n",
      "all accessor variety was computed # words = 94348\n",
      "'갓심'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3013\n",
      "all branching entropies was computed # words = 94351\n",
      "all accessor variety was computed # words = 94351\n",
      "'박수현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94373\n",
      "all accessor variety was computed # words = 94373\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94390\n",
      "all accessor variety was computed # words = 94390\n",
      "'테크트리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94390\n",
      "all accessor variety was computed # words = 94390\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94394\n",
      "all accessor variety was computed # words = 94394\n",
      "'줏었다'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94394\n",
      "all accessor variety was computed # words = 94394\n",
      "'소오금'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94394\n",
      "all accessor variety was computed # words = 94394\n",
      "'파맛첵스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94398\n",
      "all accessor variety was computed # words = 94398\n",
      "'권민아'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94399\n",
      "all accessor variety was computed # words = 94399\n",
      "'구준회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94405\n",
      "all accessor variety was computed # words = 94405\n",
      "'골든리트리버'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3017\n",
      "all branching entropies was computed # words = 94415\n",
      "all accessor variety was computed # words = 94415\n",
      "'헤어진후'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3022\n",
      "all branching entropies was computed # words = 94433\n",
      "all accessor variety was computed # words = 94433\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3022\n",
      "all branching entropies was computed # words = 94438\n",
      "all accessor variety was computed # words = 94438\n",
      "'홍종현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3022\n",
      "all branching entropies was computed # words = 94438\n",
      "all accessor variety was computed # words = 94438\n",
      "'재난긴급생활비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3022\n",
      "all branching entropies was computed # words = 94454\n",
      "all accessor variety was computed # words = 94454\n",
      "'환자들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3022\n",
      "all branching entropies was computed # words = 94454\n",
      "all accessor variety was computed # words = 94454\n",
      "'분탕새끼들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n",
      "all branching entropies was computed # words = 94471\n",
      "all accessor variety was computed # words = 94471\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 94475\n",
      "all accessor variety was computed # words = 94475\n",
      "'곽상도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n",
      "all branching entropies was computed # words = 94478\n",
      "all accessor variety was computed # words = 94478\n",
      "'피아식별'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n",
      "all branching entropies was computed # words = 94482\n",
      "all accessor variety was computed # words = 94482\n",
      "'뉴데일베'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n",
      "all branching entropies was computed # words = 94484\n",
      "all accessor variety was computed # words = 94484\n",
      "'진철이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3025\n",
      "all branching entropies was computed # words = 94491\n",
      "all accessor variety was computed # words = 94491\n",
      "'구자철'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3027\n",
      "all branching entropies was computed # words = 94503\n",
      "all accessor variety was computed # words = 94503\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3027\n",
      "all branching entropies was computed # words = 94512\n",
      "all accessor variety was computed # words = 94512\n",
      "'키스할때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3030\n",
      "all branching entropies was computed # words = 94535\n",
      "all accessor variety was computed # words = 94535\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3030\n",
      "all branching entropies was computed # words = 94535\n",
      "all accessor variety was computed # words = 94535\n",
      "'미디어워'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3031\n",
      "all branching entropies was computed # words = 94555\n",
      "all accessor variety was computed # words = 94555\n",
      "'이런것'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3031\n",
      "all branching entropies was computed # words = 94558\n",
      "all accessor variety was computed # words = 94558\n",
      "'서인국'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3031\n",
      "all branching entropies was computed # words = 94564\n",
      "all accessor variety was computed # words = 94564\n",
      "'수입과자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3031\n",
      "all branching entropies was computed # words = 94564\n",
      "all accessor variety was computed # words = 94564\n",
      "'개꿀팁'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3033\n",
      "all branching entropies was computed # words = 94572\n",
      "all accessor variety was computed # words = 94572\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3037\n",
      "all branching entropies was computed # words = 94630\n",
      "all accessor variety was computed # words = 94630\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3037\n",
      "all branching entropies was computed # words = 94632\n",
      "all accessor variety was computed # words = 94632\n",
      "'한미동맹'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3039\n",
      "all branching entropies was computed # words = 94662\n",
      "all accessor variety was computed # words = 94662\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3039\n",
      "all branching entropies was computed # words = 94665\n",
      "all accessor variety was computed # words = 94665\n",
      "'레드준표'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3039\n",
      "all branching entropies was computed # words = 94676\n",
      "all accessor variety was computed # words = 94676\n",
      "'의원님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3039\n",
      "all branching entropies was computed # words = 94680\n",
      "all accessor variety was computed # words = 94680\n",
      "'팽목항'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3042\n",
      "all branching entropies was computed # words = 94699\n",
      "all accessor variety was computed # words = 94699\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3043\n",
      "all branching entropies was computed # words = 94708\n",
      "all accessor variety was computed # words = 94708\n",
      "'이사진'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3043\n",
      "all branching entropies was computed # words = 94713\n",
      "all accessor variety was computed # words = 94713\n",
      "'젖탱이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3045\n",
      "all branching entropies was computed # words = 94763\n",
      "all accessor variety was computed # words = 94763\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3045\n",
      "all branching entropies was computed # words = 94772\n",
      "all accessor variety was computed # words = 94772\n",
      "'검머외'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3045\n",
      "all branching entropies was computed # words = 94775\n",
      "all accessor variety was computed # words = 94775\n",
      "'판사님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3045\n",
      "all branching entropies was computed # words = 94782\n",
      "all accessor variety was computed # words = 94782\n",
      "'철도민영화'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3045\n",
      "all branching entropies was computed # words = 94783\n",
      "all accessor variety was computed # words = 94783\n",
      "'타가수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3047\n",
      "all branching entropies was computed # words = 94792\n",
      "all accessor variety was computed # words = 94792\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3047\n",
      "all branching entropies was computed # words = 94798\n",
      "all accessor variety was computed # words = 94798\n",
      "'칼빵'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3048\n",
      "all branching entropies was computed # words = 94805\n",
      "all accessor variety was computed # words = 94805\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3049\n",
      "all branching entropies was computed # words = 94828\n",
      "all accessor variety was computed # words = 94828\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3051\n",
      "all branching entropies was computed # words = 94849\n",
      "all accessor variety was computed # words = 94849\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3051\n",
      "all branching entropies was computed # words = 94863\n",
      "all accessor variety was computed # words = 94863\n",
      "'짱께'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3051\n",
      "all branching entropies was computed # words = 94863\n",
      "all accessor variety was computed # words = 94863\n",
      "'행게이들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94875\n",
      "all accessor variety was computed # words = 94875\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94890\n",
      "all accessor variety was computed # words = 94890\n",
      "'쳐다보는'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94897\n",
      "all accessor variety was computed # words = 94897\n",
      "'아역배우'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94906\n",
      "all accessor variety was computed # words = 94906\n",
      "'디바제시카'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 94915\n",
      "all accessor variety was computed # words = 94915\n",
      "'홍콩시위'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94918\n",
      "all accessor variety was computed # words = 94918\n",
      "'연예부장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94920\n",
      "all accessor variety was computed # words = 94920\n",
      "'대단한점'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94923\n",
      "all accessor variety was computed # words = 94923\n",
      "'라임사태'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94931\n",
      "all accessor variety was computed # words = 94931\n",
      "'절라도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3052\n",
      "all branching entropies was computed # words = 94931\n",
      "all accessor variety was computed # words = 94931\n",
      "'문주당'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3055\n",
      "all branching entropies was computed # words = 94939\n",
      "all accessor variety was computed # words = 94939\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3055\n",
      "all branching entropies was computed # words = 94940\n",
      "all accessor variety was computed # words = 94940\n",
      "'존잘남'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3055\n",
      "all branching entropies was computed # words = 94965\n",
      "all accessor variety was computed # words = 94965\n",
      "'장군님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3056\n",
      "all branching entropies was computed # words = 94975\n",
      "all accessor variety was computed # words = 94975\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3056\n",
      "all branching entropies was computed # words = 94994\n",
      "all accessor variety was computed # words = 94994\n",
      "'친누나'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3056\n",
      "all branching entropies was computed # words = 94996\n",
      "all accessor variety was computed # words = 94996\n",
      "'똥양인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3056\n",
      "all branching entropies was computed # words = 95004\n",
      "all accessor variety was computed # words = 95004\n",
      "'검토중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3059\n",
      "all branching entropies was computed # words = 95033\n",
      "all accessor variety was computed # words = 95033\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3062\n",
      "all branching entropies was computed # words = 95068\n",
      "all accessor variety was computed # words = 95068\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3064\n",
      "all branching entropies was computed # words = 95077\n",
      "all accessor variety was computed # words = 95077\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3064\n",
      "all branching entropies was computed # words = 95087\n",
      "all accessor variety was computed # words = 95087\n",
      "'왕기춘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3066\n",
      "all branching entropies was computed # words = 95173\n",
      "all accessor variety was computed # words = 95173\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3066\n",
      "all branching entropies was computed # words = 95173\n",
      "all accessor variety was computed # words = 95173\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3066\n",
      "all branching entropies was computed # words = 95175\n",
      "all accessor variety was computed # words = 95175\n",
      "'해골찬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95194\n",
      "all accessor variety was computed # words = 95194\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95197\n",
      "all accessor variety was computed # words = 95197\n",
      "'갓갓'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95201\n",
      "all accessor variety was computed # words = 95201\n",
      "'로스트아크'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95211\n",
      "all accessor variety was computed # words = 95211\n",
      "'야인시대'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95220\n",
      "all accessor variety was computed # words = 95220\n",
      "'미친년들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3069\n",
      "all branching entropies was computed # words = 95229\n",
      "all accessor variety was computed # words = 95229\n",
      "'성소수자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3074\n",
      "all branching entropies was computed # words = 95256\n",
      "all accessor variety was computed # words = 95256\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3074\n",
      "all branching entropies was computed # words = 95266\n",
      "all accessor variety was computed # words = 95266\n",
      "'여성징병'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3074\n",
      "all branching entropies was computed # words = 95266\n",
      "all accessor variety was computed # words = 95266\n",
      "'안희정이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95272\n",
      "all accessor variety was computed # words = 95272\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95276\n",
      "all accessor variety was computed # words = 95276\n",
      "'안아키'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95291\n",
      "all accessor variety was computed # words = 95291\n",
      "'망하게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95292\n",
      "all accessor variety was computed # words = 95292\n",
      "'왕진진'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95303\n",
      "all accessor variety was computed # words = 95303\n",
      "'개정은'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95305\n",
      "all accessor variety was computed # words = 95305\n",
      "'웜비어'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95305\n",
      "all accessor variety was computed # words = 95305\n",
      "'김진태의원'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95309\n",
      "all accessor variety was computed # words = 95309\n",
      "'탈퇴인증'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95320\n",
      "all accessor variety was computed # words = 95320\n",
      "'전라도당'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3077\n",
      "all branching entropies was computed # words = 95329\n",
      "all accessor variety was computed # words = 95329\n",
      "'미만잡'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3080\n",
      "all branching entropies was computed # words = 95346\n",
      "all accessor variety was computed # words = 95346\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3080\n",
      "all branching entropies was computed # words = 95349\n",
      "all accessor variety was computed # words = 95349\n",
      "'자박꼼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95386\n",
      "all accessor variety was computed # words = 95386\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95388\n",
      "all accessor variety was computed # words = 95388\n",
      "'박그네'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95398\n",
      "all accessor variety was computed # words = 95398\n",
      "'모레츠'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95411\n",
      "all accessor variety was computed # words = 95411\n",
      "'디도스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95412\n",
      "all accessor variety was computed # words = 95412\n",
      "'통베'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95423\n",
      "all accessor variety was computed # words = 95423\n",
      "'짬밥'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95427\n",
      "all accessor variety was computed # words = 95427\n",
      "'뱀줍'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95430\n",
      "all accessor variety was computed # words = 95430\n",
      "'디시위키'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95434\n",
      "all accessor variety was computed # words = 95434\n",
      "'자원외교'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95438\n",
      "all accessor variety was computed # words = 95438\n",
      "'한명숙청'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95439\n",
      "all accessor variety was computed # words = 95439\n",
      "'심야식당'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95443\n",
      "all accessor variety was computed # words = 95443\n",
      "'이홍기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95446\n",
      "all accessor variety was computed # words = 95446\n",
      "'고영주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95451\n",
      "all accessor variety was computed # words = 95451\n",
      "'공돌이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95453\n",
      "all accessor variety was computed # words = 95453\n",
      "'친목질'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95453\n",
      "all accessor variety was computed # words = 95453\n",
      "'좆문가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3085\n",
      "all branching entropies was computed # words = 95465\n",
      "all accessor variety was computed # words = 95465\n",
      "'즐톡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95484\n",
      "all accessor variety was computed # words = 95484\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95492\n",
      "all accessor variety was computed # words = 95492\n",
      "'만렙'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95498\n",
      "all accessor variety was computed # words = 95498\n",
      "'청계광장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95499\n",
      "all accessor variety was computed # words = 95499\n",
      "'쭉빵년들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95505\n",
      "all accessor variety was computed # words = 95505\n",
      "'떠나보자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95505\n",
      "all accessor variety was computed # words = 95505\n",
      "'웹툰작가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3086\n",
      "all branching entropies was computed # words = 95507\n",
      "all accessor variety was computed # words = 95507\n",
      "'냥이찡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95511\n",
      "all accessor variety was computed # words = 95511\n",
      "'이승기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95511\n",
      "all accessor variety was computed # words = 95511\n",
      "'선장이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95517\n",
      "all accessor variety was computed # words = 95517\n",
      "'옆집여자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95518\n",
      "all accessor variety was computed # words = 95518\n",
      "'안산역'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95532\n",
      "all accessor variety was computed # words = 95532\n",
      "'있는것'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95538\n",
      "all accessor variety was computed # words = 95538\n",
      "'이택광'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95542\n",
      "all accessor variety was computed # words = 95542\n",
      "'코갤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95545\n",
      "all accessor variety was computed # words = 95545\n",
      "'어떻게생각'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95549\n",
      "all accessor variety was computed # words = 95549\n",
      "'나이트쿠폰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95560\n",
      "all accessor variety was computed # words = 95560\n",
      "'헌팅포차'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95561\n",
      "all accessor variety was computed # words = 95561\n",
      "'친구관계'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95571\n",
      "all accessor variety was computed # words = 95571\n",
      "'유효기간'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 95587\n",
      "'소개팅남'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95587\n",
      "all accessor variety was computed # words = 95587\n",
      "'진비빔면'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95590\n",
      "all accessor variety was computed # words = 95590\n",
      "'블루투스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95612\n",
      "all accessor variety was computed # words = 95612\n",
      "'남자키'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3088\n",
      "all branching entropies was computed # words = 95620\n",
      "all accessor variety was computed # words = 95620\n",
      "'백현이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3089\n",
      "all branching entropies was computed # words = 95625\n",
      "all accessor variety was computed # words = 95625\n",
      "'있는듯'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3089\n",
      "all branching entropies was computed # words = 95629\n",
      "all accessor variety was computed # words = 95629\n",
      "'안지영'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3089\n",
      "all branching entropies was computed # words = 95631\n",
      "all accessor variety was computed # words = 95631\n",
      "'부승관'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95654\n",
      "all accessor variety was computed # words = 95654\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95657\n",
      "all accessor variety was computed # words = 95657\n",
      "'뭉찬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95664\n",
      "all accessor variety was computed # words = 95664\n",
      "'머글'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95673\n",
      "all accessor variety was computed # words = 95673\n",
      "'스밍'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95692\n",
      "all accessor variety was computed # words = 95692\n",
      "'중때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95696\n",
      "all accessor variety was computed # words = 95696\n",
      "'하고싶은말'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95716\n",
      "all accessor variety was computed # words = 95716\n",
      "'무슨생각'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95732\n",
      "all accessor variety was computed # words = 95732\n",
      "'추가추가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95745\n",
      "all accessor variety was computed # words = 95745\n",
      "'후기추가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95754\n",
      "all accessor variety was computed # words = 95754\n",
      "'미친것'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95754\n",
      "all accessor variety was computed # words = 95754\n",
      "'텐미닛'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95756\n",
      "all accessor variety was computed # words = 95756\n",
      "'최준희'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3092\n",
      "all branching entropies was computed # words = 95761\n",
      "all accessor variety was computed # words = 95761\n",
      "'촬영중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95779\n",
      "all accessor variety was computed # words = 95779\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95782\n",
      "all accessor variety was computed # words = 95782\n",
      "'엑방원'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95805\n",
      "all accessor variety was computed # words = 95805\n",
      "'자취남'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95806\n",
      "all accessor variety was computed # words = 95806\n",
      "'팔로워'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95815\n",
      "all accessor variety was computed # words = 95815\n",
      "'읽씹'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95822\n",
      "all accessor variety was computed # words = 95822\n",
      "'볼살'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95835\n",
      "all accessor variety was computed # words = 95835\n",
      "'안쓰고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95838\n",
      "all accessor variety was computed # words = 95838\n",
      "'스냅백'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95839\n",
      "all accessor variety was computed # words = 95839\n",
      "'알뜰폰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95840\n",
      "all accessor variety was computed # words = 95840\n",
      "'꼴데'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95863\n",
      "all accessor variety was computed # words = 95863\n",
      "'의료진들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95863\n",
      "all accessor variety was computed # words = 95863\n",
      "'슈퍼클럽'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95863\n",
      "all accessor variety was computed # words = 95863\n",
      "'박새로이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95864\n",
      "all accessor variety was computed # words = 95864\n",
      "'격리해제'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95874\n",
      "all accessor variety was computed # words = 95874\n",
      "'기사시험'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95877\n",
      "all accessor variety was computed # words = 95877\n",
      "'기본소득'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95882\n",
      "all accessor variety was computed # words = 95882\n",
      "'저탄고지'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95885\n",
      "all accessor variety was computed # words = 95885\n",
      "'울나라'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95887\n",
      "all accessor variety was computed # words = 95887\n",
      "'강남갑'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95892\n",
      "all accessor variety was computed # words = 95892\n",
      "'교인들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95895\n",
      "all accessor variety was computed # words = 95895\n",
      "'설연휴'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95902\n",
      "all accessor variety was computed # words = 95902\n",
      "'폭락중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95909\n",
      "all accessor variety was computed # words = 95909\n",
      "'이것들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95926\n",
      "all accessor variety was computed # words = 95926\n",
      "'조중동'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3093\n",
      "all branching entropies was computed # words = 95944\n",
      "all accessor variety was computed # words = 95944\n",
      "'하빕'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3095\n",
      "all branching entropies was computed # words = 95957\n",
      "all accessor variety was computed # words = 95957\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3095\n",
      "all branching entropies was computed # words = 95960\n",
      "all accessor variety was computed # words = 95960\n",
      "'곽철용'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3095\n",
      "all branching entropies was computed # words = 95960\n",
      "all accessor variety was computed # words = 95960\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3095\n",
      "all branching entropies was computed # words = 95962\n",
      "all accessor variety was computed # words = 95962\n",
      "'김상조'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3095\n",
      "all branching entropies was computed # words = 95963\n",
      "all accessor variety was computed # words = 95963\n",
      "'민병두'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95985\n",
      "all accessor variety was computed # words = 95985\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95994\n",
      "all accessor variety was computed # words = 95994\n",
      "'보복운전'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95994\n",
      "all accessor variety was computed # words = 95994\n",
      "'구경하고가라'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95995\n",
      "all accessor variety was computed # words = 95995\n",
      "'이종인이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95995\n",
      "all accessor variety was computed # words = 95995\n",
      "'그와중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 95999\n",
      "all accessor variety was computed # words = 95999\n",
      "'남녀아이돌'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 96009\n",
      "all accessor variety was computed # words = 96009\n",
      "'생리할때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3098\n",
      "all branching entropies was computed # words = 96009\n",
      "all accessor variety was computed # words = 96009\n",
      "'안되는건'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3100\n",
      "all branching entropies was computed # words = 96038\n",
      "all accessor variety was computed # words = 96038\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3100\n",
      "all branching entropies was computed # words = 96038\n",
      "all accessor variety was computed # words = 96038\n",
      "'안흔'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3100\n",
      "all branching entropies was computed # words = 96039\n",
      "all accessor variety was computed # words = 96039\n",
      "'토스쿠폰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3100\n",
      "all branching entropies was computed # words = 96041\n",
      "all accessor variety was computed # words = 96041\n",
      "'코로나관련'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3105\n",
      "all branching entropies was computed # words = 96073\n",
      "all accessor variety was computed # words = 96073\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3105\n",
      "all branching entropies was computed # words = 96074\n",
      "all accessor variety was computed # words = 96074\n",
      "'홍미노트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3105\n",
      "all branching entropies was computed # words = 96076\n",
      "all accessor variety was computed # words = 96076\n",
      "'속보코'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3105\n",
      "all branching entropies was computed # words = 96079\n",
      "all accessor variety was computed # words = 96079\n",
      "'동백전'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96104\n",
      "all accessor variety was computed # words = 96104\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96104\n",
      "all accessor variety was computed # words = 96104\n",
      "'속보문재'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96110\n",
      "all accessor variety was computed # words = 96110\n",
      "'주토피아'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96110\n",
      "all accessor variety was computed # words = 96110\n",
      "'크레용팝이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96114\n",
      "all accessor variety was computed # words = 96114\n",
      "'그래비티'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96121\n",
      "all accessor variety was computed # words = 96121\n",
      "'동행세일'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96127\n",
      "all accessor variety was computed # words = 96127\n",
      "'다이슨'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96134\n",
      "all accessor variety was computed # words = 96134\n",
      "'하객룩'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96148\n",
      "all accessor variety was computed # words = 96148\n",
      "'생신상'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96149\n",
      "all accessor variety was computed # words = 96149\n",
      "'성지글'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 96156\n",
      "'김치년저장소'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96165\n",
      "all accessor variety was computed # words = 96165\n",
      "'화내는'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96165\n",
      "all accessor variety was computed # words = 96165\n",
      "'박지훈이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3108\n",
      "all branching entropies was computed # words = 96171\n",
      "all accessor variety was computed # words = 96171\n",
      "'일본반응'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3110\n",
      "all branching entropies was computed # words = 96200\n",
      "all accessor variety was computed # words = 96200\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3110\n",
      "all branching entropies was computed # words = 96205\n",
      "all accessor variety was computed # words = 96205\n",
      "'빵셔틀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96214\n",
      "all accessor variety was computed # words = 96214\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96219\n",
      "all accessor variety was computed # words = 96219\n",
      "'엄마아빠'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96225\n",
      "all accessor variety was computed # words = 96225\n",
      "'칼군무'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96242\n",
      "all accessor variety was computed # words = 96242\n",
      "'일베똥'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96248\n",
      "all accessor variety was computed # words = 96248\n",
      "'좃같은'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96252\n",
      "all accessor variety was computed # words = 96252\n",
      "'회사생활'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96257\n",
      "all accessor variety was computed # words = 96257\n",
      "'기상캐스터'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96265\n",
      "all accessor variety was computed # words = 96265\n",
      "'씹김치년'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96279\n",
      "all accessor variety was computed # words = 96279\n",
      "'유퀴즈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96302\n",
      "all accessor variety was computed # words = 96302\n",
      "'호갱'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96302\n",
      "all accessor variety was computed # words = 96302\n",
      "'패션좌파'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96313\n",
      "all accessor variety was computed # words = 96313\n",
      "'스쉽'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96314\n",
      "all accessor variety was computed # words = 96314\n",
      "'똥꼬충들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3112\n",
      "all branching entropies was computed # words = 96318\n",
      "all accessor variety was computed # words = 96318\n",
      "'장기하'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3114\n",
      "all branching entropies was computed # words = 96328\n",
      "all accessor variety was computed # words = 96328\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3114\n",
      "all branching entropies was computed # words = 96336\n",
      "all accessor variety was computed # words = 96336\n",
      "'새끼고양이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3114\n",
      "all branching entropies was computed # words = 96346\n",
      "all accessor variety was computed # words = 96346\n",
      "'아깽이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3114\n",
      "all branching entropies was computed # words = 96353\n",
      "all accessor variety was computed # words = 96353\n",
      "'재난기금'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96384\n",
      "all accessor variety was computed # words = 96384\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96388\n",
      "all accessor variety was computed # words = 96388\n",
      "'미스터씨발'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96394\n",
      "all accessor variety was computed # words = 96394\n",
      "'탈모갤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96397\n",
      "all accessor variety was computed # words = 96397\n",
      "'수만휘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96415\n",
      "all accessor variety was computed # words = 96415\n",
      "'씨유'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96415\n",
      "all accessor variety was computed # words = 96415\n",
      "'인스타스토리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3119\n",
      "all branching entropies was computed # words = 96420\n",
      "all accessor variety was computed # words = 96420\n",
      "'아하부장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96467\n",
      "all accessor variety was computed # words = 96467\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96484\n",
      "all accessor variety was computed # words = 96484\n",
      "'폭발사고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96493\n",
      "all accessor variety was computed # words = 96493\n",
      "'인생역전'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96496\n",
      "all accessor variety was computed # words = 96496\n",
      "'소대가리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96510\n",
      "all accessor variety was computed # words = 96510\n",
      "'임산부석'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96515\n",
      "all accessor variety was computed # words = 96515\n",
      "'애미애비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3122\n",
      "all branching entropies was computed # words = 96515\n",
      "all accessor variety was computed # words = 96515\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3125\n",
      "all branching entropies was computed # words = 96524\n",
      "all accessor variety was computed # words = 96524\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 96529\n",
      "all accessor variety was computed # words = 96529\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3128\n",
      "all branching entropies was computed # words = 96531\n",
      "all accessor variety was computed # words = 96531\n",
      "'분탕새끼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3133\n",
      "all branching entropies was computed # words = 96544\n",
      "all accessor variety was computed # words = 96544\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3133\n",
      "all branching entropies was computed # words = 96544\n",
      "all accessor variety was computed # words = 96544\n",
      "'정경심이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3136\n",
      "all branching entropies was computed # words = 96554\n",
      "all accessor variety was computed # words = 96554\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3139\n",
      "all branching entropies was computed # words = 96616\n",
      "all accessor variety was computed # words = 96616\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3139\n",
      "all branching entropies was computed # words = 96622\n",
      "all accessor variety was computed # words = 96622\n",
      "'피자집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3141\n",
      "all branching entropies was computed # words = 96684\n",
      "all accessor variety was computed # words = 96684\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3143\n",
      "all branching entropies was computed # words = 96704\n",
      "all accessor variety was computed # words = 96704\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3143\n",
      "all branching entropies was computed # words = 96710\n",
      "all accessor variety was computed # words = 96710\n",
      "'대재앙'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3143\n",
      "all branching entropies was computed # words = 96711\n",
      "all accessor variety was computed # words = 96711\n",
      "'짱깨년'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3143\n",
      "all branching entropies was computed # words = 96715\n",
      "all accessor variety was computed # words = 96715\n",
      "'리뉴얼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3145\n",
      "all branching entropies was computed # words = 96717\n",
      "all accessor variety was computed # words = 96717\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3148\n",
      "all branching entropies was computed # words = 96764\n",
      "all accessor variety was computed # words = 96764\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3150\n",
      "all branching entropies was computed # words = 96776\n",
      "all accessor variety was computed # words = 96776\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3150\n",
      "all branching entropies was computed # words = 96781\n",
      "all accessor variety was computed # words = 96781\n",
      "'최단기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3150\n",
      "all branching entropies was computed # words = 96781\n",
      "all accessor variety was computed # words = 96781\n",
      "'본투표'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3150\n",
      "all branching entropies was computed # words = 96786\n",
      "all accessor variety was computed # words = 96786\n",
      "'삼립빵'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3152\n",
      "all branching entropies was computed # words = 96808\n",
      "all accessor variety was computed # words = 96808\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3152\n",
      "all branching entropies was computed # words = 96809\n",
      "all accessor variety was computed # words = 96809\n",
      "'임현주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3152\n",
      "all branching entropies was computed # words = 96854\n",
      "all accessor variety was computed # words = 96854\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3153\n",
      "all branching entropies was computed # words = 96885\n",
      "all accessor variety was computed # words = 96885\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3156\n",
      "all branching entropies was computed # words = 96937\n",
      "all accessor variety was computed # words = 96937\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96952\n",
      "all accessor variety was computed # words = 96952\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96956\n",
      "all accessor variety was computed # words = 96956\n",
      "'네이버댓글'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96956\n",
      "all accessor variety was computed # words = 96956\n",
      "'북괴새끼들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96957\n",
      "all accessor variety was computed # words = 96957\n",
      "'아쿠아맨'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96965\n",
      "all accessor variety was computed # words = 96965\n",
      "'탈코르셋'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96968\n",
      "all accessor variety was computed # words = 96968\n",
      "'국정농단'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96982\n",
      "all accessor variety was computed # words = 96982\n",
      "'친자확인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96982\n",
      "all accessor variety was computed # words = 96982\n",
      "'꼴페미들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96986\n",
      "all accessor variety was computed # words = 96986\n",
      "'소개팅녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96986\n",
      "all accessor variety was computed # words = 96986\n",
      "'웜퇘지들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3157\n",
      "all branching entropies was computed # words = 96986\n",
      "all accessor variety was computed # words = 96986\n",
      "'애꾸보수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97550\n",
      "all accessor variety was computed # words = 97550\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97550\n",
      "all accessor variety was computed # words = 97550\n",
      "'문용식'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97552\n",
      "all accessor variety was computed # words = 97552\n",
      "'혼모노'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97553\n",
      "all accessor variety was computed # words = 97553\n",
      "'갓양녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97564\n",
      "all accessor variety was computed # words = 97564\n",
      "'할배들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 97571\n",
      "all accessor variety was computed # words = 97571\n",
      "'송민순'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3171\n",
      "all branching entropies was computed # words = 97577\n",
      "all accessor variety was computed # words = 97577\n",
      "'사츠키'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97613\n",
      "all accessor variety was computed # words = 97613\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97627\n",
      "all accessor variety was computed # words = 97627\n",
      "'키배'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97627\n",
      "all accessor variety was computed # words = 97627\n",
      "'갤럭시노트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97648\n",
      "all accessor variety was computed # words = 97648\n",
      "'최고존엄'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97651\n",
      "all accessor variety was computed # words = 97651\n",
      "'운지벌레'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97653\n",
      "all accessor variety was computed # words = 97653\n",
      "'일베회원'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97663\n",
      "all accessor variety was computed # words = 97663\n",
      "'천일염'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97664\n",
      "all accessor variety was computed # words = 97664\n",
      "'삐라줍'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97668\n",
      "all accessor variety was computed # words = 97668\n",
      "'원수님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97671\n",
      "all accessor variety was computed # words = 97671\n",
      "'박인비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97694\n",
      "all accessor variety was computed # words = 97694\n",
      "'오승환'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3174\n",
      "all branching entropies was computed # words = 97714\n",
      "all accessor variety was computed # words = 97714\n",
      "'갤노트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97718\n",
      "all accessor variety was computed # words = 97718\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97726\n",
      "all accessor variety was computed # words = 97726\n",
      "'줄리엔강'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97726\n",
      "all accessor variety was computed # words = 97726\n",
      "'돌고래호'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97728\n",
      "all accessor variety was computed # words = 97728\n",
      "'전자담배'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97732\n",
      "all accessor variety was computed # words = 97732\n",
      "'동성결혼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97733\n",
      "all accessor variety was computed # words = 97733\n",
      "'스카이프'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97737\n",
      "all accessor variety was computed # words = 97737\n",
      "'없는것이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3177\n",
      "all branching entropies was computed # words = 97740\n",
      "all accessor variety was computed # words = 97740\n",
      "'시발년아'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3179\n",
      "all branching entropies was computed # words = 97743\n",
      "all accessor variety was computed # words = 97743\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3179\n",
      "all branching entropies was computed # words = 97745\n",
      "all accessor variety was computed # words = 97745\n",
      "'나지완'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3179\n",
      "all branching entropies was computed # words = 97759\n",
      "all accessor variety was computed # words = 97759\n",
      "'배워야'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97801\n",
      "all accessor variety was computed # words = 97801\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97807\n",
      "all accessor variety was computed # words = 97807\n",
      "'엄성섭'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97816\n",
      "all accessor variety was computed # words = 97816\n",
      "'떼창'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97832\n",
      "all accessor variety was computed # words = 97832\n",
      "'우덜'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97834\n",
      "all accessor variety was computed # words = 97834\n",
      "'성재기대표'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97841\n",
      "all accessor variety was computed # words = 97841\n",
      "'도움앙망'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97841\n",
      "all accessor variety was computed # words = 97841\n",
      "'강제일밍'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3182\n",
      "all branching entropies was computed # words = 97855\n",
      "all accessor variety was computed # words = 97855\n",
      "'일베용어'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3185\n",
      "all branching entropies was computed # words = 97866\n",
      "all accessor variety was computed # words = 97866\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3185\n",
      "all branching entropies was computed # words = 97866\n",
      "all accessor variety was computed # words = 97866\n",
      "'추적분'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3185\n",
      "all branching entropies was computed # words = 97871\n",
      "all accessor variety was computed # words = 97871\n",
      "'슨탄절'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3185\n",
      "all branching entropies was computed # words = 97871\n",
      "all accessor variety was computed # words = 97871\n",
      "'판년들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3185\n",
      "all branching entropies was computed # words = 97875\n",
      "all accessor variety was computed # words = 97875\n",
      "'영민이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3187\n",
      "all branching entropies was computed # words = 97894\n",
      "all accessor variety was computed # words = 97894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97928\n",
      "all accessor variety was computed # words = 97928\n",
      "'고렙'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97935\n",
      "all accessor variety was computed # words = 97935\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97939\n",
      "all accessor variety was computed # words = 97939\n",
      "'팀킬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97952\n",
      "all accessor variety was computed # words = 97952\n",
      "'대박고기집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97953\n",
      "all accessor variety was computed # words = 97953\n",
      "'선동만화'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97954\n",
      "all accessor variety was computed # words = 97954\n",
      "'꾸밈비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97957\n",
      "all accessor variety was computed # words = 97957\n",
      "'안영미'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97968\n",
      "all accessor variety was computed # words = 97968\n",
      "'넣으면'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97979\n",
      "all accessor variety was computed # words = 97979\n",
      "'다이쥬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97994\n",
      "all accessor variety was computed # words = 97994\n",
      "'쎾쓰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 97996\n",
      "all accessor variety was computed # words = 97996\n",
      "'여자아이들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 98002\n",
      "all accessor variety was computed # words = 98002\n",
      "'여혐종자들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3188\n",
      "all branching entropies was computed # words = 98006\n",
      "all accessor variety was computed # words = 98006\n",
      "'마지막날'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3191\n",
      "all branching entropies was computed # words = 98033\n",
      "all accessor variety was computed # words = 98033\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98046\n",
      "all accessor variety was computed # words = 98046\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98057\n",
      "all accessor variety was computed # words = 98057\n",
      "'보더콜리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98063\n",
      "all accessor variety was computed # words = 98063\n",
      "'로켓배송'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98073\n",
      "all accessor variety was computed # words = 98073\n",
      "'말해야'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98088\n",
      "all accessor variety was computed # words = 98088\n",
      "'여우짓'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98092\n",
      "all accessor variety was computed # words = 98092\n",
      "'블러셔'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98094\n",
      "all accessor variety was computed # words = 98094\n",
      "'차트인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98096\n",
      "all accessor variety was computed # words = 98096\n",
      "'반팔티'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98100\n",
      "all accessor variety was computed # words = 98100\n",
      "'업텐션'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98102\n",
      "all accessor variety was computed # words = 98102\n",
      "'언팔'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98108\n",
      "all accessor variety was computed # words = 98108\n",
      "'뭘로'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98123\n",
      "all accessor variety was computed # words = 98123\n",
      "'남녀사이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98134\n",
      "all accessor variety was computed # words = 98134\n",
      "'이별통보'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98141\n",
      "all accessor variety was computed # words = 98141\n",
      "'추가친구'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98146\n",
      "all accessor variety was computed # words = 98146\n",
      "'부정투표'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98157\n",
      "all accessor variety was computed # words = 98157\n",
      "'다리길이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98160\n",
      "all accessor variety was computed # words = 98160\n",
      "'추가남친'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98183\n",
      "all accessor variety was computed # words = 98183\n",
      "'좋아할때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98208\n",
      "all accessor variety was computed # words = 98208\n",
      "'헤다판'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98212\n",
      "all accessor variety was computed # words = 98212\n",
      "'시월드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98216\n",
      "all accessor variety was computed # words = 98216\n",
      "'이제훈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98222\n",
      "all accessor variety was computed # words = 98222\n",
      "'이쁜애'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98224\n",
      "all accessor variety was computed # words = 98224\n",
      "'시엄니'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98230\n",
      "all accessor variety was computed # words = 98230\n",
      "'양갈래'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 98237\n",
      "'만나게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98246\n",
      "all accessor variety was computed # words = 98246\n",
      "'사귄지'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98254\n",
      "all accessor variety was computed # words = 98254\n",
      "'시조카'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98255\n",
      "all accessor variety was computed # words = 98255\n",
      "'팬덤명'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98267\n",
      "all accessor variety was computed # words = 98267\n",
      "'박소담'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98267\n",
      "all accessor variety was computed # words = 98267\n",
      "'까글'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98268\n",
      "all accessor variety was computed # words = 98268\n",
      "'시스루뱅'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98273\n",
      "all accessor variety was computed # words = 98273\n",
      "'비에이피'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98295\n",
      "all accessor variety was computed # words = 98295\n",
      "'음란마귀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98299\n",
      "all accessor variety was computed # words = 98299\n",
      "'송화가루'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98303\n",
      "all accessor variety was computed # words = 98303\n",
      "'쀼의세계'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98306\n",
      "all accessor variety was computed # words = 98306\n",
      "'이여자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98306\n",
      "all accessor variety was computed # words = 98306\n",
      "'정유미'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98306\n",
      "all accessor variety was computed # words = 98306\n",
      "'귀엽게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98310\n",
      "all accessor variety was computed # words = 98310\n",
      "'서울사랑상품권'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98314\n",
      "all accessor variety was computed # words = 98314\n",
      "'다른의견'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3194\n",
      "all branching entropies was computed # words = 98317\n",
      "all accessor variety was computed # words = 98317\n",
      "'대구사람'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98336\n",
      "all accessor variety was computed # words = 98336\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98339\n",
      "all accessor variety was computed # words = 98339\n",
      "'올렸는데'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98347\n",
      "all accessor variety was computed # words = 98347\n",
      "'방역당국'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98353\n",
      "all accessor variety was computed # words = 98353\n",
      "'종교집회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98354\n",
      "all accessor variety was computed # words = 98354\n",
      "'주식장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98357\n",
      "all accessor variety was computed # words = 98357\n",
      "'마봉춘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98362\n",
      "all accessor variety was computed # words = 98362\n",
      "'빅세일'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98364\n",
      "all accessor variety was computed # words = 98364\n",
      "'선거날'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98373\n",
      "all accessor variety was computed # words = 98373\n",
      "'국가직'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98381\n",
      "all accessor variety was computed # words = 98381\n",
      "'퍼지고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98385\n",
      "all accessor variety was computed # words = 98385\n",
      "'미국서'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98387\n",
      "all accessor variety was computed # words = 98387\n",
      "'차감염'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98389\n",
      "all accessor variety was computed # words = 98389\n",
      "'뽐뿌인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98391\n",
      "all accessor variety was computed # words = 98391\n",
      "'변상욱'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98402\n",
      "all accessor variety was computed # words = 98402\n",
      "'많다고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98404\n",
      "all accessor variety was computed # words = 98404\n",
      "'두분'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98415\n",
      "all accessor variety was computed # words = 98415\n",
      "'알아보지'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98418\n",
      "all accessor variety was computed # words = 98418\n",
      "'한미게이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98418\n",
      "all accessor variety was computed # words = 98418\n",
      "'적와대'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98419\n",
      "all accessor variety was computed # words = 98419\n",
      "'엠빙신'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98430\n",
      "all accessor variety was computed # words = 98430\n",
      "'일베가고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98430\n",
      "all accessor variety was computed # words = 98430\n",
      "'멍뭉찡들'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98435\n",
      "all accessor variety was computed # words = 98435\n",
      "'퍼갔다'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98435\n",
      "all accessor variety was computed # words = 98435\n",
      "'과천시청'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3196\n",
      "all branching entropies was computed # words = 98444\n",
      "all accessor variety was computed # words = 98444\n",
      "'최대집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3197\n",
      "all branching entropies was computed # words = 98468\n",
      "all accessor variety was computed # words = 98468\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3197\n",
      "all branching entropies was computed # words = 98471\n",
      "all accessor variety was computed # words = 98471\n",
      "'선화예고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98478\n",
      "all accessor variety was computed # words = 98478\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98482\n",
      "all accessor variety was computed # words = 98482\n",
      "'자동가입'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98482\n",
      "all accessor variety was computed # words = 98482\n",
      "'김기종이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98492\n",
      "all accessor variety was computed # words = 98492\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98492\n",
      "all accessor variety was computed # words = 98492\n",
      "'서술형'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98492\n",
      "all accessor variety was computed # words = 98492\n",
      "'임나영'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98493\n",
      "all accessor variety was computed # words = 98493\n",
      "'남자심리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98506\n",
      "all accessor variety was computed # words = 98506\n",
      "'얼마정'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98523\n",
      "all accessor variety was computed # words = 98523\n",
      "'뭐하고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98523\n",
      "all accessor variety was computed # words = 98523\n",
      "'이미주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98543\n",
      "all accessor variety was computed # words = 98543\n",
      "'한국녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98544\n",
      "all accessor variety was computed # words = 98544\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3199\n",
      "all branching entropies was computed # words = 98544\n",
      "all accessor variety was computed # words = 98544\n",
      "'나오늘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3200\n",
      "all branching entropies was computed # words = 98564\n",
      "all accessor variety was computed # words = 98564\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3200\n",
      "all branching entropies was computed # words = 98564\n",
      "all accessor variety was computed # words = 98564\n",
      "'국뽕주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3202\n",
      "all branching entropies was computed # words = 98603\n",
      "all accessor variety was computed # words = 98603\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3205\n",
      "all branching entropies was computed # words = 98621\n",
      "all accessor variety was computed # words = 98621\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98664\n",
      "all accessor variety was computed # words = 98664\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98668\n",
      "all accessor variety was computed # words = 98668\n",
      "'주작범'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98671\n",
      "all accessor variety was computed # words = 98671\n",
      "'위례별초'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98674\n",
      "all accessor variety was computed # words = 98674\n",
      "'씹소름'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98686\n",
      "all accessor variety was computed # words = 98686\n",
      "'빡대가리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98688\n",
      "all accessor variety was computed # words = 98688\n",
      "'견찰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98735\n",
      "all accessor variety was computed # words = 98735\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98738\n",
      "all accessor variety was computed # words = 98738\n",
      "'정자게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98741\n",
      "all accessor variety was computed # words = 98741\n",
      "'닭근혜'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3206\n",
      "all branching entropies was computed # words = 98743\n",
      "all accessor variety was computed # words = 98743\n",
      "'재벌세'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98795\n",
      "all accessor variety was computed # words = 98795\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98807\n",
      "all accessor variety was computed # words = 98807\n",
      "'아주매미'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98815\n",
      "all accessor variety was computed # words = 98815\n",
      "'탈북녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98831\n",
      "all accessor variety was computed # words = 98831\n",
      "'반려동물'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98833\n",
      "all accessor variety was computed # words = 98833\n",
      "'가카찡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3207\n",
      "all branching entropies was computed # words = 98841\n",
      "all accessor variety was computed # words = 98841\n",
      "'커플들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 98898\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98902\n",
      "all accessor variety was computed # words = 98902\n",
      "'시발새끼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98904\n",
      "all accessor variety was computed # words = 98904\n",
      "'흑수저'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98908\n",
      "all accessor variety was computed # words = 98908\n",
      "'구해줘홈즈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98908\n",
      "all accessor variety was computed # words = 98908\n",
      "'야후재팬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98908\n",
      "all accessor variety was computed # words = 98908\n",
      "'숙식노가다'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98930\n",
      "all accessor variety was computed # words = 98930\n",
      "'성폭행범'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98946\n",
      "all accessor variety was computed # words = 98946\n",
      "'펜벤다졸'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3211\n",
      "all branching entropies was computed # words = 98955\n",
      "all accessor variety was computed # words = 98955\n",
      "'사촌누나'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3215\n",
      "all branching entropies was computed # words = 98972\n",
      "all accessor variety was computed # words = 98972\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3215\n",
      "all branching entropies was computed # words = 98986\n",
      "all accessor variety was computed # words = 98986\n",
      "'리얼미터'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3215\n",
      "all branching entropies was computed # words = 98986\n",
      "all accessor variety was computed # words = 98986\n",
      "'설라도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3215\n",
      "all branching entropies was computed # words = 98992\n",
      "all accessor variety was computed # words = 98992\n",
      "'헬조선'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3222\n",
      "all branching entropies was computed # words = 99300\n",
      "all accessor variety was computed # words = 99300\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3225\n",
      "all branching entropies was computed # words = 99330\n",
      "all accessor variety was computed # words = 99330\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3225\n",
      "all branching entropies was computed # words = 99332\n",
      "all accessor variety was computed # words = 99332\n",
      "'개극혐'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3225\n",
      "all branching entropies was computed # words = 99334\n",
      "all accessor variety was computed # words = 99334\n",
      "'유행중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99356\n",
      "all accessor variety was computed # words = 99356\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99380\n",
      "all accessor variety was computed # words = 99380\n",
      "'이강인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99382\n",
      "all accessor variety was computed # words = 99382\n",
      "'태국인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99382\n",
      "all accessor variety was computed # words = 99382\n",
      "'빤쓰런'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99397\n",
      "all accessor variety was computed # words = 99397\n",
      "'분노한'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99398\n",
      "all accessor variety was computed # words = 99398\n",
      "'근근웹'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99398\n",
      "all accessor variety was computed # words = 99398\n",
      "'억울이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99417\n",
      "all accessor variety was computed # words = 99417\n",
      "'이희진'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99427\n",
      "all accessor variety was computed # words = 99427\n",
      "'홍콩인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3227\n",
      "all branching entropies was computed # words = 99433\n",
      "all accessor variety was computed # words = 99433\n",
      "'쿄애니'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3229\n",
      "all branching entropies was computed # words = 99441\n",
      "all accessor variety was computed # words = 99441\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3229\n",
      "all branching entropies was computed # words = 99443\n",
      "all accessor variety was computed # words = 99443\n",
      "'떡락'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3230\n",
      "all branching entropies was computed # words = 99520\n",
      "all accessor variety was computed # words = 99520\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99539\n",
      "all accessor variety was computed # words = 99539\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99546\n",
      "all accessor variety was computed # words = 99546\n",
      "'전국노래자랑'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99546\n",
      "all accessor variety was computed # words = 99546\n",
      "'막차가능하냐'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99546\n",
      "all accessor variety was computed # words = 99546\n",
      "'소름돋는다'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99546\n",
      "all accessor variety was computed # words = 99546\n",
      "'푸드트럭'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99548\n",
      "all accessor variety was computed # words = 99548\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99549\n",
      "all accessor variety was computed # words = 99549\n",
      "'보한민국'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99552\n",
      "all accessor variety was computed # words = 99552\n",
      "'유흥탐정'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99554\n",
      "all accessor variety was computed # words = 99554\n",
      "'귀족노조'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 99563\n",
      "all accessor variety was computed # words = 99563\n",
      "'레이싱걸'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99574\n",
      "all accessor variety was computed # words = 99574\n",
      "'폭동절'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99577\n",
      "all accessor variety was computed # words = 99577\n",
      "'테러범'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99577\n",
      "all accessor variety was computed # words = 99577\n",
      "'찍은거'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99582\n",
      "all accessor variety was computed # words = 99582\n",
      "'권양숙'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99583\n",
      "all accessor variety was computed # words = 99583\n",
      "'문정부'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99588\n",
      "all accessor variety was computed # words = 99588\n",
      "'업로더'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3231\n",
      "all branching entropies was computed # words = 99590\n",
      "all accessor variety was computed # words = 99590\n",
      "'박기량'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3233\n",
      "all branching entropies was computed # words = 99598\n",
      "all accessor variety was computed # words = 99598\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3235\n",
      "all branching entropies was computed # words = 99600\n",
      "all accessor variety was computed # words = 99600\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99659\n",
      "all accessor variety was computed # words = 99659\n",
      "'노잼'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99674\n",
      "all accessor variety was computed # words = 99674\n",
      "'폭침'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99677\n",
      "all accessor variety was computed # words = 99677\n",
      "'던파'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99685\n",
      "all accessor variety was computed # words = 99685\n",
      "'서울메트로'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99698\n",
      "all accessor variety was computed # words = 99698\n",
      "'호남향우회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99706\n",
      "all accessor variety was computed # words = 99706\n",
      "'여성배려칸'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3236\n",
      "all branching entropies was computed # words = 99710\n",
      "all accessor variety was computed # words = 99710\n",
      "'태양의후예'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99721\n",
      "all accessor variety was computed # words = 99721\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99722\n",
      "all accessor variety was computed # words = 99722\n",
      "'넥슨시위'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99728\n",
      "all accessor variety was computed # words = 99728\n",
      "'승부조작'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99728\n",
      "all accessor variety was computed # words = 99728\n",
      "'메오후들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99729\n",
      "all accessor variety was computed # words = 99729\n",
      "'더민주당'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99730\n",
      "all accessor variety was computed # words = 99730\n",
      "'그림아재'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3241\n",
      "all branching entropies was computed # words = 99735\n",
      "all accessor variety was computed # words = 99735\n",
      "'생계형'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3242\n",
      "all branching entropies was computed # words = 99748\n",
      "all accessor variety was computed # words = 99748\n",
      "'새키들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3242\n",
      "all branching entropies was computed # words = 99749\n",
      "all accessor variety was computed # words = 99749\n",
      "'심수미'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3242\n",
      "all branching entropies was computed # words = 99751\n",
      "all accessor variety was computed # words = 99751\n",
      "'슨베누'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99869\n",
      "all accessor variety was computed # words = 99869\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99877\n",
      "all accessor variety was computed # words = 99877\n",
      "'건게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99895\n",
      "all accessor variety was computed # words = 99895\n",
      "'아베노믹스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99895\n",
      "all accessor variety was computed # words = 99895\n",
      "'종북세력'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99901\n",
      "all accessor variety was computed # words = 99901\n",
      "'느끼는점'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99903\n",
      "all accessor variety was computed # words = 99903\n",
      "'특례입학'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99907\n",
      "all accessor variety was computed # words = 99907\n",
      "'애플워치'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99916\n",
      "all accessor variety was computed # words = 99916\n",
      "'국토종주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99922\n",
      "all accessor variety was computed # words = 99922\n",
      "'선동글'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99926\n",
      "all accessor variety was computed # words = 99926\n",
      "'제라드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3243\n",
      "all branching entropies was computed # words = 99932\n",
      "all accessor variety was computed # words = 99932\n",
      "'넷우익'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3244\n",
      "all branching entropies was computed # words = 99957\n",
      "all accessor variety was computed # words = 99957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99966\n",
      "all accessor variety was computed # words = 99966\n",
      "'이러고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99968\n",
      "all accessor variety was computed # words = 99968\n",
      "'억게이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99971\n",
      "all accessor variety was computed # words = 99971\n",
      "'천베'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99981\n",
      "all accessor variety was computed # words = 99981\n",
      "'픽업아티스트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99992\n",
      "all accessor variety was computed # words = 99992\n",
      "'의료민영화'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99996\n",
      "all accessor variety was computed # words = 99996\n",
      "'정성산감독'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3246\n",
      "all branching entropies was computed # words = 99999\n",
      "all accessor variety was computed # words = 99999\n",
      "'러시아전'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100029\n",
      "all accessor variety was computed # words = 100029\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100037\n",
      "all accessor variety was computed # words = 100037\n",
      "'대선개입'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100040\n",
      "all accessor variety was computed # words = 100040\n",
      "'서울수복'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100040\n",
      "all accessor variety was computed # words = 100040\n",
      "'유가족이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100041\n",
      "all accessor variety was computed # words = 100041\n",
      "'전라민국'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100047\n",
      "all accessor variety was computed # words = 100047\n",
      "'아스날'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100050\n",
      "all accessor variety was computed # words = 100050\n",
      "'탕웨이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100059\n",
      "all accessor variety was computed # words = 100059\n",
      "'초간단'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100061\n",
      "all accessor variety was computed # words = 100061\n",
      "'울동네'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100062\n",
      "all accessor variety was computed # words = 100062\n",
      "'몽준찡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100067\n",
      "all accessor variety was computed # words = 100067\n",
      "'황장수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100075\n",
      "all accessor variety was computed # words = 100075\n",
      "'짤을'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100076\n",
      "all accessor variety was computed # words = 100076\n",
      "'우리누나'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100076\n",
      "all accessor variety was computed # words = 100076\n",
      "'레전드짤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100087\n",
      "all accessor variety was computed # words = 100087\n",
      "'주호민'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100091\n",
      "all accessor variety was computed # words = 100091\n",
      "'일상툰'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100093\n",
      "all accessor variety was computed # words = 100093\n",
      "'신아람'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100097\n",
      "all accessor variety was computed # words = 100097\n",
      "'화영이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100101\n",
      "all accessor variety was computed # words = 100101\n",
      "'좌태호'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100111\n",
      "all accessor variety was computed # words = 100111\n",
      "'횟집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100116\n",
      "all accessor variety was computed # words = 100116\n",
      "'몬스타엑스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100128\n",
      "all accessor variety was computed # words = 100128\n",
      "'번개장터'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100143\n",
      "all accessor variety was computed # words = 100143\n",
      "'이런경우'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100152\n",
      "all accessor variety was computed # words = 100152\n",
      "'아리아나'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100164\n",
      "all accessor variety was computed # words = 100164\n",
      "'추가남편'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100172\n",
      "all accessor variety was computed # words = 100172\n",
      "'전교등'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100180\n",
      "all accessor variety was computed # words = 100180\n",
      "'먹을만'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100186\n",
      "all accessor variety was computed # words = 100186\n",
      "'흥민이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100191\n",
      "all accessor variety was computed # words = 100191\n",
      "'칼소폭'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100202\n",
      "all accessor variety was computed # words = 100202\n",
      "'내남친'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100203\n",
      "all accessor variety was computed # words = 100203\n",
      "'금동현'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100203\n",
      "all accessor variety was computed # words = 100203\n",
      "'머리색'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100203\n",
      "all accessor variety was computed # words = 100203\n",
      "'갭차'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100205\n",
      "all accessor variety was computed # words = 100205\n",
      "'결혼비용'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100213\n",
      "all accessor variety was computed # words = 100213\n",
      "'이런남자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100219\n",
      "all accessor variety was computed # words = 100219\n",
      "'표절논란'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100229\n",
      "all accessor variety was computed # words = 100229\n",
      "'이혼사유'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100244\n",
      "all accessor variety was computed # words = 100244\n",
      "'공동명의'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100252\n",
      "all accessor variety was computed # words = 100252\n",
      "'거지근성'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100264\n",
      "all accessor variety was computed # words = 100264\n",
      "'밥먹을때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100268\n",
      "all accessor variety was computed # words = 100268\n",
      "'더유닛'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100269\n",
      "all accessor variety was computed # words = 100269\n",
      "'개신기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100291\n",
      "all accessor variety was computed # words = 100291\n",
      "'이남자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100291\n",
      "all accessor variety was computed # words = 100291\n",
      "'피카부'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100291\n",
      "all accessor variety was computed # words = 100291\n",
      "'추추추'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100304\n",
      "all accessor variety was computed # words = 100304\n",
      "'쓰는거'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100304\n",
      "all accessor variety was computed # words = 100304\n",
      "'얼평좀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100309\n",
      "all accessor variety was computed # words = 100309\n",
      "'럽스타'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100309\n",
      "all accessor variety was computed # words = 100309\n",
      "'이채연'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3251\n",
      "all branching entropies was computed # words = 100314\n",
      "all accessor variety was computed # words = 100314\n",
      "'반톡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100436\n",
      "all accessor variety was computed # words = 100436\n",
      "'인듯'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100442\n",
      "all accessor variety was computed # words = 100442\n",
      "'팀명'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100446\n",
      "all accessor variety was computed # words = 100446\n",
      "'보조배터리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100451\n",
      "all accessor variety was computed # words = 100451\n",
      "'스마일데이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100460\n",
      "all accessor variety was computed # words = 100460\n",
      "'네이버페이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100465\n",
      "all accessor variety was computed # words = 100465\n",
      "'엠마왓슨'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100470\n",
      "all accessor variety was computed # words = 100470\n",
      "'씨엔블루'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100488\n",
      "all accessor variety was computed # words = 100488\n",
      "'팝스타'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100489\n",
      "all accessor variety was computed # words = 100489\n",
      "'유기묘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100490\n",
      "all accessor variety was computed # words = 100490\n",
      "'행동들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100496\n",
      "all accessor variety was computed # words = 100496\n",
      "'황정음'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100501\n",
      "all accessor variety was computed # words = 100501\n",
      "'옥택연'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100501\n",
      "all accessor variety was computed # words = 100501\n",
      "'지성호'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100503\n",
      "all accessor variety was computed # words = 100503\n",
      "'자취녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100531\n",
      "all accessor variety was computed # words = 100531\n",
      "'병맛'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100539\n",
      "all accessor variety was computed # words = 100539\n",
      "'콩이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100545\n",
      "all accessor variety was computed # words = 100545\n",
      "'굽네'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100552\n",
      "all accessor variety was computed # words = 100552\n",
      "'긴급생계자금'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100557\n",
      "all accessor variety was computed # words = 100557\n",
      "'익스트랙션'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100557\n",
      "all accessor variety was computed # words = 100557\n",
      "'짜빠구리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100564\n",
      "all accessor variety was computed # words = 100564\n",
      "'피디수첩'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100568\n",
      "all accessor variety was computed # words = 100568\n",
      "'청정지역'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100571\n",
      "all accessor variety was computed # words = 100571\n",
      "'천지일보'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100574\n",
      "all accessor variety was computed # words = 100574\n",
      "'백단장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100576\n",
      "all accessor variety was computed # words = 100576\n",
      "'왕뚜껑'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100581\n",
      "all accessor variety was computed # words = 100581\n",
      "'받을수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100583\n",
      "all accessor variety was computed # words = 100583\n",
      "'만천원'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100584\n",
      "all accessor variety was computed # words = 100584\n",
      "'전범풀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100587\n",
      "all accessor variety was computed # words = 100587\n",
      "'배송비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100591\n",
      "all accessor variety was computed # words = 100591\n",
      "'현황월'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100601\n",
      "all accessor variety was computed # words = 100601\n",
      "'나라꼴'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100615\n",
      "all accessor variety was computed # words = 100615\n",
      "'지들이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100618\n",
      "all accessor variety was computed # words = 100618\n",
      "'북미회담'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100623\n",
      "all accessor variety was computed # words = 100623\n",
      "'정미홍'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100629\n",
      "all accessor variety was computed # words = 100629\n",
      "'풀어본다'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3252\n",
      "all branching entropies was computed # words = 100631\n",
      "all accessor variety was computed # words = 100631\n",
      "'풀영상'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100675\n",
      "all accessor variety was computed # words = 100675\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100682\n",
      "all accessor variety was computed # words = 100682\n",
      "'레이디스코드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100689\n",
      "all accessor variety was computed # words = 100689\n",
      "'베스킨라빈스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100689\n",
      "all accessor variety was computed # words = 100689\n",
      "'한소희'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100698\n",
      "all accessor variety was computed # words = 100698\n",
      "'지쳐서'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100700\n",
      "all accessor variety was computed # words = 100700\n",
      "'대구시장님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100701\n",
      "all accessor variety was computed # words = 100701\n",
      "'백반집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3256\n",
      "all branching entropies was computed # words = 100704\n",
      "all accessor variety was computed # words = 100704\n",
      "'주만에'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3260\n",
      "all branching entropies was computed # words = 100710\n",
      "all accessor variety was computed # words = 100710\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3260\n",
      "all branching entropies was computed # words = 100717\n",
      "all accessor variety was computed # words = 100717\n",
      "'박정희대통령'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3260\n",
      "all branching entropies was computed # words = 100718\n",
      "all accessor variety was computed # words = 100718\n",
      "'팀추월'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3263\n",
      "all branching entropies was computed # words = 100785\n",
      "all accessor variety was computed # words = 100785\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100804\n",
      "all accessor variety was computed # words = 100804\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100807\n",
      "all accessor variety was computed # words = 100807\n",
      "'우리언니'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100810\n",
      "all accessor variety was computed # words = 100810\n",
      "'서예지'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100815\n",
      "all accessor variety was computed # words = 100815\n",
      "'설렜던'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100829\n",
      "all accessor variety was computed # words = 100829\n",
      "'스타들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100829\n",
      "all accessor variety was computed # words = 100829\n",
      "'평창렬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3265\n",
      "all branching entropies was computed # words = 100832\n",
      "all accessor variety was computed # words = 100832\n",
      "'여시저장소'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100855\n",
      "all accessor variety was computed # words = 100855\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100859\n",
      "all accessor variety was computed # words = 100859\n",
      "'문씨'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100859\n",
      "all accessor variety was computed # words = 100859\n",
      "'팬사랑'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100859\n",
      "all accessor variety was computed # words = 100859\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100862\n",
      "all accessor variety was computed # words = 100862\n",
      "'쪽본'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100893\n",
      "all accessor variety was computed # words = 100893\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3268\n",
      "all branching entropies was computed # words = 100893\n",
      "all accessor variety was computed # words = 100893\n",
      "'올린놈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3269\n",
      "all branching entropies was computed # words = 100919\n",
      "all accessor variety was computed # words = 100919\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3269\n",
      "all branching entropies was computed # words = 100919\n",
      "all accessor variety was computed # words = 100919\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3269\n",
      "all branching entropies was computed # words = 100931\n",
      "all accessor variety was computed # words = 100931\n",
      "'조윤선'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100940\n",
      "all accessor variety was computed # words = 100940\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100942\n",
      "all accessor variety was computed # words = 100942\n",
      "'추가우리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100943\n",
      "all accessor variety was computed # words = 100943\n",
      "'오틀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100943\n",
      "all accessor variety was computed # words = 100943\n",
      "'국뽕들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100947\n",
      "all accessor variety was computed # words = 100947\n",
      "'주니엘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100950\n",
      "all accessor variety was computed # words = 100950\n",
      "'신인아이돌'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3271\n",
      "all branching entropies was computed # words = 100960\n",
      "all accessor variety was computed # words = 100960\n",
      "'일보드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3272\n",
      "all branching entropies was computed # words = 100973\n",
      "all accessor variety was computed # words = 100973\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3272\n",
      "all branching entropies was computed # words = 100975\n",
      "all accessor variety was computed # words = 100975\n",
      "'룸빵녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 100981\n",
      "all accessor variety was computed # words = 100981\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 100991\n",
      "all accessor variety was computed # words = 100991\n",
      "'슈퍼카'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 100994\n",
      "all accessor variety was computed # words = 100994\n",
      "'페티쉬'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 101009\n",
      "all accessor variety was computed # words = 101009\n",
      "'여자화장실'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 101013\n",
      "all accessor variety was computed # words = 101013\n",
      "'광화문집회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3275\n",
      "all branching entropies was computed # words = 101018\n",
      "all accessor variety was computed # words = 101018\n",
      "'좌배드림'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3280\n",
      "all branching entropies was computed # words = 101040\n",
      "all accessor variety was computed # words = 101040\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3282\n",
      "all branching entropies was computed # words = 101047\n",
      "all accessor variety was computed # words = 101047\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3285\n",
      "all branching entropies was computed # words = 101058\n",
      "all accessor variety was computed # words = 101058\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3285\n",
      "all branching entropies was computed # words = 101064\n",
      "all accessor variety was computed # words = 101064\n",
      "'사전선거'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3285\n",
      "all branching entropies was computed # words = 101072\n",
      "all accessor variety was computed # words = 101072\n",
      "'중학교때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101080\n",
      "all accessor variety was computed # words = 101080\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101082\n",
      "all accessor variety was computed # words = 101082\n",
      "'이상한점'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101082\n",
      "all accessor variety was computed # words = 101082\n",
      "'톳마늘환'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101094\n",
      "all accessor variety was computed # words = 101094\n",
      "'자살사건'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101099\n",
      "all accessor variety was computed # words = 101099\n",
      "'미국내'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3290\n",
      "all branching entropies was computed # words = 101100\n",
      "all accessor variety was computed # words = 101100\n",
      "'유부게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3292\n",
      "all branching entropies was computed # words = 101120\n",
      "all accessor variety was computed # words = 101120\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3292\n",
      "all branching entropies was computed # words = 101120\n",
      "all accessor variety was computed # words = 101120\n",
      "'만천명'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3295\n",
      "all branching entropies was computed # words = 101152\n",
      "all accessor variety was computed # words = 101152\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3295\n",
      "all branching entropies was computed # words = 101167\n",
      "all accessor variety was computed # words = 101167\n",
      "'전재산'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 101175\n",
      "all accessor variety was computed # words = 101175\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3298\n",
      "all branching entropies was computed # words = 101175\n",
      "all accessor variety was computed # words = 101175\n",
      "'입장문'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3298\n",
      "all branching entropies was computed # words = 101187\n",
      "all accessor variety was computed # words = 101187\n",
      "'이지랄'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3298\n",
      "all branching entropies was computed # words = 101193\n",
      "all accessor variety was computed # words = 101193\n",
      "'똥군기'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3299\n",
      "all branching entropies was computed # words = 101218\n",
      "all accessor variety was computed # words = 101218\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3299\n",
      "all branching entropies was computed # words = 101220\n",
      "all accessor variety was computed # words = 101220\n",
      "'키얼굴'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3300\n",
      "all branching entropies was computed # words = 101271\n",
      "all accessor variety was computed # words = 101271\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3300\n",
      "all branching entropies was computed # words = 101271\n",
      "all accessor variety was computed # words = 101271\n",
      "'마두로'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3300\n",
      "all branching entropies was computed # words = 101286\n",
      "all accessor variety was computed # words = 101286\n",
      "'이길수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3300\n",
      "all branching entropies was computed # words = 101290\n",
      "all accessor variety was computed # words = 101290\n",
      "'갓조국'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3300\n",
      "all branching entropies was computed # words = 101294\n",
      "all accessor variety was computed # words = 101294\n",
      "'누나년'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3301\n",
      "all branching entropies was computed # words = 101330\n",
      "all accessor variety was computed # words = 101330\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3301\n",
      "all branching entropies was computed # words = 101358\n",
      "all accessor variety was computed # words = 101358\n",
      "'왁싱'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3302\n",
      "all branching entropies was computed # words = 101402\n",
      "all accessor variety was computed # words = 101402\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3303\n",
      "all branching entropies was computed # words = 101411\n",
      "all accessor variety was computed # words = 101411\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101479\n",
      "all accessor variety was computed # words = 101479\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101485\n",
      "all accessor variety was computed # words = 101485\n",
      "'섹파'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101485\n",
      "all accessor variety was computed # words = 101485\n",
      "'이수역사건'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101492\n",
      "all accessor variety was computed # words = 101492\n",
      "'총여학생회'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101496\n",
      "all accessor variety was computed # words = 101496\n",
      "'테이저건'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101500\n",
      "all accessor variety was computed # words = 101500\n",
      "'지지자들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101503\n",
      "all accessor variety was computed # words = 101503\n",
      "'허니팝콘'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101504\n",
      "all accessor variety was computed # words = 101504\n",
      "'야간편돌'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3305\n",
      "all branching entropies was computed # words = 101505\n",
      "all accessor variety was computed # words = 101505\n",
      "'스트리머'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101507\n",
      "all accessor variety was computed # words = 101507\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101526\n",
      "all accessor variety was computed # words = 101526\n",
      "'조건만남'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101528\n",
      "all accessor variety was computed # words = 101528\n",
      "'오픈톡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101530\n",
      "all accessor variety was computed # words = 101530\n",
      "'세월충'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101532\n",
      "all accessor variety was computed # words = 101532\n",
      "'냉면집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101542\n",
      "all accessor variety was computed # words = 101542\n",
      "'미군이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101557\n",
      "all accessor variety was computed # words = 101557\n",
      "'상폐년'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101567\n",
      "all accessor variety was computed # words = 101567\n",
      "'김재규'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3308\n",
      "all branching entropies was computed # words = 101572\n",
      "all accessor variety was computed # words = 101572\n",
      "'극과극'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3309\n",
      "all branching entropies was computed # words = 101592\n",
      "all accessor variety was computed # words = 101592\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101614\n",
      "all accessor variety was computed # words = 101614\n",
      "'년뒤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101620\n",
      "all accessor variety was computed # words = 101620\n",
      "'프롤레타리아녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101641\n",
      "all accessor variety was computed # words = 101641\n",
      "'뉴욕타임즈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101641\n",
      "all accessor variety was computed # words = 101641\n",
      "'야간편돌이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101641\n",
      "all accessor variety was computed # words = 101641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101654\n",
      "all accessor variety was computed # words = 101654\n",
      "'나라망신'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101657\n",
      "all accessor variety was computed # words = 101657\n",
      "'신안사건'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101662\n",
      "all accessor variety was computed # words = 101662\n",
      "'방산비리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101662\n",
      "all accessor variety was computed # words = 101662\n",
      "'갓든어택'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3310\n",
      "all branching entropies was computed # words = 101669\n",
      "all accessor variety was computed # words = 101669\n",
      "'개념녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3312\n",
      "all branching entropies was computed # words = 101690\n",
      "all accessor variety was computed # words = 101690\n",
      "'코믹스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3312\n",
      "all branching entropies was computed # words = 101697\n",
      "all accessor variety was computed # words = 101697\n",
      "'로우지'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101716\n",
      "all accessor variety was computed # words = 101716\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101725\n",
      "all accessor variety was computed # words = 101725\n",
      "'히잡'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101729\n",
      "all accessor variety was computed # words = 101729\n",
      "'새정치연합'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101733\n",
      "all accessor variety was computed # words = 101733\n",
      "'사촌여동생'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101737\n",
      "all accessor variety was computed # words = 101737\n",
      "'디씨위키'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101741\n",
      "all accessor variety was computed # words = 101741\n",
      "'캡사이신'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101744\n",
      "all accessor variety was computed # words = 101744\n",
      "'전라동화'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101744\n",
      "all accessor variety was computed # words = 101744\n",
      "'토토가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101745\n",
      "all accessor variety was computed # words = 101745\n",
      "'노미현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101759\n",
      "all accessor variety was computed # words = 101759\n",
      "'회원들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101762\n",
      "all accessor variety was computed # words = 101762\n",
      "'박하선'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101771\n",
      "all accessor variety was computed # words = 101771\n",
      "'개박살'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101786\n",
      "all accessor variety was computed # words = 101786\n",
      "'탈라도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3315\n",
      "all branching entropies was computed # words = 101789\n",
      "all accessor variety was computed # words = 101789\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101802\n",
      "all accessor variety was computed # words = 101802\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101809\n",
      "all accessor variety was computed # words = 101809\n",
      "'반대한'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101829\n",
      "all accessor variety was computed # words = 101829\n",
      "'개월간'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101842\n",
      "all accessor variety was computed # words = 101842\n",
      "'문창극'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101846\n",
      "all accessor variety was computed # words = 101846\n",
      "'레이디각하'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101846\n",
      "all accessor variety was computed # words = 101846\n",
      "'선동꾼들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101848\n",
      "all accessor variety was computed # words = 101848\n",
      "'오스먼드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101849\n",
      "all accessor variety was computed # words = 101849\n",
      "'졸업앨범'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101849\n",
      "all accessor variety was computed # words = 101849\n",
      "'방금일베'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3318\n",
      "all branching entropies was computed # words = 101849\n",
      "all accessor variety was computed # words = 101849\n",
      "'똥싼게이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3319\n",
      "all branching entropies was computed # words = 101864\n",
      "all accessor variety was computed # words = 101864\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3319\n",
      "all branching entropies was computed # words = 101870\n",
      "all accessor variety was computed # words = 101870\n",
      "'네임드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3319\n",
      "all branching entropies was computed # words = 101878\n",
      "all accessor variety was computed # words = 101878\n",
      "'김치들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101896\n",
      "all accessor variety was computed # words = 101896\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101909\n",
      "all accessor variety was computed # words = 101909\n",
      "'유포자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101911\n",
      "all accessor variety was computed # words = 101911\n",
      "'간찰스'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101939\n",
      "all accessor variety was computed # words = 101939\n",
      "'년부터'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101941\n",
      "all accessor variety was computed # words = 101941\n",
      "'후린'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101949\n",
      "all accessor variety was computed # words = 101949\n",
      "'광명납작체'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101959\n",
      "all accessor variety was computed # words = 101959\n",
      "'전투식량'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101961\n",
      "all accessor variety was computed # words = 101961\n",
      "'효성성님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101962\n",
      "all accessor variety was computed # words = 101962\n",
      "'부평역'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3322\n",
      "all branching entropies was computed # words = 101966\n",
      "all accessor variety was computed # words = 101966\n",
      "'보슬년'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101976\n",
      "all accessor variety was computed # words = 101976\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101981\n",
      "all accessor variety was computed # words = 101981\n",
      "'편하게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101981\n",
      "all accessor variety was computed # words = 101981\n",
      "'페북녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101984\n",
      "all accessor variety was computed # words = 101984\n",
      "'양학선'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101985\n",
      "all accessor variety was computed # words = 101985\n",
      "'곽노현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101989\n",
      "all accessor variety was computed # words = 101989\n",
      "'와갤'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101994\n",
      "all accessor variety was computed # words = 101994\n",
      "'몰빵'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101995\n",
      "all accessor variety was computed # words = 101995\n",
      "'늬들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 101997\n",
      "all accessor variety was computed # words = 101997\n",
      "'밀덕'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102007\n",
      "all accessor variety was computed # words = 102007\n",
      "'다이어트중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102010\n",
      "all accessor variety was computed # words = 102010\n",
      "'에이블리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102013\n",
      "all accessor variety was computed # words = 102013\n",
      "'배송시작'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102017\n",
      "all accessor variety was computed # words = 102017\n",
      "'상승곡선'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102017\n",
      "all accessor variety was computed # words = 102017\n",
      "'얼마정도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102018\n",
      "all accessor variety was computed # words = 102018\n",
      "'아이랜드'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3327\n",
      "all branching entropies was computed # words = 102021\n",
      "all accessor variety was computed # words = 102021\n",
      "'닭가슴살'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102036\n",
      "all accessor variety was computed # words = 102036\n",
      "'하시는분'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102042\n",
      "all accessor variety was computed # words = 102042\n",
      "'제로콜라'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102054\n",
      "all accessor variety was computed # words = 102054\n",
      "'지민이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102059\n",
      "all accessor variety was computed # words = 102059\n",
      "'무슨색'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102068\n",
      "all accessor variety was computed # words = 102068\n",
      "'없는듯'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102087\n",
      "all accessor variety was computed # words = 102087\n",
      "'먹는거'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102090\n",
      "all accessor variety was computed # words = 102090\n",
      "'전세집'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102094\n",
      "all accessor variety was computed # words = 102094\n",
      "'휴닝카'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102121\n",
      "all accessor variety was computed # words = 102121\n",
      "'무슨뜻'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102128\n",
      "all accessor variety was computed # words = 102128\n",
      "'여적여'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102132\n",
      "all accessor variety was computed # words = 102132\n",
      "'김광현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102133\n",
      "all accessor variety was computed # words = 102133\n",
      "'버려도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102134\n",
      "all accessor variety was computed # words = 102134\n",
      "'박지민'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102145\n",
      "all accessor variety was computed # words = 102145\n",
      "'나와도'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3328\n",
      "all branching entropies was computed # words = 102147\n",
      "all accessor variety was computed # words = 102147\n",
      "'공홈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102148\n",
      "all accessor variety was computed # words = 102148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102149\n",
      "all accessor variety was computed # words = 102149\n",
      "'뷰러'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102154\n",
      "all accessor variety was computed # words = 102154\n",
      "'놀토'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102165\n",
      "all accessor variety was computed # words = 102165\n",
      "'뭔일'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102168\n",
      "all accessor variety was computed # words = 102168\n",
      "'에셈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102170\n",
      "all accessor variety was computed # words = 102170\n",
      "'가온차트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102170\n",
      "all accessor variety was computed # words = 102170\n",
      "'걸그룹들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102172\n",
      "all accessor variety was computed # words = 102172\n",
      "'피땀눈물'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102172\n",
      "all accessor variety was computed # words = 102172\n",
      "'소름돋아'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102184\n",
      "all accessor variety was computed # words = 102184\n",
      "'미칠것'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102186\n",
      "all accessor variety was computed # words = 102186\n",
      "'타그룹'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102186\n",
      "all accessor variety was computed # words = 102186\n",
      "'남돌중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102187\n",
      "all accessor variety was computed # words = 102187\n",
      "'힘들게'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102192\n",
      "all accessor variety was computed # words = 102192\n",
      "'코성형'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102194\n",
      "all accessor variety was computed # words = 102194\n",
      "'관린이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102198\n",
      "all accessor variety was computed # words = 102198\n",
      "'예단비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102198\n",
      "all accessor variety was computed # words = 102198\n",
      "'민현이'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102199\n",
      "all accessor variety was computed # words = 102199\n",
      "'남우현'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102207\n",
      "all accessor variety was computed # words = 102207\n",
      "'후덜덜'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102207\n",
      "all accessor variety was computed # words = 102207\n",
      "'남프듀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102211\n",
      "all accessor variety was computed # words = 102211\n",
      "'명품백'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102217\n",
      "all accessor variety was computed # words = 102217\n",
      "'철벽녀'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102227\n",
      "all accessor variety was computed # words = 102227\n",
      "'김고은'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102230\n",
      "all accessor variety was computed # words = 102230\n",
      "'애아빠'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102242\n",
      "all accessor variety was computed # words = 102242\n",
      "'싫다고'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102243\n",
      "all accessor variety was computed # words = 102243\n",
      "'조진웅'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102246\n",
      "all accessor variety was computed # words = 102246\n",
      "'류화영'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102248\n",
      "all accessor variety was computed # words = 102248\n",
      "'하민호'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102249\n",
      "all accessor variety was computed # words = 102249\n",
      "'스쉐'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102251\n",
      "all accessor variety was computed # words = 102251\n",
      "'존좋'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102254\n",
      "all accessor variety was computed # words = 102254\n",
      "'뿜뿜'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102257\n",
      "all accessor variety was computed # words = 102257\n",
      "'팬석'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102262\n",
      "all accessor variety was computed # words = 102262\n",
      "'다른남자'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102267\n",
      "all accessor variety was computed # words = 102267\n",
      "'펀쿨섹좌'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102275\n",
      "all accessor variety was computed # words = 102275\n",
      "'자리양보'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102276\n",
      "all accessor variety was computed # words = 102276\n",
      "'청구할인'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102281\n",
      "all accessor variety was computed # words = 102281\n",
      "'경주시장'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102281\n",
      "all accessor variety was computed # words = 102281\n",
      "'등교개학'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102281\n",
      "all accessor variety was computed # words = 102281\n",
      "'길냥이들'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102289\n",
      "all accessor variety was computed # words = 102289\n",
      "'어쩔수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102290\n",
      "all accessor variety was computed # words = 102290\n",
      "'김남주'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102292\n",
      "all accessor variety was computed # words = 102292\n",
      "'리한나'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102293\n",
      "all accessor variety was computed # words = 102293\n",
      "'오연서'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102299\n",
      "all accessor variety was computed # words = 102299\n",
      "'마스크대란'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102306\n",
      "all accessor variety was computed # words = 102306\n",
      "'성지순례단'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102306\n",
      "all accessor variety was computed # words = 102306\n",
      "'레스모아'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102310\n",
      "all accessor variety was computed # words = 102310\n",
      "'천마스크'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102313\n",
      "all accessor variety was computed # words = 102313\n",
      "'워킹스루'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102315\n",
      "all accessor variety was computed # words = 102315\n",
      "'의심증상'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102315\n",
      "all accessor variety was computed # words = 102315\n",
      "'입국자들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102319\n",
      "all accessor variety was computed # words = 102319\n",
      "'한화손보'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102320\n",
      "all accessor variety was computed # words = 102320\n",
      "'아십니까'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102320\n",
      "all accessor variety was computed # words = 102320\n",
      "'대구분들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102328\n",
      "all accessor variety was computed # words = 102328\n",
      "'커피머신'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102331\n",
      "all accessor variety was computed # words = 102331\n",
      "'홈플퀴즈'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102334\n",
      "all accessor variety was computed # words = 102334\n",
      "'편애중계'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102335\n",
      "all accessor variety was computed # words = 102335\n",
      "'의료장비'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102336\n",
      "all accessor variety was computed # words = 102336\n",
      "'후생성'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102336\n",
      "all accessor variety was computed # words = 102336\n",
      "'빌런들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102338\n",
      "all accessor variety was computed # words = 102338\n",
      "'흑백판'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102339\n",
      "all accessor variety was computed # words = 102339\n",
      "'슈퍼문'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102341\n",
      "all accessor variety was computed # words = 102341\n",
      "'송범근'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102342\n",
      "all accessor variety was computed # words = 102342\n",
      "'한시적'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102345\n",
      "all accessor variety was computed # words = 102345\n",
      "'지인들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102353\n",
      "all accessor variety was computed # words = 102353\n",
      "'나올수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102356\n",
      "all accessor variety was computed # words = 102356\n",
      "'시기준'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102356\n",
      "all accessor variety was computed # words = 102356\n",
      "'관객수'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102359\n",
      "all accessor variety was computed # words = 102359\n",
      "'파는데'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102361\n",
      "all accessor variety was computed # words = 102361\n",
      "'판데믹'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3329\n",
      "all branching entropies was computed # words = 102362\n",
      "all accessor variety was computed # words = 102362\n",
      "'출첵'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3332\n",
      "all branching entropies was computed # words = 102373\n",
      "all accessor variety was computed # words = 102373\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3334\n",
      "all branching entropies was computed # words = 102389\n",
      "all accessor variety was computed # words = 102389\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3334\n",
      "all branching entropies was computed # words = 102391\n",
      "all accessor variety was computed # words = 102391\n",
      "'의미가'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3334\n",
      "all branching entropies was computed # words = 102393\n",
      "all accessor variety was computed # words = 102393\n",
      "'펜스부통령'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3337\n",
      "all branching entropies was computed # words = 102399\n",
      "all accessor variety was computed # words = 102399\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3337\n",
      "all branching entropies was computed # words = 102401\n",
      "all accessor variety was computed # words = 102401\n",
      "'징병제'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102408\n",
      "all accessor variety was computed # words = 102408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102412\n",
      "all accessor variety was computed # words = 102412\n",
      "'인명진'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102416\n",
      "all accessor variety was computed # words = 102416\n",
      "'꿀직업'\n",
      "training was done. used memory 1.105 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102431\n",
      "all accessor variety was computed # words = 102431\n",
      "'백댄서'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102432\n",
      "all accessor variety was computed # words = 102432\n",
      "'동양미용타운'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102435\n",
      "all accessor variety was computed # words = 102435\n",
      "'멜론차트'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102440\n",
      "all accessor variety was computed # words = 102440\n",
      "'나혼산'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102441\n",
      "all accessor variety was computed # words = 102441\n",
      "'윤두준'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102456\n",
      "all accessor variety was computed # words = 102456\n",
      "'먹을때'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102460\n",
      "all accessor variety was computed # words = 102460\n",
      "'학원선생님'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102464\n",
      "all accessor variety was computed # words = 102464\n",
      "'명랑핫도그'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102466\n",
      "all accessor variety was computed # words = 102466\n",
      "'실화탐사대'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102466\n",
      "all accessor variety was computed # words = 102466\n",
      "'뽐뻐분들'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102472\n",
      "all accessor variety was computed # words = 102472\n",
      "'무료나눔'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102474\n",
      "all accessor variety was computed # words = 102474\n",
      "'주요국'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102476\n",
      "all accessor variety was computed # words = 102476\n",
      "'감귤국'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102481\n",
      "all accessor variety was computed # words = 102481\n",
      "'플짤'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102482\n",
      "all accessor variety was computed # words = 102482\n",
      "'머전대'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102486\n",
      "all accessor variety was computed # words = 102486\n",
      "'소드녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102486\n",
      "all accessor variety was computed # words = 102486\n",
      "'레드벨벳이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102498\n",
      "all accessor variety was computed # words = 102498\n",
      "'챙기는'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102498\n",
      "all accessor variety was computed # words = 102498\n",
      "'이채영'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102500\n",
      "all accessor variety was computed # words = 102500\n",
      "'후드티'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102502\n",
      "all accessor variety was computed # words = 102502\n",
      "'얇은피'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102505\n",
      "all accessor variety was computed # words = 102505\n",
      "'따먹고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3340\n",
      "all branching entropies was computed # words = 102507\n",
      "all accessor variety was computed # words = 102507\n",
      "'시위현장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3343\n",
      "all branching entropies was computed # words = 102517\n",
      "all accessor variety was computed # words = 102517\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3343\n",
      "all branching entropies was computed # words = 102519\n",
      "all accessor variety was computed # words = 102519\n",
      "'팩트폭력'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3343\n",
      "all branching entropies was computed # words = 102537\n",
      "all accessor variety was computed # words = 102537\n",
      "'움직이는'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3346\n",
      "all branching entropies was computed # words = 102549\n",
      "all accessor variety was computed # words = 102549\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102555\n",
      "all accessor variety was computed # words = 102555\n",
      "'베댓'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102556\n",
      "all accessor variety was computed # words = 102556\n",
      "'옴걸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102565\n",
      "all accessor variety was computed # words = 102565\n",
      "'남친집'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102578\n",
      "all accessor variety was computed # words = 102578\n",
      "'그래픽카드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102588\n",
      "all accessor variety was computed # words = 102588\n",
      "'새아빠'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102591\n",
      "all accessor variety was computed # words = 102591\n",
      "'메르스갤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102604\n",
      "all accessor variety was computed # words = 102604\n",
      "'은혁'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102605\n",
      "all accessor variety was computed # words = 102605\n",
      "'이기광'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102610\n",
      "all accessor variety was computed # words = 102610\n",
      "'투썸'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3347\n",
      "all branching entropies was computed # words = 102621\n",
      "all accessor variety was computed # words = 102621\n",
      "'자살시도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3348\n",
      "all branching entropies was computed # words = 102633\n",
      "all accessor variety was computed # words = 102633\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3350\n",
      "all branching entropies was computed # words = 102636\n",
      "all accessor variety was computed # words = 102636\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3350\n",
      "all branching entropies was computed # words = 102647\n",
      "all accessor variety was computed # words = 102647\n",
      "'한국남성'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3350\n",
      "all branching entropies was computed # words = 102652\n",
      "all accessor variety was computed # words = 102652\n",
      "'키작남'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3352\n",
      "all branching entropies was computed # words = 102659\n",
      "all accessor variety was computed # words = 102659\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3353\n",
      "all branching entropies was computed # words = 102698\n",
      "all accessor variety was computed # words = 102698\n",
      "'군필'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3353\n",
      "all branching entropies was computed # words = 102718\n",
      "all accessor variety was computed # words = 102718\n",
      "'몇키로'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3353\n",
      "all branching entropies was computed # words = 102730\n",
      "all accessor variety was computed # words = 102730\n",
      "'최강자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102753\n",
      "all accessor variety was computed # words = 102753\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102756\n",
      "all accessor variety was computed # words = 102756\n",
      "'노가다꾼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102759\n",
      "all accessor variety was computed # words = 102759\n",
      "'유우머'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102767\n",
      "all accessor variety was computed # words = 102767\n",
      "'저격게이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102767\n",
      "all accessor variety was computed # words = 102767\n",
      "'후폭풍이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3354\n",
      "all branching entropies was computed # words = 102771\n",
      "all accessor variety was computed # words = 102771\n",
      "'맛점'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3357\n",
      "all branching entropies was computed # words = 102791\n",
      "all accessor variety was computed # words = 102791\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3357\n",
      "all branching entropies was computed # words = 102793\n",
      "all accessor variety was computed # words = 102793\n",
      "'개표조작'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102807\n",
      "all accessor variety was computed # words = 102807\n",
      "'년후'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102812\n",
      "all accessor variety was computed # words = 102812\n",
      "'존못'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102813\n",
      "all accessor variety was computed # words = 102813\n",
      "'얼굴인증'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102817\n",
      "all accessor variety was computed # words = 102817\n",
      "'한정판'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102821\n",
      "all accessor variety was computed # words = 102821\n",
      "'동물학대'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102828\n",
      "all accessor variety was computed # words = 102828\n",
      "'광주사태'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102828\n",
      "all accessor variety was computed # words = 102828\n",
      "'한녀충'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102832\n",
      "all accessor variety was computed # words = 102832\n",
      "'경찰버스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102835\n",
      "all accessor variety was computed # words = 102835\n",
      "'관심병사'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102844\n",
      "all accessor variety was computed # words = 102844\n",
      "'이런걸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102846\n",
      "all accessor variety was computed # words = 102846\n",
      "'그림체'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102849\n",
      "all accessor variety was computed # words = 102849\n",
      "'중후반'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102853\n",
      "all accessor variety was computed # words = 102853\n",
      "'저새끼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3358\n",
      "all branching entropies was computed # words = 102859\n",
      "all accessor variety was computed # words = 102859\n",
      "'실검위'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3361\n",
      "all branching entropies was computed # words = 102879\n",
      "all accessor variety was computed # words = 102879\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3364\n",
      "all branching entropies was computed # words = 102885\n",
      "all accessor variety was computed # words = 102885\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3364\n",
      "all branching entropies was computed # words = 102885\n",
      "all accessor variety was computed # words = 102885\n",
      "'동물짤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3364\n",
      "all branching entropies was computed # words = 102885\n",
      "all accessor variety was computed # words = 102885\n",
      "'명확진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3364\n",
      "all branching entropies was computed # words = 102890\n",
      "all accessor variety was computed # words = 102890\n",
      "'소득주도성장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3364\n",
      "all branching entropies was computed # words = 102890\n",
      "all accessor variety was computed # words = 102890\n",
      "'문재인정부'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102901\n",
      "all accessor variety was computed # words = 102901\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 102907\n",
      "all accessor variety was computed # words = 102907\n",
      "'흑인폭동'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102914\n",
      "all accessor variety was computed # words = 102914\n",
      "'대나무숲'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102923\n",
      "all accessor variety was computed # words = 102923\n",
      "'적화통일'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102930\n",
      "all accessor variety was computed # words = 102930\n",
      "'최신근황'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102936\n",
      "all accessor variety was computed # words = 102936\n",
      "'비상식량'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3367\n",
      "all branching entropies was computed # words = 102936\n",
      "all accessor variety was computed # words = 102936\n",
      "'영준게이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3368\n",
      "all branching entropies was computed # words = 102978\n",
      "all accessor variety was computed # words = 102978\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3368\n",
      "all branching entropies was computed # words = 102979\n",
      "all accessor variety was computed # words = 102979\n",
      "'닥터드레'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3368\n",
      "all branching entropies was computed # words = 102983\n",
      "all accessor variety was computed # words = 102983\n",
      "'살해협박'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3368\n",
      "all branching entropies was computed # words = 102987\n",
      "all accessor variety was computed # words = 102987\n",
      "'시선강간'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3371\n",
      "all branching entropies was computed # words = 103016\n",
      "all accessor variety was computed # words = 103016\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3371\n",
      "all branching entropies was computed # words = 103020\n",
      "all accessor variety was computed # words = 103020\n",
      "'돼지열병'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3375\n",
      "all branching entropies was computed # words = 103042\n",
      "all accessor variety was computed # words = 103042\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103049\n",
      "all accessor variety was computed # words = 103049\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103051\n",
      "all accessor variety was computed # words = 103051\n",
      "'두산그룹'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103052\n",
      "all accessor variety was computed # words = 103052\n",
      "'엔젤두환'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103054\n",
      "all accessor variety was computed # words = 103054\n",
      "'공개처형'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103057\n",
      "all accessor variety was computed # words = 103057\n",
      "'폴더블폰'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103060\n",
      "all accessor variety was computed # words = 103060\n",
      "'진짜이유'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3377\n",
      "all branching entropies was computed # words = 103070\n",
      "all accessor variety was computed # words = 103070\n",
      "'리갈하이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3379\n",
      "all branching entropies was computed # words = 103081\n",
      "all accessor variety was computed # words = 103081\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3379\n",
      "all branching entropies was computed # words = 103081\n",
      "all accessor variety was computed # words = 103081\n",
      "'짱깨국'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3379\n",
      "all branching entropies was computed # words = 103088\n",
      "all accessor variety was computed # words = 103088\n",
      "'카레국'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3382\n",
      "all branching entropies was computed # words = 103101\n",
      "all accessor variety was computed # words = 103101\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3384\n",
      "all branching entropies was computed # words = 103109\n",
      "all accessor variety was computed # words = 103109\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3386\n",
      "all branching entropies was computed # words = 103133\n",
      "all accessor variety was computed # words = 103133\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3388\n",
      "all branching entropies was computed # words = 103145\n",
      "all accessor variety was computed # words = 103145\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3388\n",
      "all branching entropies was computed # words = 103158\n",
      "all accessor variety was computed # words = 103158\n",
      "'백인들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3389\n",
      "all branching entropies was computed # words = 103177\n",
      "all accessor variety was computed # words = 103177\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3389\n",
      "all branching entropies was computed # words = 103199\n",
      "all accessor variety was computed # words = 103199\n",
      "'죽여야'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3389\n",
      "all branching entropies was computed # words = 103208\n",
      "all accessor variety was computed # words = 103208\n",
      "'뒤질뻔'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3389\n",
      "all branching entropies was computed # words = 103219\n",
      "all accessor variety was computed # words = 103219\n",
      "'업소녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3389\n",
      "all branching entropies was computed # words = 103229\n",
      "all accessor variety was computed # words = 103229\n",
      "'글삭튀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3392\n",
      "all branching entropies was computed # words = 103246\n",
      "all accessor variety was computed # words = 103246\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3392\n",
      "all branching entropies was computed # words = 103246\n",
      "all accessor variety was computed # words = 103246\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3394\n",
      "all branching entropies was computed # words = 103254\n",
      "all accessor variety was computed # words = 103254\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3394\n",
      "all branching entropies was computed # words = 103261\n",
      "all accessor variety was computed # words = 103261\n",
      "'문신충'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3395\n",
      "all branching entropies was computed # words = 103267\n",
      "all accessor variety was computed # words = 103267\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3398\n",
      "all branching entropies was computed # words = 103303\n",
      "all accessor variety was computed # words = 103303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3400\n",
      "all branching entropies was computed # words = 103310\n",
      "all accessor variety was computed # words = 103310\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3401\n",
      "all branching entropies was computed # words = 103357\n",
      "all accessor variety was computed # words = 103357\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3401\n",
      "all branching entropies was computed # words = 103358\n",
      "all accessor variety was computed # words = 103358\n",
      "'문다혜'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3404\n",
      "all branching entropies was computed # words = 103371\n",
      "all accessor variety was computed # words = 103371\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3404\n",
      "all branching entropies was computed # words = 103371\n",
      "all accessor variety was computed # words = 103371\n",
      "'수영이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3404\n",
      "all branching entropies was computed # words = 103372\n",
      "all accessor variety was computed # words = 103372\n",
      "'그분들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3404\n",
      "all branching entropies was computed # words = 103388\n",
      "all accessor variety was computed # words = 103388\n",
      "'지만원'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3404\n",
      "all branching entropies was computed # words = 103399\n",
      "all accessor variety was computed # words = 103399\n",
      "'노인들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103400\n",
      "all accessor variety was computed # words = 103400\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103400\n",
      "all accessor variety was computed # words = 103400\n",
      "'짱개국'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103402\n",
      "all accessor variety was computed # words = 103402\n",
      "'씹재앙'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103416\n",
      "all accessor variety was computed # words = 103416\n",
      "'마약왕'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103427\n",
      "all accessor variety was computed # words = 103427\n",
      "'현실판'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3407\n",
      "all branching entropies was computed # words = 103427\n",
      "all accessor variety was computed # words = 103427\n",
      "'김민교'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3409\n",
      "all branching entropies was computed # words = 103503\n",
      "all accessor variety was computed # words = 103503\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3418\n",
      "all branching entropies was computed # words = 104578\n",
      "all accessor variety was computed # words = 104578\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3419\n",
      "all branching entropies was computed # words = 104603\n",
      "all accessor variety was computed # words = 104603\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104619\n",
      "all accessor variety was computed # words = 104619\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104619\n",
      "all accessor variety was computed # words = 104619\n",
      "'밴쯔'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104633\n",
      "all accessor variety was computed # words = 104633\n",
      "'페미니스트들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104637\n",
      "all accessor variety was computed # words = 104637\n",
      "'황제도시락'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104637\n",
      "all accessor variety was computed # words = 104637\n",
      "'여성징병제'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104637\n",
      "all accessor variety was computed # words = 104637\n",
      "'여시언냐들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104643\n",
      "all accessor variety was computed # words = 104643\n",
      "'바른미래당'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104647\n",
      "all accessor variety was computed # words = 104647\n",
      "'탄저균백신'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104649\n",
      "all accessor variety was computed # words = 104649\n",
      "'자영업게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104657\n",
      "all accessor variety was computed # words = 104657\n",
      "'사망사건'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104660\n",
      "all accessor variety was computed # words = 104660\n",
      "'유리천장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104660\n",
      "all accessor variety was computed # words = 104660\n",
      "'관훈토론'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104661\n",
      "all accessor variety was computed # words = 104661\n",
      "'입장표명'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104661\n",
      "all accessor variety was computed # words = 104661\n",
      "'김보름이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104674\n",
      "all accessor variety was computed # words = 104674\n",
      "'최저시급'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3420\n",
      "all branching entropies was computed # words = 104678\n",
      "all accessor variety was computed # words = 104678\n",
      "'마루마루'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3421\n",
      "all branching entropies was computed # words = 104691\n",
      "all accessor variety was computed # words = 104691\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3421\n",
      "all branching entropies was computed # words = 104698\n",
      "all accessor variety was computed # words = 104698\n",
      "'이승윤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3421\n",
      "all branching entropies was computed # words = 104698\n",
      "all accessor variety was computed # words = 104698\n",
      "'미해군'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3421\n",
      "all branching entropies was computed # words = 104703\n",
      "all accessor variety was computed # words = 104703\n",
      "'해결법'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3421\n",
      "all branching entropies was computed # words = 104708\n",
      "all accessor variety was computed # words = 104708\n",
      "'좆될뻔'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 104752\n",
      "all accessor variety was computed # words = 104752\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3424\n",
      "all branching entropies was computed # words = 104753\n",
      "all accessor variety was computed # words = 104753\n",
      "'찍어야'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3424\n",
      "all branching entropies was computed # words = 104764\n",
      "all accessor variety was computed # words = 104764\n",
      "'뽑았다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3426\n",
      "all branching entropies was computed # words = 104775\n",
      "all accessor variety was computed # words = 104775\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104779\n",
      "all accessor variety was computed # words = 104779\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104787\n",
      "all accessor variety was computed # words = 104787\n",
      "'피씨방'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104789\n",
      "all accessor variety was computed # words = 104789\n",
      "'갓럼프'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104793\n",
      "all accessor variety was computed # words = 104793\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104800\n",
      "all accessor variety was computed # words = 104800\n",
      "'강은비'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3429\n",
      "all branching entropies was computed # words = 104807\n",
      "all accessor variety was computed # words = 104807\n",
      "'의게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104823\n",
      "all accessor variety was computed # words = 104823\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104825\n",
      "all accessor variety was computed # words = 104825\n",
      "'빚투'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104833\n",
      "all accessor variety was computed # words = 104833\n",
      "'몇장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104834\n",
      "all accessor variety was computed # words = 104834\n",
      "'박근혜대통령'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104834\n",
      "all accessor variety was computed # words = 104834\n",
      "'전라도사람들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104838\n",
      "all accessor variety was computed # words = 104838\n",
      "'씹어먹는'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104839\n",
      "all accessor variety was computed # words = 104839\n",
      "'신안군청'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104845\n",
      "all accessor variety was computed # words = 104845\n",
      "'가혹행위'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104849\n",
      "all accessor variety was computed # words = 104849\n",
      "'일베간거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104854\n",
      "all accessor variety was computed # words = 104854\n",
      "'여시년들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104857\n",
      "all accessor variety was computed # words = 104857\n",
      "'음악의신'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104857\n",
      "all accessor variety was computed # words = 104857\n",
      "'악어아재'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104859\n",
      "all accessor variety was computed # words = 104859\n",
      "'헬스녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3430\n",
      "all branching entropies was computed # words = 104866\n",
      "all accessor variety was computed # words = 104866\n",
      "'이번일'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104887\n",
      "all accessor variety was computed # words = 104887\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104889\n",
      "all accessor variety was computed # words = 104889\n",
      "'불반도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104894\n",
      "all accessor variety was computed # words = 104894\n",
      "'중국군'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104907\n",
      "all accessor variety was computed # words = 104907\n",
      "'좌편향'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104910\n",
      "all accessor variety was computed # words = 104910\n",
      "'관우게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104910\n",
      "all accessor variety was computed # words = 104910\n",
      "'주신이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3432\n",
      "all branching entropies was computed # words = 104910\n",
      "all accessor variety was computed # words = 104910\n",
      "'흥궈신'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3434\n",
      "all branching entropies was computed # words = 104917\n",
      "all accessor variety was computed # words = 104917\n",
      "'이태임'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3434\n",
      "all branching entropies was computed # words = 104921\n",
      "all accessor variety was computed # words = 104921\n",
      "'사까시'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3435\n",
      "all branching entropies was computed # words = 104935\n",
      "all accessor variety was computed # words = 104935\n",
      "'안해도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3440\n",
      "all branching entropies was computed # words = 105100\n",
      "all accessor variety was computed # words = 105100\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3440\n",
      "all branching entropies was computed # words = 105102\n",
      "all accessor variety was computed # words = 105102\n",
      "'개병대'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105113\n",
      "all accessor variety was computed # words = 105113\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105117\n",
      "all accessor variety was computed # words = 105117\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105121\n",
      "all accessor variety was computed # words = 105121\n",
      "'오유소방관'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105124\n",
      "all accessor variety was computed # words = 105124\n",
      "'한식대첩'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105124\n",
      "all accessor variety was computed # words = 105124\n",
      "'문죄인이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105125\n",
      "all accessor variety was computed # words = 105125\n",
      "'여성비하'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105125\n",
      "all accessor variety was computed # words = 105125\n",
      "'앱등이들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3441\n",
      "all branching entropies was computed # words = 105125\n",
      "all accessor variety was computed # words = 105125\n",
      "'좆문대'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105131\n",
      "all accessor variety was computed # words = 105131\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105133\n",
      "all accessor variety was computed # words = 105133\n",
      "'러버덕'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105139\n",
      "all accessor variety was computed # words = 105139\n",
      "'연베대'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105139\n",
      "all accessor variety was computed # words = 105139\n",
      "'좆미개'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105148\n",
      "all accessor variety was computed # words = 105148\n",
      "'보릉내'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105153\n",
      "all accessor variety was computed # words = 105153\n",
      "'권리세'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105163\n",
      "all accessor variety was computed # words = 105163\n",
      "'받아라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105183\n",
      "all accessor variety was computed # words = 105183\n",
      "'고기집'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105194\n",
      "all accessor variety was computed # words = 105194\n",
      "'버젼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105194\n",
      "all accessor variety was computed # words = 105194\n",
      "'딸감'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105197\n",
      "all accessor variety was computed # words = 105197\n",
      "'라인플레이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105202\n",
      "all accessor variety was computed # words = 105202\n",
      "'만물일베설'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105209\n",
      "all accessor variety was computed # words = 105209\n",
      "'달라졌어요'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105214\n",
      "all accessor variety was computed # words = 105214\n",
      "'무쿵현따'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105215\n",
      "all accessor variety was computed # words = 105215\n",
      "'일베간글'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105215\n",
      "all accessor variety was computed # words = 105215\n",
      "'재평가행'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105216\n",
      "all accessor variety was computed # words = 105216\n",
      "'야카오톡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105225\n",
      "all accessor variety was computed # words = 105225\n",
      "'주영신'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105225\n",
      "all accessor variety was computed # words = 105225\n",
      "'병림픽'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105226\n",
      "all accessor variety was computed # words = 105226\n",
      "'브로게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105226\n",
      "all accessor variety was computed # words = 105226\n",
      "'흑형들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105228\n",
      "all accessor variety was computed # words = 105228\n",
      "'클럽녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105231\n",
      "all accessor variety was computed # words = 105231\n",
      "'초식남'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3443\n",
      "all branching entropies was computed # words = 105238\n",
      "all accessor variety was computed # words = 105238\n",
      "'고퀄'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105277\n",
      "all accessor variety was computed # words = 105277\n",
      "'한놈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105282\n",
      "all accessor variety was computed # words = 105282\n",
      "'우리민족끼리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105286\n",
      "all accessor variety was computed # words = 105286\n",
      "'설라디언'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105293\n",
      "all accessor variety was computed # words = 105293\n",
      "'한달동안'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105293\n",
      "all accessor variety was computed # words = 105293\n",
      "'레이디각'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105293\n",
      "all accessor variety was computed # words = 105293\n",
      "'뉴쭉빵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105300\n",
      "all accessor variety was computed # words = 105300\n",
      "'지릴뻔'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105304\n",
      "all accessor variety was computed # words = 105304\n",
      "'닉세탁'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105311\n",
      "all accessor variety was computed # words = 105311\n",
      "'노운지'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105311\n",
      "all accessor variety was computed # words = 105311\n",
      "'눈팅중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105316\n",
      "all accessor variety was computed # words = 105316\n",
      "'애니팡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105317\n",
      "all accessor variety was computed # words = 105317\n",
      "'나나찡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105317\n",
      "all accessor variety was computed # words = 105317\n",
      "'니네들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105324\n",
      "all accessor variety was computed # words = 105324\n",
      "'김치걸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3444\n",
      "all branching entropies was computed # words = 105327\n",
      "all accessor variety was computed # words = 105327\n",
      "'좌위터'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3446\n",
      "all branching entropies was computed # words = 105334\n",
      "all accessor variety was computed # words = 105334\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3446\n",
      "all branching entropies was computed # words = 105334\n",
      "all accessor variety was computed # words = 105334\n",
      "'임신테스트기'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3446\n",
      "all branching entropies was computed # words = 105340\n",
      "all accessor variety was computed # words = 105340\n",
      "'비긴어게인'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3446\n",
      "all branching entropies was computed # words = 105345\n",
      "all accessor variety was computed # words = 105345\n",
      "'머리스타일'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3446\n",
      "all branching entropies was computed # words = 105349\n",
      "all accessor variety was computed # words = 105349\n",
      "'단체사진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105375\n",
      "all accessor variety was computed # words = 105375\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105387\n",
      "all accessor variety was computed # words = 105387\n",
      "'퇴근하고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105408\n",
      "all accessor variety was computed # words = 105408\n",
      "'돌아가고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105408\n",
      "all accessor variety was computed # words = 105408\n",
      "'써본사람'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105410\n",
      "all accessor variety was computed # words = 105410\n",
      "'직고용'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105419\n",
      "all accessor variety was computed # words = 105419\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105425\n",
      "all accessor variety was computed # words = 105425\n",
      "'회피형'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105435\n",
      "all accessor variety was computed # words = 105435\n",
      "'개불쌍'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105435\n",
      "all accessor variety was computed # words = 105435\n",
      "'전소연'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105456\n",
      "all accessor variety was computed # words = 105456\n",
      "'몇시간'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105466\n",
      "all accessor variety was computed # words = 105466\n",
      "'나영석'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105477\n",
      "all accessor variety was computed # words = 105477\n",
      "'입는거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105487\n",
      "all accessor variety was computed # words = 105487\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105496\n",
      "all accessor variety was computed # words = 105496\n",
      "'쌍커풀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105499\n",
      "all accessor variety was computed # words = 105499\n",
      "'어디꺼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105507\n",
      "all accessor variety was computed # words = 105507\n",
      "'만날때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105509\n",
      "all accessor variety was computed # words = 105509\n",
      "'옾챗'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105516\n",
      "all accessor variety was computed # words = 105516\n",
      "'안친'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105522\n",
      "all accessor variety was computed # words = 105522\n",
      "'생얼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105531\n",
      "all accessor variety was computed # words = 105531\n",
      "'옷에'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105541\n",
      "all accessor variety was computed # words = 105541\n",
      "'뚫고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105542\n",
      "all accessor variety was computed # words = 105542\n",
      "'쌍테'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105542\n",
      "all accessor variety was computed # words = 105542\n",
      "'남자연예인'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105545\n",
      "all accessor variety was computed # words = 105545\n",
      "'추가시댁'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105548\n",
      "all accessor variety was computed # words = 105548\n",
      "'음악중심'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105548\n",
      "all accessor variety was computed # words = 105548\n",
      "'판타지오'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105560\n",
      "all accessor variety was computed # words = 105560\n",
      "'웨딩촬영'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105570\n",
      "all accessor variety was computed # words = 105570\n",
      "'독박육아'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105570\n",
      "all accessor variety was computed # words = 105570\n",
      "'하성운이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105575\n",
      "all accessor variety was computed # words = 105575\n",
      "'결혼준비'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105575\n",
      "all accessor variety was computed # words = 105575\n",
      "'욕먹는거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105577\n",
      "all accessor variety was computed # words = 105577\n",
      "'아스트로'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105581\n",
      "all accessor variety was computed # words = 105581\n",
      "'추가엄마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105587\n",
      "all accessor variety was computed # words = 105587\n",
      "'저희엄마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105589\n",
      "all accessor variety was computed # words = 105589\n",
      "'소속사별'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105589\n",
      "all accessor variety was computed # words = 105589\n",
      "'애엄마들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105597\n",
      "all accessor variety was computed # words = 105597\n",
      "'느끼는건'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105600\n",
      "all accessor variety was computed # words = 105600\n",
      "'변백현'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105603\n",
      "all accessor variety was computed # words = 105603\n",
      "'전애인'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105618\n",
      "all accessor variety was computed # words = 105618\n",
      "'시엄마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105642\n",
      "all accessor variety was computed # words = 105642\n",
      "'발연기'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105653\n",
      "all accessor variety was computed # words = 105653\n",
      "'워킹맘'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105657\n",
      "all accessor variety was computed # words = 105657\n",
      "'사귀게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105658\n",
      "all accessor variety was computed # words = 105658\n",
      "'새멤버'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105665\n",
      "all accessor variety was computed # words = 105665\n",
      "'병원비'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105667\n",
      "all accessor variety was computed # words = 105667\n",
      "'류수정'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3449\n",
      "all branching entropies was computed # words = 105673\n",
      "all accessor variety was computed # words = 105673\n",
      "'심한거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3450\n",
      "all branching entropies was computed # words = 105714\n",
      "all accessor variety was computed # words = 105714\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3450\n",
      "all branching entropies was computed # words = 105715\n",
      "all accessor variety was computed # words = 105715\n",
      "'단콘'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3450\n",
      "all branching entropies was computed # words = 105728\n",
      "all accessor variety was computed # words = 105728\n",
      "'엄빠'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3450\n",
      "all branching entropies was computed # words = 105741\n",
      "all accessor variety was computed # words = 105741\n",
      "'돌싱'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105768\n",
      "all accessor variety was computed # words = 105768\n",
      "'같냐'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105770\n",
      "all accessor variety was computed # words = 105770\n",
      "'잘때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105785\n",
      "all accessor variety was computed # words = 105785\n",
      "'우리가족'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105787\n",
      "all accessor variety was computed # words = 105787\n",
      "'헷갈리게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105791\n",
      "all accessor variety was computed # words = 105791\n",
      "'아기냥이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105807\n",
      "all accessor variety was computed # words = 105807\n",
      "'행동이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105823\n",
      "all accessor variety was computed # words = 105823\n",
      "'믹스견'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105826\n",
      "all accessor variety was computed # words = 105826\n",
      "'플린이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105831\n",
      "all accessor variety was computed # words = 105831\n",
      "'태민이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105834\n",
      "all accessor variety was computed # words = 105834\n",
      "'백진희'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3451\n",
      "all branching entropies was computed # words = 105836\n",
      "all accessor variety was computed # words = 105836\n",
      "'이태오'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105878\n",
      "all accessor variety was computed # words = 105878\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105886\n",
      "all accessor variety was computed # words = 105886\n",
      "'피곤한'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105890\n",
      "all accessor variety was computed # words = 105890\n",
      "'죽기전'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105893\n",
      "all accessor variety was computed # words = 105893\n",
      "'화장빨'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105897\n",
      "all accessor variety was computed # words = 105897\n",
      "'중계권'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105901\n",
      "all accessor variety was computed # words = 105901\n",
      "'박지윤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105903\n",
      "all accessor variety was computed # words = 105903\n",
      "'박규리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105918\n",
      "all accessor variety was computed # words = 105918\n",
      "'받으러'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105921\n",
      "all accessor variety was computed # words = 105921\n",
      "'여판'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3454\n",
      "all branching entropies was computed # words = 105940\n",
      "all accessor variety was computed # words = 105940\n",
      "'내눈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 105985\n",
      "all accessor variety was computed # words = 105985\n",
      "'할듯'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 105985\n",
      "all accessor variety was computed # words = 105985\n",
      "'엘조'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 105991\n",
      "all accessor variety was computed # words = 105991\n",
      "'국가비상사태'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106002\n",
      "all accessor variety was computed # words = 106002\n",
      "'전동킥보드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106002\n",
      "all accessor variety was computed # words = 106002\n",
      "'스마일카드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106015\n",
      "all accessor variety was computed # words = 106015\n",
      "'호나우지뉴'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106021\n",
      "all accessor variety was computed # words = 106021\n",
      "'정직한후보'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106037\n",
      "all accessor variety was computed # words = 106037\n",
      "'신호위반'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106037\n",
      "all accessor variety was computed # words = 106037\n",
      "'마스크안'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106040\n",
      "all accessor variety was computed # words = 106040\n",
      "'수출금지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106046\n",
      "all accessor variety was computed # words = 106046\n",
      "'후라이팬'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106050\n",
      "all accessor variety was computed # words = 106050\n",
      "'이상한게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106050\n",
      "all accessor variety was computed # words = 106050\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106059\n",
      "all accessor variety was computed # words = 106059\n",
      "'전문가들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106061\n",
      "all accessor variety was computed # words = 106061\n",
      "'다른거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106061\n",
      "all accessor variety was computed # words = 106061\n",
      "'몇일전'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106068\n",
      "all accessor variety was computed # words = 106068\n",
      "'다녀야'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106096\n",
      "all accessor variety was computed # words = 106096\n",
      "'나와야'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106105\n",
      "all accessor variety was computed # words = 106105\n",
      "'잡으면'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106106\n",
      "all accessor variety was computed # words = 106106\n",
      "'멤버쉽'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106114\n",
      "all accessor variety was computed # words = 106114\n",
      "'회사들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3456\n",
      "all branching entropies was computed # words = 106115\n",
      "all accessor variety was computed # words = 106115\n",
      "'장범준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106136\n",
      "all accessor variety was computed # words = 106136\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106140\n",
      "all accessor variety was computed # words = 106140\n",
      "'못구'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106203\n",
      "all accessor variety was computed # words = 106203\n",
      "'할만'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106212\n",
      "all accessor variety was computed # words = 106212\n",
      "'휴점'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106214\n",
      "all accessor variety was computed # words = 106214\n",
      "'휴원'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106233\n",
      "all accessor variety was computed # words = 106233\n",
      "'수출규제'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3459\n",
      "all branching entropies was computed # words = 106236\n",
      "all accessor variety was computed # words = 106236\n",
      "'미야와'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3460\n",
      "all branching entropies was computed # words = 106243\n",
      "all accessor variety was computed # words = 106243\n",
      "'좋음'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106253\n",
      "all accessor variety was computed # words = 106253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106255\n",
      "all accessor variety was computed # words = 106255\n",
      "'슨파이더'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106257\n",
      "all accessor variety was computed # words = 106257\n",
      "'광고게이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106260\n",
      "all accessor variety was computed # words = 106260\n",
      "'밀양화재'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106262\n",
      "all accessor variety was computed # words = 106262\n",
      "'일재앙'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106263\n",
      "all accessor variety was computed # words = 106263\n",
      "'최민정'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106263\n",
      "all accessor variety was computed # words = 106263\n",
      "'레젼드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106266\n",
      "all accessor variety was computed # words = 106266\n",
      "'뽑는다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106273\n",
      "all accessor variety was computed # words = 106273\n",
      "'의령경찰서'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106291\n",
      "all accessor variety was computed # words = 106291\n",
      "'노무현때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106296\n",
      "all accessor variety was computed # words = 106296\n",
      "'시빌워'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106296\n",
      "all accessor variety was computed # words = 106296\n",
      "'차오루'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106299\n",
      "all accessor variety was computed # words = 106299\n",
      "'나비효'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106302\n",
      "all accessor variety was computed # words = 106302\n",
      "'마동석'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106302\n",
      "all accessor variety was computed # words = 106302\n",
      "'캥거루국'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106306\n",
      "all accessor variety was computed # words = 106306\n",
      "'많은이유'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106306\n",
      "all accessor variety was computed # words = 106306\n",
      "'변휘재'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106310\n",
      "all accessor variety was computed # words = 106310\n",
      "'여수엑스포'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3461\n",
      "all branching entropies was computed # words = 106313\n",
      "all accessor variety was computed # words = 106313\n",
      "'그네찡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3465\n",
      "all branching entropies was computed # words = 106338\n",
      "all accessor variety was computed # words = 106338\n",
      "'페이판'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3465\n",
      "all branching entropies was computed # words = 106338\n",
      "all accessor variety was computed # words = 106338\n",
      "'민효린'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3465\n",
      "all branching entropies was computed # words = 106339\n",
      "all accessor variety was computed # words = 106339\n",
      "'이낙연이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3465\n",
      "all branching entropies was computed # words = 106341\n",
      "all accessor variety was computed # words = 106341\n",
      "'개독교'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3465\n",
      "all branching entropies was computed # words = 106354\n",
      "all accessor variety was computed # words = 106354\n",
      "'언론들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106359\n",
      "all accessor variety was computed # words = 106359\n",
      "'야짤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106369\n",
      "all accessor variety was computed # words = 106369\n",
      "'대북확성기'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106372\n",
      "all accessor variety was computed # words = 106372\n",
      "'그려보자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106374\n",
      "all accessor variety was computed # words = 106374\n",
      "'백주부'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106375\n",
      "all accessor variety was computed # words = 106375\n",
      "'윤완주'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106376\n",
      "all accessor variety was computed # words = 106376\n",
      "'불꽃속으로'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106395\n",
      "all accessor variety was computed # words = 106395\n",
      "'바란다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106395\n",
      "all accessor variety was computed # words = 106395\n",
      "'황찬성'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106395\n",
      "all accessor variety was computed # words = 106395\n",
      "'걸그룹이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106398\n",
      "all accessor variety was computed # words = 106398\n",
      "'조혜수'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3466\n",
      "all branching entropies was computed # words = 106400\n",
      "all accessor variety was computed # words = 106400\n",
      "'정리글'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3468\n",
      "all branching entropies was computed # words = 106424\n",
      "all accessor variety was computed # words = 106424\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3468\n",
      "all branching entropies was computed # words = 106438\n",
      "all accessor variety was computed # words = 106438\n",
      "'다리평가'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3469\n",
      "all branching entropies was computed # words = 106445\n",
      "all accessor variety was computed # words = 106445\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3471\n",
      "all branching entropies was computed # words = 106451\n",
      "all accessor variety was computed # words = 106451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106470\n",
      "all accessor variety was computed # words = 106470\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106485\n",
      "all accessor variety was computed # words = 106485\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106492\n",
      "all accessor variety was computed # words = 106492\n",
      "'바이든'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106495\n",
      "all accessor variety was computed # words = 106495\n",
      "'실물느낌'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106497\n",
      "all accessor variety was computed # words = 106497\n",
      "'악뮤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3474\n",
      "all branching entropies was computed # words = 106497\n",
      "all accessor variety was computed # words = 106497\n",
      "'나대한'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106512\n",
      "all accessor variety was computed # words = 106512\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106517\n",
      "all accessor variety was computed # words = 106517\n",
      "'오유충들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106520\n",
      "all accessor variety was computed # words = 106520\n",
      "'팔로우'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106529\n",
      "all accessor variety was computed # words = 106529\n",
      "'안보고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106539\n",
      "all accessor variety was computed # words = 106539\n",
      "'기사님'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106539\n",
      "all accessor variety was computed # words = 106539\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106542\n",
      "all accessor variety was computed # words = 106542\n",
      "'쇠파이프'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106549\n",
      "all accessor variety was computed # words = 106549\n",
      "'어떤지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106554\n",
      "all accessor variety was computed # words = 106554\n",
      "'운지중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106555\n",
      "all accessor variety was computed # words = 106555\n",
      "'떼껄룩'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106583\n",
      "all accessor variety was computed # words = 106583\n",
      "'제친구'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3475\n",
      "all branching entropies was computed # words = 106593\n",
      "all accessor variety was computed # words = 106593\n",
      "'징역년'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3476\n",
      "all branching entropies was computed # words = 106605\n",
      "all accessor variety was computed # words = 106605\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3476\n",
      "all branching entropies was computed # words = 106607\n",
      "all accessor variety was computed # words = 106607\n",
      "'재인증'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3478\n",
      "all branching entropies was computed # words = 106627\n",
      "all accessor variety was computed # words = 106627\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3479\n",
      "all branching entropies was computed # words = 106639\n",
      "all accessor variety was computed # words = 106639\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3482\n",
      "all branching entropies was computed # words = 106641\n",
      "all accessor variety was computed # words = 106641\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3482\n",
      "all branching entropies was computed # words = 106645\n",
      "all accessor variety was computed # words = 106645\n",
      "'초라넷'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3482\n",
      "all branching entropies was computed # words = 106650\n",
      "all accessor variety was computed # words = 106650\n",
      "'스쿼트'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3482\n",
      "all branching entropies was computed # words = 106651\n",
      "all accessor variety was computed # words = 106651\n",
      "'개좋아'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3482\n",
      "all branching entropies was computed # words = 106684\n",
      "all accessor variety was computed # words = 106684\n",
      "'불닭'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3484\n",
      "all branching entropies was computed # words = 106704\n",
      "all accessor variety was computed # words = 106704\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3485\n",
      "all branching entropies was computed # words = 106743\n",
      "all accessor variety was computed # words = 106743\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3485\n",
      "all branching entropies was computed # words = 106753\n",
      "all accessor variety was computed # words = 106753\n",
      "'종자들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3486\n",
      "all branching entropies was computed # words = 106772\n",
      "all accessor variety was computed # words = 106772\n",
      "'이러는'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3486\n",
      "all branching entropies was computed # words = 106776\n",
      "all accessor variety was computed # words = 106776\n",
      "'확찐자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3486\n",
      "all branching entropies was computed # words = 106780\n",
      "all accessor variety was computed # words = 106780\n",
      "'합필'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3486\n",
      "all branching entropies was computed # words = 106780\n",
      "all accessor variety was computed # words = 106780\n",
      "'야갤성님'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3486\n",
      "all branching entropies was computed # words = 106790\n",
      "all accessor variety was computed # words = 106790\n",
      "'진상손님'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106795\n",
      "all accessor variety was computed # words = 106795\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106795\n",
      "all accessor variety was computed # words = 106795\n",
      "'틀무새들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106796\n",
      "all accessor variety was computed # words = 106796\n",
      "'머구게이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 106797\n",
      "all accessor variety was computed # words = 106797\n",
      "'쿵쾅이들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106807\n",
      "all accessor variety was computed # words = 106807\n",
      "'로드먼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106807\n",
      "all accessor variety was computed # words = 106807\n",
      "'시랭이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106810\n",
      "all accessor variety was computed # words = 106810\n",
      "'가로세로연구소'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106812\n",
      "all accessor variety was computed # words = 106812\n",
      "'조선족게이트'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106818\n",
      "all accessor variety was computed # words = 106818\n",
      "'미래통합당'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3487\n",
      "all branching entropies was computed # words = 106833\n",
      "all accessor variety was computed # words = 106833\n",
      "'연쇄살인마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106843\n",
      "all accessor variety was computed # words = 106843\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106844\n",
      "all accessor variety was computed # words = 106844\n",
      "'담당일진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106847\n",
      "all accessor variety was computed # words = 106847\n",
      "'전범기업'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106854\n",
      "all accessor variety was computed # words = 106854\n",
      "'오르가즘'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106866\n",
      "all accessor variety was computed # words = 106866\n",
      "'트라우마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106872\n",
      "all accessor variety was computed # words = 106872\n",
      "'부비트랩'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106878\n",
      "all accessor variety was computed # words = 106878\n",
      "'일본불매'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106881\n",
      "all accessor variety was computed # words = 106881\n",
      "'반대청원'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106892\n",
      "all accessor variety was computed # words = 106892\n",
      "'세계최초'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3492\n",
      "all branching entropies was computed # words = 106896\n",
      "all accessor variety was computed # words = 106896\n",
      "'폰허브'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3495\n",
      "all branching entropies was computed # words = 106923\n",
      "all accessor variety was computed # words = 106923\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3496\n",
      "all branching entropies was computed # words = 106929\n",
      "all accessor variety was computed # words = 106929\n",
      "'키스방'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3496\n",
      "all branching entropies was computed # words = 106934\n",
      "all accessor variety was computed # words = 106934\n",
      "'여행중'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3497\n",
      "all branching entropies was computed # words = 106952\n",
      "all accessor variety was computed # words = 106952\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3500\n",
      "all branching entropies was computed # words = 106966\n",
      "all accessor variety was computed # words = 106966\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3502\n",
      "all branching entropies was computed # words = 106969\n",
      "all accessor variety was computed # words = 106969\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3505\n",
      "all branching entropies was computed # words = 106991\n",
      "all accessor variety was computed # words = 106991\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3505\n",
      "all branching entropies was computed # words = 106993\n",
      "all accessor variety was computed # words = 106993\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3505\n",
      "all branching entropies was computed # words = 107001\n",
      "all accessor variety was computed # words = 107001\n",
      "'로또등'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3507\n",
      "all branching entropies was computed # words = 107018\n",
      "all accessor variety was computed # words = 107018\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3507\n",
      "all branching entropies was computed # words = 107021\n",
      "all accessor variety was computed # words = 107021\n",
      "'년놈들'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3507\n",
      "all branching entropies was computed # words = 107030\n",
      "all accessor variety was computed # words = 107030\n",
      "'알바녀'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3507\n",
      "all branching entropies was computed # words = 107038\n",
      "all accessor variety was computed # words = 107038\n",
      "'빡치게'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3510\n",
      "all branching entropies was computed # words = 107130\n",
      "all accessor variety was computed # words = 107130\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3510\n",
      "all branching entropies was computed # words = 107130\n",
      "all accessor variety was computed # words = 107130\n",
      "'노재팬'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3513\n",
      "all branching entropies was computed # words = 107144\n",
      "all accessor variety was computed # words = 107144\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3513\n",
      "all branching entropies was computed # words = 107145\n",
      "all accessor variety was computed # words = 107145\n",
      "'페이커'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3514\n",
      "all branching entropies was computed # words = 107157\n",
      "all accessor variety was computed # words = 107157\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3516\n",
      "all branching entropies was computed # words = 107167\n",
      "all accessor variety was computed # words = 107167\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3516\n",
      "all branching entropies was computed # words = 107173\n",
      "all accessor variety was computed # words = 107173\n",
      "'명재권'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3516\n",
      "all branching entropies was computed # words = 107176\n",
      "all accessor variety was computed # words = 107176\n",
      "'여상규'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3516\n",
      "all branching entropies was computed # words = 107190\n",
      "all accessor variety was computed # words = 107190\n",
      "'장하성'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3516\n",
      "all branching entropies was computed # words = 107197\n",
      "all accessor variety was computed # words = 107197\n",
      "'펨코'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3520\n",
      "all branching entropies was computed # words = 107240\n",
      "all accessor variety was computed # words = 107240\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3520\n",
      "all branching entropies was computed # words = 107244\n",
      "all accessor variety was computed # words = 107244\n",
      "'탑슨'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3520\n",
      "all branching entropies was computed # words = 107247\n",
      "all accessor variety was computed # words = 107247\n",
      "'슨리'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3524\n",
      "all branching entropies was computed # words = 107268\n",
      "all accessor variety was computed # words = 107268\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3524\n",
      "all branching entropies was computed # words = 107269\n",
      "all accessor variety was computed # words = 107269\n",
      "'문각기동대'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3524\n",
      "all branching entropies was computed # words = 107271\n",
      "all accessor variety was computed # words = 107271\n",
      "'적폐세력'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3524\n",
      "all branching entropies was computed # words = 107271\n",
      "all accessor variety was computed # words = 107271\n",
      "'도시어부'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3524\n",
      "all branching entropies was computed # words = 107288\n",
      "all accessor variety was computed # words = 107288\n",
      "'쓰레기들'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3527\n",
      "all branching entropies was computed # words = 107310\n",
      "all accessor variety was computed # words = 107310\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3530\n",
      "all branching entropies was computed # words = 107311\n",
      "all accessor variety was computed # words = 107311\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3530\n",
      "all branching entropies was computed # words = 107316\n",
      "all accessor variety was computed # words = 107316\n",
      "'사드반대'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3530\n",
      "all branching entropies was computed # words = 107317\n",
      "all accessor variety was computed # words = 107317\n",
      "'확인사살'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3530\n",
      "all branching entropies was computed # words = 107330\n",
      "all accessor variety was computed # words = 107330\n",
      "'핫바할배'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3530\n",
      "all branching entropies was computed # words = 107335\n",
      "all accessor variety was computed # words = 107335\n",
      "'영업이익'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107347\n",
      "all accessor variety was computed # words = 107347\n",
      "'하는중'\n",
      "training was done. used memory 1.106 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107348\n",
      "all accessor variety was computed # words = 107348\n",
      "'양념단'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107352\n",
      "all accessor variety was computed # words = 107352\n",
      "'탈조선'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107360\n",
      "all accessor variety was computed # words = 107360\n",
      "'때리고'\n",
      "training was done. used memory 1.106 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107365\n",
      "all accessor variety was computed # words = 107365\n",
      "'깝치는'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107380\n",
      "all accessor variety was computed # words = 107380\n",
      "'시바견'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107381\n",
      "all accessor variety was computed # words = 107381\n",
      "'재인이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107385\n",
      "all accessor variety was computed # words = 107385\n",
      "'마크롱'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107394\n",
      "all accessor variety was computed # words = 107394\n",
      "'배려석'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107406\n",
      "all accessor variety was computed # words = 107406\n",
      "'최지룡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3532\n",
      "all branching entropies was computed # words = 107406\n",
      "all accessor variety was computed # words = 107406\n",
      "'좆됐노'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3534\n",
      "all branching entropies was computed # words = 107414\n",
      "all accessor variety was computed # words = 107414\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3537\n",
      "all branching entropies was computed # words = 107428\n",
      "all accessor variety was computed # words = 107428\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3538\n",
      "all branching entropies was computed # words = 107444\n",
      "all accessor variety was computed # words = 107444\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3538\n",
      "all branching entropies was computed # words = 107451\n",
      "all accessor variety was computed # words = 107451\n",
      "'좆집'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3538\n",
      "all branching entropies was computed # words = 107451\n",
      "all accessor variety was computed # words = 107451\n",
      "'호남사람들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107462\n",
      "all accessor variety was computed # words = 107462\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107462\n",
      "all accessor variety was computed # words = 107462\n",
      "'탄핵사유'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107472\n",
      "all accessor variety was computed # words = 107472\n",
      "'중국어선'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107475\n",
      "all accessor variety was computed # words = 107475\n",
      "'마린시티'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107475\n",
      "all accessor variety was computed # words = 107475\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107476\n",
      "all accessor variety was computed # words = 107476\n",
      "'채증게이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107478\n",
      "all accessor variety was computed # words = 107478\n",
      "'대천왕'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 107484\n",
      "all accessor variety was computed # words = 107484\n",
      "'카연갤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107492\n",
      "all accessor variety was computed # words = 107492\n",
      "'북괴군'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3541\n",
      "all branching entropies was computed # words = 107493\n",
      "all accessor variety was computed # words = 107493\n",
      "'씹노잼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3542\n",
      "all branching entropies was computed # words = 107501\n",
      "all accessor variety was computed # words = 107501\n",
      "'킹무성'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3543\n",
      "all branching entropies was computed # words = 107537\n",
      "all accessor variety was computed # words = 107537\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107553\n",
      "all accessor variety was computed # words = 107553\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107560\n",
      "all accessor variety was computed # words = 107560\n",
      "'돌림빵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107562\n",
      "all accessor variety was computed # words = 107562\n",
      "'피닉제'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107564\n",
      "all accessor variety was computed # words = 107564\n",
      "'이자혜'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107566\n",
      "all accessor variety was computed # words = 107566\n",
      "'기보배'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107567\n",
      "all accessor variety was computed # words = 107567\n",
      "'마약녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107570\n",
      "all accessor variety was computed # words = 107570\n",
      "'장노도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107572\n",
      "all accessor variety was computed # words = 107572\n",
      "'나오노'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107572\n",
      "all accessor variety was computed # words = 107572\n",
      "'중고딩나라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107578\n",
      "all accessor variety was computed # words = 107578\n",
      "'현대기아차'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107578\n",
      "all accessor variety was computed # words = 107578\n",
      "'양승오박사'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107593\n",
      "all accessor variety was computed # words = 107593\n",
      "'명이나물'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107596\n",
      "all accessor variety was computed # words = 107596\n",
      "'총기사고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107599\n",
      "all accessor variety was computed # words = 107599\n",
      "'애미창렬'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3546\n",
      "all branching entropies was computed # words = 107600\n",
      "all accessor variety was computed # words = 107600\n",
      "'단룡인들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107609\n",
      "all accessor variety was computed # words = 107609\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107612\n",
      "all accessor variety was computed # words = 107612\n",
      "'찾아보자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107618\n",
      "all accessor variety was computed # words = 107618\n",
      "'스타리그'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107618\n",
      "all accessor variety was computed # words = 107618\n",
      "'화생방'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107619\n",
      "all accessor variety was computed # words = 107619\n",
      "'하이킥'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107627\n",
      "all accessor variety was computed # words = 107627\n",
      "'좆고전'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107635\n",
      "all accessor variety was computed # words = 107635\n",
      "'개노답'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107636\n",
      "all accessor variety was computed # words = 107636\n",
      "'번의사'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107636\n",
      "all accessor variety was computed # words = 107636\n",
      "'이분들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107643\n",
      "all accessor variety was computed # words = 107643\n",
      "'삼둥이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107643\n",
      "all accessor variety was computed # words = 107643\n",
      "'김양건'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107658\n",
      "all accessor variety was computed # words = 107658\n",
      "'창렬이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107661\n",
      "all accessor variety was computed # words = 107661\n",
      "'김천게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107667\n",
      "all accessor variety was computed # words = 107667\n",
      "'새끼냥'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107667\n",
      "all accessor variety was computed # words = 107667\n",
      "'국산차'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107677\n",
      "all accessor variety was computed # words = 107677\n",
      "'뻘짓'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107681\n",
      "all accessor variety was computed # words = 107681\n",
      "'박원순시장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107681\n",
      "all accessor variety was computed # words = 107681\n",
      "'암베충년'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 107697\n",
      "'개그우먼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107700\n",
      "all accessor variety was computed # words = 107700\n",
      "'어벤저스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107701\n",
      "all accessor variety was computed # words = 107701\n",
      "'당하는중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107702\n",
      "all accessor variety was computed # words = 107702\n",
      "'박찬수'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107703\n",
      "all accessor variety was computed # words = 107703\n",
      "'부산게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107703\n",
      "all accessor variety was computed # words = 107703\n",
      "'페북똥'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107725\n",
      "all accessor variety was computed # words = 107725\n",
      "'인시위'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107727\n",
      "all accessor variety was computed # words = 107727\n",
      "'몸인증'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107730\n",
      "all accessor variety was computed # words = 107730\n",
      "'정으니'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107730\n",
      "all accessor variety was computed # words = 107730\n",
      "'유즈맵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107731\n",
      "all accessor variety was computed # words = 107731\n",
      "'민폐남'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107736\n",
      "all accessor variety was computed # words = 107736\n",
      "'아주라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107759\n",
      "all accessor variety was computed # words = 107759\n",
      "'식겁'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107759\n",
      "all accessor variety was computed # words = 107759\n",
      "'광딸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107761\n",
      "all accessor variety was computed # words = 107761\n",
      "'오늘의유머'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107761\n",
      "all accessor variety was computed # words = 107761\n",
      "'유머저장소'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107775\n",
      "all accessor variety was computed # words = 107775\n",
      "'장난전화'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107775\n",
      "all accessor variety was computed # words = 107775\n",
      "'찍은사진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107778\n",
      "all accessor variety was computed # words = 107778\n",
      "'착시현상'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107784\n",
      "all accessor variety was computed # words = 107784\n",
      "'운영지원'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107789\n",
      "all accessor variety was computed # words = 107789\n",
      "'무릎팍'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107795\n",
      "all accessor variety was computed # words = 107795\n",
      "'삼대장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107797\n",
      "all accessor variety was computed # words = 107797\n",
      "'운지절'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107799\n",
      "all accessor variety was computed # words = 107799\n",
      "'패션왕'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107802\n",
      "all accessor variety was computed # words = 107802\n",
      "'무기들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107806\n",
      "all accessor variety was computed # words = 107806\n",
      "'먹는법'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107806\n",
      "all accessor variety was computed # words = 107806\n",
      "'모에화'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107806\n",
      "all accessor variety was computed # words = 107806\n",
      "'박시후'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3548\n",
      "all branching entropies was computed # words = 107810\n",
      "all accessor variety was computed # words = 107810\n",
      "'이명갓'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107831\n",
      "all accessor variety was computed # words = 107831\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107833\n",
      "all accessor variety was computed # words = 107833\n",
      "'손담비'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107839\n",
      "all accessor variety was computed # words = 107839\n",
      "'빨지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107841\n",
      "all accessor variety was computed # words = 107841\n",
      "'하시는분들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107848\n",
      "all accessor variety was computed # words = 107848\n",
      "'써큘레이터'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107851\n",
      "all accessor variety was computed # words = 107851\n",
      "'기사사진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107853\n",
      "all accessor variety was computed # words = 107853\n",
      "'장마전선'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107856\n",
      "all accessor variety was computed # words = 107856\n",
      "'크래비티'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107858\n",
      "all accessor variety was computed # words = 107858\n",
      "'엔시티즌'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107861\n",
      "all accessor variety was computed # words = 107861\n",
      "'블랙스완'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107866\n",
      "all accessor variety was computed # words = 107866\n",
      "'이런애들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107872\n",
      "all accessor variety was computed # words = 107872\n",
      "'무신사'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107872\n",
      "all accessor variety was computed # words = 107872\n",
      "'서공예'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107900\n",
      "all accessor variety was computed # words = 107900\n",
      "'개충격'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107900\n",
      "all accessor variety was computed # words = 107900\n",
      "'오리에'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107901\n",
      "all accessor variety was computed # words = 107901\n",
      "'데뷔곡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107901\n",
      "all accessor variety was computed # words = 107901\n",
      "'윤서빈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107904\n",
      "all accessor variety was computed # words = 107904\n",
      "'미밴드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107908\n",
      "all accessor variety was computed # words = 107908\n",
      "'인생날'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107913\n",
      "all accessor variety was computed # words = 107913\n",
      "'갑분싸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107926\n",
      "all accessor variety was computed # words = 107926\n",
      "'학년때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107929\n",
      "all accessor variety was computed # words = 107929\n",
      "'학기때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107932\n",
      "all accessor variety was computed # words = 107932\n",
      "'시험끝'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107947\n",
      "all accessor variety was computed # words = 107947\n",
      "'이상순'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107956\n",
      "all accessor variety was computed # words = 107956\n",
      "'이쁜거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107962\n",
      "all accessor variety was computed # words = 107962\n",
      "'손편지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107969\n",
      "all accessor variety was computed # words = 107969\n",
      "'입냄새'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107972\n",
      "all accessor variety was computed # words = 107972\n",
      "'김호중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107974\n",
      "all accessor variety was computed # words = 107974\n",
      "'박지현'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3550\n",
      "all branching entropies was computed # words = 107979\n",
      "all accessor variety was computed # words = 107979\n",
      "'종부세'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3551\n",
      "all branching entropies was computed # words = 107987\n",
      "all accessor variety was computed # words = 107987\n",
      "'탈덕'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3552\n",
      "all branching entropies was computed # words = 108018\n",
      "all accessor variety was computed # words = 108018\n",
      "'쓰니'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3553\n",
      "all branching entropies was computed # words = 108055\n",
      "all accessor variety was computed # words = 108055\n",
      "'모솔'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108124\n",
      "all accessor variety was computed # words = 108124\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108127\n",
      "all accessor variety was computed # words = 108127\n",
      "'뭐로'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108129\n",
      "all accessor variety was computed # words = 108129\n",
      "'몬엑'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108130\n",
      "all accessor variety was computed # words = 108130\n",
      "'생윤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108139\n",
      "all accessor variety was computed # words = 108139\n",
      "'테넷'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108139\n",
      "all accessor variety was computed # words = 108139\n",
      "'메가히트곡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108140\n",
      "all accessor variety was computed # words = 108140\n",
      "'음반판매량'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108145\n",
      "all accessor variety was computed # words = 108145\n",
      "'만나는거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108153\n",
      "all accessor variety was computed # words = 108153\n",
      "'연인사이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108157\n",
      "all accessor variety was computed # words = 108157\n",
      "'타이틀곡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108159\n",
      "all accessor variety was computed # words = 108159\n",
      "'아이콘이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108160\n",
      "all accessor variety was computed # words = 108160\n",
      "'좋아하고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108165\n",
      "all accessor variety was computed # words = 108165\n",
      "'대리효도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108166\n",
      "all accessor variety was computed # words = 108166\n",
      "'무대의상'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108175\n",
      "all accessor variety was computed # words = 108175\n",
      "'누구잘못'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108179\n",
      "all accessor variety was computed # words = 108179\n",
      "'추가아내'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108182\n",
      "all accessor variety was computed # words = 108182\n",
      "'친정아빠'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108184\n",
      "all accessor variety was computed # words = 108184\n",
      "'어머님들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108184\n",
      "all accessor variety was computed # words = 108184\n",
      "'나이차이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108187\n",
      "all accessor variety was computed # words = 108187\n",
      "'남편친구'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108190\n",
      "all accessor variety was computed # words = 108190\n",
      "'자기가수'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108191\n",
      "all accessor variety was computed # words = 108191\n",
      "'응원법'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108191\n",
      "all accessor variety was computed # words = 108191\n",
      "'장용준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108222\n",
      "all accessor variety was computed # words = 108222\n",
      "'식구들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108226\n",
      "all accessor variety was computed # words = 108226\n",
      "'살남자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108227\n",
      "all accessor variety was computed # words = 108227\n",
      "'그룹명'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108234\n",
      "all accessor variety was computed # words = 108234\n",
      "'임효준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108236\n",
      "all accessor variety was computed # words = 108236\n",
      "'고경표'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108244\n",
      "all accessor variety was computed # words = 108244\n",
      "'년연애'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108247\n",
      "all accessor variety was computed # words = 108247\n",
      "'한초원'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108249\n",
      "all accessor variety was computed # words = 108249\n",
      "'공승연'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108252\n",
      "all accessor variety was computed # words = 108252\n",
      "'내가수'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108258\n",
      "all accessor variety was computed # words = 108258\n",
      "'쌍액'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108267\n",
      "all accessor variety was computed # words = 108267\n",
      "'내돈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108267\n",
      "all accessor variety was computed # words = 108267\n",
      "'빅스마일데이때'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108273\n",
      "all accessor variety was computed # words = 108273\n",
      "'제작발표회'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108276\n",
      "all accessor variety was computed # words = 108276\n",
      "'슈퍼세이브'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108283\n",
      "all accessor variety was computed # words = 108283\n",
      "'싱글벙글쇼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108293\n",
      "all accessor variety was computed # words = 108293\n",
      "'할머니들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108296\n",
      "all accessor variety was computed # words = 108296\n",
      "'사용가능'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108300\n",
      "all accessor variety was computed # words = 108300\n",
      "'이성친구'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3554\n",
      "all branching entropies was computed # words = 108309\n",
      "all accessor variety was computed # words = 108309\n",
      "'여리여리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108321\n",
      "all accessor variety was computed # words = 108321\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108324\n",
      "all accessor variety was computed # words = 108324\n",
      "'다이어터'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108333\n",
      "all accessor variety was computed # words = 108333\n",
      "'오드아이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108343\n",
      "all accessor variety was computed # words = 108343\n",
      "'동심파괴'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108351\n",
      "all accessor variety was computed # words = 108351\n",
      "'컴쟁이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108357\n",
      "all accessor variety was computed # words = 108357\n",
      "'야롱이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108359\n",
      "all accessor variety was computed # words = 108359\n",
      "'생머리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108360\n",
      "all accessor variety was computed # words = 108360\n",
      "'울엄마'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108361\n",
      "all accessor variety was computed # words = 108361\n",
      "'정자매'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108363\n",
      "all accessor variety was computed # words = 108363\n",
      "'김재경'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108373\n",
      "all accessor variety was computed # words = 108373\n",
      "'그남자'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108379\n",
      "all accessor variety was computed # words = 108379\n",
      "'공민지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108386\n",
      "all accessor variety was computed # words = 108386\n",
      "'미남이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108386\n",
      "all accessor variety was computed # words = 108386\n",
      "'쀼의'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108387\n",
      "all accessor variety was computed # words = 108387\n",
      "'이솜'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108410\n",
      "all accessor variety was computed # words = 108410\n",
      "'해피포인트'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108413\n",
      "all accessor variety was computed # words = 108413\n",
      "'신천지교인'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108423\n",
      "all accessor variety was computed # words = 108423\n",
      "'온라인몰'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108424\n",
      "all accessor variety was computed # words = 108424\n",
      "'전복사고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108425\n",
      "all accessor variety was computed # words = 108425\n",
      "'전자팔찌'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108427\n",
      "all accessor variety was computed # words = 108427\n",
      "'소득기준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108428\n",
      "all accessor variety was computed # words = 108428\n",
      "'평화나라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108433\n",
      "all accessor variety was computed # words = 108433\n",
      "'취약계층'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108433\n",
      "all accessor variety was computed # words = 108433\n",
      "'다시보기'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108436\n",
      "all accessor variety was computed # words = 108436\n",
      "'강남모녀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108444\n",
      "all accessor variety was computed # words = 108444\n",
      "'포교활동'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108450\n",
      "all accessor variety was computed # words = 108450\n",
      "'검사결과'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108461\n",
      "all accessor variety was computed # words = 108461\n",
      "'기재부'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108465\n",
      "all accessor variety was computed # words = 108465\n",
      "'미개봉'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108470\n",
      "all accessor variety was computed # words = 108470\n",
      "'대체품'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108470\n",
      "all accessor variety was computed # words = 108470\n",
      "'검사중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108474\n",
      "all accessor variety was computed # words = 108474\n",
      "'댄스팀'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108475\n",
      "all accessor variety was computed # words = 108475\n",
      "'왓포드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108482\n",
      "all accessor variety was computed # words = 108482\n",
      "'쿠팡맨'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108487\n",
      "all accessor variety was computed # words = 108487\n",
      "'머그컵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108498\n",
      "all accessor variety was computed # words = 108498\n",
      "'외신들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108514\n",
      "all accessor variety was computed # words = 108514\n",
      "'힘든데'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108515\n",
      "all accessor variety was computed # words = 108515\n",
      "'광진을'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108515\n",
      "all accessor variety was computed # words = 108515\n",
      "'리원량'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108520\n",
      "all accessor variety was computed # words = 108520\n",
      "'주말이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108521\n",
      "all accessor variety was computed # words = 108521\n",
      "'마이샵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108527\n",
      "all accessor variety was computed # words = 108527\n",
      "'미쿡'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108535\n",
      "all accessor variety was computed # words = 108535\n",
      "'햇반'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108539\n",
      "all accessor variety was computed # words = 108539\n",
      "'장씩'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108539\n",
      "all accessor variety was computed # words = 108539\n",
      "'문재인씨'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3558\n",
      "all branching entropies was computed # words = 108539\n",
      "all accessor variety was computed # words = 108539\n",
      "'돈까스집'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3561\n",
      "all branching entropies was computed # words = 108546\n",
      "all accessor variety was computed # words = 108546\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3561\n",
      "all branching entropies was computed # words = 108554\n",
      "all accessor variety was computed # words = 108554\n",
      "'분탕들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3564\n",
      "all branching entropies was computed # words = 108579\n",
      "all accessor variety was computed # words = 108579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3564\n",
      "all branching entropies was computed # words = 108580\n",
      "all accessor variety was computed # words = 108580\n",
      "'애미추'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108630\n",
      "all accessor variety was computed # words = 108630\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108632\n",
      "all accessor variety was computed # words = 108632\n",
      "'걸복동'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108632\n",
      "all accessor variety was computed # words = 108632\n",
      "'배현진이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108638\n",
      "all accessor variety was computed # words = 108638\n",
      "'갈때까지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108639\n",
      "all accessor variety was computed # words = 108639\n",
      "'합성사진'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108641\n",
      "all accessor variety was computed # words = 108641\n",
      "'보고한다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108644\n",
      "all accessor variety was computed # words = 108644\n",
      "'좆프리카'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108646\n",
      "all accessor variety was computed # words = 108646\n",
      "'갓형욱'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108652\n",
      "all accessor variety was computed # words = 108652\n",
      "'목함지뢰'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108668\n",
      "all accessor variety was computed # words = 108668\n",
      "'재저격'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3566\n",
      "all branching entropies was computed # words = 108673\n",
      "all accessor variety was computed # words = 108673\n",
      "'현상금'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108693\n",
      "all accessor variety was computed # words = 108693\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108699\n",
      "all accessor variety was computed # words = 108699\n",
      "'최현석'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108707\n",
      "all accessor variety was computed # words = 108707\n",
      "'광주분향소'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108709\n",
      "all accessor variety was computed # words = 108709\n",
      "'좌빨새끼들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108711\n",
      "all accessor variety was computed # words = 108711\n",
      "'소드년들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108716\n",
      "all accessor variety was computed # words = 108716\n",
      "'국제망신'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108729\n",
      "all accessor variety was computed # words = 108729\n",
      "'물어봤다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108735\n",
      "all accessor variety was computed # words = 108735\n",
      "'어제밤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3569\n",
      "all branching entropies was computed # words = 108737\n",
      "all accessor variety was computed # words = 108737\n",
      "'혜령아'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108748\n",
      "all accessor variety was computed # words = 108748\n",
      "'왓다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108749\n",
      "all accessor variety was computed # words = 108749\n",
      "'안된다고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108749\n",
      "all accessor variety was computed # words = 108749\n",
      "'심시티'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108752\n",
      "all accessor variety was computed # words = 108752\n",
      "'그걸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108756\n",
      "all accessor variety was computed # words = 108756\n",
      "'포기하고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3570\n",
      "all branching entropies was computed # words = 108766\n",
      "all accessor variety was computed # words = 108766\n",
      "'예쁘다고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108784\n",
      "all accessor variety was computed # words = 108784\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108801\n",
      "all accessor variety was computed # words = 108801\n",
      "'대인데'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108815\n",
      "all accessor variety was computed # words = 108815\n",
      "'킥보드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108821\n",
      "all accessor variety was computed # words = 108821\n",
      "'굿즈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108843\n",
      "all accessor variety was computed # words = 108843\n",
      "'자궁경부암'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108845\n",
      "all accessor variety was computed # words = 108845\n",
      "'정용화'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108845\n",
      "all accessor variety was computed # words = 108845\n",
      "'박시연'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108860\n",
      "all accessor variety was computed # words = 108860\n",
      "'이하늬'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108861\n",
      "all accessor variety was computed # words = 108861\n",
      "'생긴일'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108861\n",
      "all accessor variety was computed # words = 108861\n",
      "'상속자들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108872\n",
      "all accessor variety was computed # words = 108872\n",
      "'츤데레'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108877\n",
      "all accessor variety was computed # words = 108877\n",
      "'일본애들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108877\n",
      "all accessor variety was computed # words = 108877\n",
      "'펭수쿠션'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108877\n",
      "all accessor variety was computed # words = 108877\n",
      "'확진율'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108882\n",
      "all accessor variety was computed # words = 108882\n",
      "'화이트리스트'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108889\n",
      "all accessor variety was computed # words = 108889\n",
      "'오거돈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108894\n",
      "all accessor variety was computed # words = 108894\n",
      "'미필들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3571\n",
      "all branching entropies was computed # words = 108902\n",
      "all accessor variety was computed # words = 108902\n",
      "'김성모'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3576\n",
      "all branching entropies was computed # words = 108913\n",
      "all accessor variety was computed # words = 108913\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3576\n",
      "all branching entropies was computed # words = 108922\n",
      "all accessor variety was computed # words = 108922\n",
      "'대선주자'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3576\n",
      "all branching entropies was computed # words = 108923\n",
      "all accessor variety was computed # words = 108923\n",
      "'갓현중'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3576\n",
      "all branching entropies was computed # words = 108924\n",
      "all accessor variety was computed # words = 108924\n",
      "'정성룡이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3576\n",
      "all branching entropies was computed # words = 108929\n",
      "all accessor variety was computed # words = 108929\n",
      "'유채영'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3580\n",
      "all branching entropies was computed # words = 108946\n",
      "all accessor variety was computed # words = 108946\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3580\n",
      "all branching entropies was computed # words = 108953\n",
      "all accessor variety was computed # words = 108953\n",
      "'임윤택'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3580\n",
      "all branching entropies was computed # words = 108960\n",
      "all accessor variety was computed # words = 108960\n",
      "'더럽게'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3580\n",
      "all branching entropies was computed # words = 108968\n",
      "all accessor variety was computed # words = 108968\n",
      "'즌라도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 108988\n",
      "all accessor variety was computed # words = 108988\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 108989\n",
      "all accessor variety was computed # words = 108989\n",
      "'실감짤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 108990\n",
      "all accessor variety was computed # words = 108990\n",
      "'닉쿤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 108994\n",
      "all accessor variety was computed # words = 108994\n",
      "'외장하드'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 109004\n",
      "all accessor variety was computed # words = 109004\n",
      "'구매후기'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 109004\n",
      "all accessor variety was computed # words = 109004\n",
      "'부목사'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 109006\n",
      "all accessor variety was computed # words = 109006\n",
      "'옆나라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3583\n",
      "all branching entropies was computed # words = 109009\n",
      "all accessor variety was computed # words = 109009\n",
      "'김의성'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3584\n",
      "all branching entropies was computed # words = 109035\n",
      "all accessor variety was computed # words = 109035\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3584\n",
      "all branching entropies was computed # words = 109044\n",
      "all accessor variety was computed # words = 109044\n",
      "'안녕들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3584\n",
      "all branching entropies was computed # words = 109048\n",
      "all accessor variety was computed # words = 109048\n",
      "'깊티'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3584\n",
      "all branching entropies was computed # words = 109062\n",
      "all accessor variety was computed # words = 109062\n",
      "'만수르'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3584\n",
      "all branching entropies was computed # words = 109062\n",
      "all accessor variety was computed # words = 109062\n",
      "'조원태'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3585\n",
      "all branching entropies was computed # words = 109065\n",
      "all accessor variety was computed # words = 109065\n",
      "'좆족'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3585\n",
      "all branching entropies was computed # words = 109069\n",
      "all accessor variety was computed # words = 109069\n",
      "'갓성'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3585\n",
      "all branching entropies was computed # words = 109069\n",
      "all accessor variety was computed # words = 109069\n",
      "'좌고라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3585\n",
      "all branching entropies was computed # words = 109078\n",
      "all accessor variety was computed # words = 109078\n",
      "'친한친구'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3587\n",
      "all branching entropies was computed # words = 109083\n",
      "all accessor variety was computed # words = 109083\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109111\n",
      "all accessor variety was computed # words = 109111\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109112\n",
      "all accessor variety was computed # words = 109112\n",
      "'사토미'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109113\n",
      "all accessor variety was computed # words = 109113\n",
      "'박시장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109114\n",
      "all accessor variety was computed # words = 109114\n",
      "'김성규'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 109120\n",
      "all accessor variety was computed # words = 109120\n",
      "'몇점'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109122\n",
      "all accessor variety was computed # words = 109122\n",
      "'종교단체'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109124\n",
      "all accessor variety was computed # words = 109124\n",
      "'보배펌'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3589\n",
      "all branching entropies was computed # words = 109130\n",
      "all accessor variety was computed # words = 109130\n",
      "'남자화장실'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3592\n",
      "all branching entropies was computed # words = 109140\n",
      "all accessor variety was computed # words = 109140\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3592\n",
      "all branching entropies was computed # words = 109151\n",
      "all accessor variety was computed # words = 109151\n",
      "'치맥'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3592\n",
      "all branching entropies was computed # words = 109154\n",
      "all accessor variety was computed # words = 109154\n",
      "'청정국'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3592\n",
      "all branching entropies was computed # words = 109163\n",
      "all accessor variety was computed # words = 109163\n",
      "'이춘재'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109203\n",
      "all accessor variety was computed # words = 109203\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109213\n",
      "all accessor variety was computed # words = 109213\n",
      "'생긴거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109213\n",
      "all accessor variety was computed # words = 109213\n",
      "'데이터주의'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109223\n",
      "all accessor variety was computed # words = 109223\n",
      "'얼굴평가'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109232\n",
      "all accessor variety was computed # words = 109232\n",
      "'첫키스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109234\n",
      "all accessor variety was computed # words = 109234\n",
      "'조국사태'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109238\n",
      "all accessor variety was computed # words = 109238\n",
      "'여초딩'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109238\n",
      "all accessor variety was computed # words = 109238\n",
      "'웜톤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109243\n",
      "all accessor variety was computed # words = 109243\n",
      "'추가살'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109249\n",
      "all accessor variety was computed # words = 109249\n",
      "'어떤걸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109254\n",
      "all accessor variety was computed # words = 109254\n",
      "'개꿀인'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109254\n",
      "all accessor variety was computed # words = 109254\n",
      "'보가놈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109255\n",
      "all accessor variety was computed # words = 109255\n",
      "'퍼니제곱'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109266\n",
      "all accessor variety was computed # words = 109266\n",
      "'대북전단'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109275\n",
      "all accessor variety was computed # words = 109275\n",
      "'수원역'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109276\n",
      "all accessor variety was computed # words = 109276\n",
      "'내글'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109277\n",
      "all accessor variety was computed # words = 109277\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109277\n",
      "all accessor variety was computed # words = 109277\n",
      "'남팬'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109277\n",
      "all accessor variety was computed # words = 109277\n",
      "'사전투표소'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3598\n",
      "all branching entropies was computed # words = 109280\n",
      "all accessor variety was computed # words = 109280\n",
      "'급충'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109287\n",
      "all accessor variety was computed # words = 109287\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109311\n",
      "all accessor variety was computed # words = 109311\n",
      "'번호따'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109314\n",
      "all accessor variety was computed # words = 109314\n",
      "'연쇄살인범'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109317\n",
      "all accessor variety was computed # words = 109317\n",
      "'우한페렴'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109333\n",
      "all accessor variety was computed # words = 109333\n",
      "'대형견'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109333\n",
      "all accessor variety was computed # words = 109333\n",
      "'좌파새끼들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109338\n",
      "all accessor variety was computed # words = 109338\n",
      "'여관바리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109338\n",
      "all accessor variety was computed # words = 109338\n",
      "'걸린거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109346\n",
      "all accessor variety was computed # words = 109346\n",
      "'의대생'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109349\n",
      "all accessor variety was computed # words = 109349\n",
      "'여시충'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 109349\n",
      "'많은듯'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109354\n",
      "all accessor variety was computed # words = 109354\n",
      "'이기적인건'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109354\n",
      "all accessor variety was computed # words = 109354\n",
      "'에핑'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109356\n",
      "all accessor variety was computed # words = 109356\n",
      "'옆동네'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109358\n",
      "all accessor variety was computed # words = 109358\n",
      "'유산균'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109361\n",
      "all accessor variety was computed # words = 109361\n",
      "'그랜져'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3600\n",
      "all branching entropies was computed # words = 109362\n",
      "all accessor variety was computed # words = 109362\n",
      "'이명박근혜'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109372\n",
      "all accessor variety was computed # words = 109372\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109376\n",
      "all accessor variety was computed # words = 109376\n",
      "'합리적의심'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109379\n",
      "all accessor variety was computed # words = 109379\n",
      "'패스트트랙'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109385\n",
      "all accessor variety was computed # words = 109385\n",
      "'폭스뉴스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109386\n",
      "all accessor variety was computed # words = 109386\n",
      "'동대구역'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109387\n",
      "all accessor variety was computed # words = 109387\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109397\n",
      "all accessor variety was computed # words = 109397\n",
      "'정치성향'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109400\n",
      "all accessor variety was computed # words = 109400\n",
      "'조리돌림'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109403\n",
      "all accessor variety was computed # words = 109403\n",
      "'이번선거'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109403\n",
      "all accessor variety was computed # words = 109403\n",
      "'블랙시위'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3604\n",
      "all branching entropies was computed # words = 109404\n",
      "all accessor variety was computed # words = 109404\n",
      "'음성판정'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3607\n",
      "all branching entropies was computed # words = 109437\n",
      "all accessor variety was computed # words = 109437\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3607\n",
      "all branching entropies was computed # words = 109449\n",
      "all accessor variety was computed # words = 109449\n",
      "'재드래곤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3607\n",
      "all branching entropies was computed # words = 109451\n",
      "all accessor variety was computed # words = 109451\n",
      "'올리버쌤'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3607\n",
      "all branching entropies was computed # words = 109452\n",
      "all accessor variety was computed # words = 109452\n",
      "'부산시장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3607\n",
      "all branching entropies was computed # words = 109452\n",
      "all accessor variety was computed # words = 109452\n",
      "'빨렙새끼'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3609\n",
      "all branching entropies was computed # words = 109461\n",
      "all accessor variety was computed # words = 109461\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3612\n",
      "all branching entropies was computed # words = 109469\n",
      "all accessor variety was computed # words = 109469\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3612\n",
      "all branching entropies was computed # words = 109477\n",
      "all accessor variety was computed # words = 109477\n",
      "'수개표'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3612\n",
      "all branching entropies was computed # words = 109484\n",
      "all accessor variety was computed # words = 109484\n",
      "'내생각'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3612\n",
      "all branching entropies was computed # words = 109486\n",
      "all accessor variety was computed # words = 109486\n",
      "'성명준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3612\n",
      "all branching entropies was computed # words = 109496\n",
      "all accessor variety was computed # words = 109496\n",
      "'합의금'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3614\n",
      "all branching entropies was computed # words = 109518\n",
      "all accessor variety was computed # words = 109518\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3615\n",
      "all branching entropies was computed # words = 109531\n",
      "all accessor variety was computed # words = 109531\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3617\n",
      "all branching entropies was computed # words = 109570\n",
      "all accessor variety was computed # words = 109570\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3617\n",
      "all branching entropies was computed # words = 109573\n",
      "all accessor variety was computed # words = 109573\n",
      "'유저들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3617\n",
      "all branching entropies was computed # words = 109585\n",
      "all accessor variety was computed # words = 109585\n",
      "'별풍선'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3619\n",
      "all branching entropies was computed # words = 109614\n",
      "all accessor variety was computed # words = 109614\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3619\n",
      "all branching entropies was computed # words = 109620\n",
      "all accessor variety was computed # words = 109620\n",
      "'한달전'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3619\n",
      "all branching entropies was computed # words = 109624\n",
      "all accessor variety was computed # words = 109624\n",
      "'일베내'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3622\n",
      "all branching entropies was computed # words = 109644\n",
      "all accessor variety was computed # words = 109644\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3622\n",
      "all branching entropies was computed # words = 109646\n",
      "all accessor variety was computed # words = 109646\n",
      "'김미균'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3622\n",
      "all branching entropies was computed # words = 109655\n",
      "all accessor variety was computed # words = 109655\n",
      "'두가지'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3622\n",
      "all branching entropies was computed # words = 109656\n",
      "all accessor variety was computed # words = 109656\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3625\n",
      "all branching entropies was computed # words = 109684\n",
      "all accessor variety was computed # words = 109684\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3632\n",
      "all branching entropies was computed # words = 109819\n",
      "all accessor variety was computed # words = 109819\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3632\n",
      "all branching entropies was computed # words = 109825\n",
      "all accessor variety was computed # words = 109825\n",
      "'부모들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109833\n",
      "all accessor variety was computed # words = 109833\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109838\n",
      "all accessor variety was computed # words = 109838\n",
      "'타노스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109846\n",
      "all accessor variety was computed # words = 109846\n",
      "'박상학'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109848\n",
      "all accessor variety was computed # words = 109848\n",
      "'어깨빵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109850\n",
      "all accessor variety was computed # words = 109850\n",
      "'그성별'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3635\n",
      "all branching entropies was computed # words = 109854\n",
      "all accessor variety was computed # words = 109854\n",
      "'노가더'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3638\n",
      "all branching entropies was computed # words = 109880\n",
      "all accessor variety was computed # words = 109880\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3640\n",
      "all branching entropies was computed # words = 109890\n",
      "all accessor variety was computed # words = 109890\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3640\n",
      "all branching entropies was computed # words = 109895\n",
      "all accessor variety was computed # words = 109895\n",
      "'평균키'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3640\n",
      "all branching entropies was computed # words = 109898\n",
      "all accessor variety was computed # words = 109898\n",
      "'깜빵'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3640\n",
      "all branching entropies was computed # words = 109902\n",
      "all accessor variety was computed # words = 109902\n",
      "'유튭'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3641\n",
      "all branching entropies was computed # words = 109920\n",
      "all accessor variety was computed # words = 109920\n",
      "'빨고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3641\n",
      "all branching entropies was computed # words = 109922\n",
      "all accessor variety was computed # words = 109922\n",
      "'윾머'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3641\n",
      "all branching entropies was computed # words = 109922\n",
      "all accessor variety was computed # words = 109922\n",
      "'억명'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 109995\n",
      "all accessor variety was computed # words = 109995\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110008\n",
      "all accessor variety was computed # words = 110008\n",
      "'자유민주주의'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110008\n",
      "all accessor variety was computed # words = 110008\n",
      "'문재인대통령'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110013\n",
      "all accessor variety was computed # words = 110013\n",
      "'롯데월드타워'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110013\n",
      "all accessor variety was computed # words = 110013\n",
      "'레몬테라스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110015\n",
      "all accessor variety was computed # words = 110015\n",
      "'돼지발정제'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110018\n",
      "all accessor variety was computed # words = 110018\n",
      "'국민신문고'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110019\n",
      "all accessor variety was computed # words = 110019\n",
      "'성추행사건'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110019\n",
      "all accessor variety was computed # words = 110019\n",
      "'댓글조작단'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110019\n",
      "all accessor variety was computed # words = 110019\n",
      "'연관검색어'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110025\n",
      "all accessor variety was computed # words = 110025\n",
      "'어금니아빠'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110027\n",
      "all accessor variety was computed # words = 110027\n",
      "'국회연설'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110027\n",
      "all accessor variety was computed # words = 110027\n",
      "'상폐년들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110034\n",
      "all accessor variety was computed # words = 110034\n",
      "'랜섬웨어'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110037\n",
      "all accessor variety was computed # words = 110037\n",
      "'망한이유'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110042\n",
      "all accessor variety was computed # words = 110042\n",
      "'선제타격'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110047\n",
      "all accessor variety was computed # words = 110047\n",
      "'관세폭탄'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3642\n",
      "all branching entropies was computed # words = 110050\n",
      "all accessor variety was computed # words = 110050\n",
      "'자지듀스'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3646\n",
      "all branching entropies was computed # words = 110075\n",
      "all accessor variety was computed # words = 110075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110093\n",
      "all accessor variety was computed # words = 110093\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110097\n",
      "all accessor variety was computed # words = 110097\n",
      "'증거인멸'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110100\n",
      "all accessor variety was computed # words = 110100\n",
      "'캡틴마블'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110103\n",
      "all accessor variety was computed # words = 110103\n",
      "'지능수준'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110109\n",
      "all accessor variety was computed # words = 110109\n",
      "'여학생들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110113\n",
      "all accessor variety was computed # words = 110113\n",
      "'핵실험장'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110134\n",
      "all accessor variety was computed # words = 110134\n",
      "'광주폭동'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110137\n",
      "all accessor variety was computed # words = 110137\n",
      "'범죄도시'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110137\n",
      "all accessor variety was computed # words = 110137\n",
      "'원조갓카'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110139\n",
      "all accessor variety was computed # words = 110139\n",
      "'문제인이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110144\n",
      "all accessor variety was computed # words = 110144\n",
      "'보내줘라'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110158\n",
      "all accessor variety was computed # words = 110158\n",
      "'문캠프'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110162\n",
      "all accessor variety was computed # words = 110162\n",
      "'시간전'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110162\n",
      "all accessor variety was computed # words = 110162\n",
      "'좆망각'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110162\n",
      "all accessor variety was computed # words = 110162\n",
      "'탄핵각'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3651\n",
      "all branching entropies was computed # words = 110171\n",
      "all accessor variety was computed # words = 110171\n",
      "'신혜식'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3653\n",
      "all branching entropies was computed # words = 110187\n",
      "all accessor variety was computed # words = 110187\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110206\n",
      "all accessor variety was computed # words = 110206\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110219\n",
      "all accessor variety was computed # words = 110219\n",
      "'사람줍'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110219\n",
      "all accessor variety was computed # words = 110219\n",
      "'폭행범'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110219\n",
      "all accessor variety was computed # words = 110219\n",
      "'천만평'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110227\n",
      "all accessor variety was computed # words = 110227\n",
      "'죽여도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110230\n",
      "all accessor variety was computed # words = 110230\n",
      "'보라니'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110232\n",
      "all accessor variety was computed # words = 110232\n",
      "'기쁨조'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110234\n",
      "all accessor variety was computed # words = 110234\n",
      "'김평우'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110239\n",
      "all accessor variety was computed # words = 110239\n",
      "'리설주'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110249\n",
      "all accessor variety was computed # words = 110249\n",
      "'좆된다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110274\n",
      "all accessor variety was computed # words = 110274\n",
      "'느낀다'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3654\n",
      "all branching entropies was computed # words = 110279\n",
      "all accessor variety was computed # words = 110279\n",
      "'정철승'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3655\n",
      "all branching entropies was computed # words = 110329\n",
      "all accessor variety was computed # words = 110329\n",
      "'할것'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3655\n",
      "all branching entropies was computed # words = 110340\n",
      "all accessor variety was computed # words = 110340\n",
      "'닉값'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3655\n",
      "all branching entropies was computed # words = 110343\n",
      "all accessor variety was computed # words = 110343\n",
      "'홍카'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3655\n",
      "all branching entropies was computed # words = 110345\n",
      "all accessor variety was computed # words = 110345\n",
      "'우리집댕댕이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3655\n",
      "all branching entropies was computed # words = 110345\n",
      "all accessor variety was computed # words = 110345\n",
      "'대정부질문'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3661\n",
      "all branching entropies was computed # words = 110345\n",
      "all accessor variety was computed # words = 110345\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3661\n",
      "all branching entropies was computed # words = 110345\n",
      "all accessor variety was computed # words = 110345\n",
      "'민중연합당'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3664\n",
      "all branching entropies was computed # words = 110353\n",
      "all accessor variety was computed # words = 110353\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3664\n",
      "all branching entropies was computed # words = 110364\n",
      "all accessor variety was computed # words = 110364\n",
      "'없어져야'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3664\n",
      "all branching entropies was computed # words = 110366\n",
      "all accessor variety was computed # words = 110366\n",
      "'예비후보'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3664\n",
      "all branching entropies was computed # words = 110370\n",
      "all accessor variety was computed # words = 110370\n",
      "'뉴스타파'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3664\n",
      "all branching entropies was computed # words = 110372\n",
      "all accessor variety was computed # words = 110372\n",
      "'미국대선'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3668\n",
      "all branching entropies was computed # words = 110386\n",
      "all accessor variety was computed # words = 110386\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3668\n",
      "all branching entropies was computed # words = 110387\n",
      "all accessor variety was computed # words = 110387\n",
      "'시발련아'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3668\n",
      "all branching entropies was computed # words = 110390\n",
      "all accessor variety was computed # words = 110390\n",
      "'청년배당'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110447\n",
      "all accessor variety was computed # words = 110447\n",
      "'하는이유'\n",
      "training was done. used memory 1.110 Gb1.081 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110453\n",
      "all accessor variety was computed # words = 110453\n",
      "'로드뷰'\n",
      "training was done. used memory 1.105 Gb1.081 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110461\n",
      "all accessor variety was computed # words = 110461\n",
      "'시위중'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110464\n",
      "all accessor variety was computed # words = 110464\n",
      "'파워볼'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110473\n",
      "all accessor variety was computed # words = 110473\n",
      "'그만좀'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110474\n",
      "all accessor variety was computed # words = 110474\n",
      "'까는글'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110475\n",
      "all accessor variety was computed # words = 110475\n",
      "'창녀촌'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110476\n",
      "all accessor variety was computed # words = 110476\n",
      "'오상진'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110487\n",
      "all accessor variety was computed # words = 110487\n",
      "'맞는말'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110499\n",
      "all accessor variety was computed # words = 110499\n",
      "'거부한'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3670\n",
      "all branching entropies was computed # words = 110503\n",
      "all accessor variety was computed # words = 110503\n",
      "'원펀맨'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3673\n",
      "all branching entropies was computed # words = 110538\n",
      "all accessor variety was computed # words = 110538\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3676\n",
      "all branching entropies was computed # words = 110588\n",
      "all accessor variety was computed # words = 110588\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3676\n",
      "all branching entropies was computed # words = 110588\n",
      "all accessor variety was computed # words = 110588\n",
      "'순실이'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3676\n",
      "all branching entropies was computed # words = 110598\n",
      "all accessor variety was computed # words = 110598\n",
      "'나쁜놈'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3676\n",
      "all branching entropies was computed # words = 110605\n",
      "all accessor variety was computed # words = 110605\n",
      "'임요환'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3676\n",
      "all branching entropies was computed # words = 110608\n",
      "all accessor variety was computed # words = 110608\n",
      "'머슬퀸'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110636\n",
      "all accessor variety was computed # words = 110636\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110640\n",
      "all accessor variety was computed # words = 110640\n",
      "'퇴근후'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110647\n",
      "all accessor variety was computed # words = 110647\n",
      "'씹도'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110650\n",
      "all accessor variety was computed # words = 110650\n",
      "'차벽'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110651\n",
      "all accessor variety was computed # words = 110651\n",
      "'팔빈'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110656\n",
      "all accessor variety was computed # words = 110656\n",
      "'새정치민주연합'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110656\n",
      "all accessor variety was computed # words = 110656\n",
      "'자연인이다'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110657\n",
      "all accessor variety was computed # words = 110657\n",
      "'대북지원'\n",
      "training was done. used memory 1.109 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110657\n",
      "all accessor variety was computed # words = 110657\n",
      "'막차가능'\n",
      "training was done. used memory 1.108 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110660\n",
      "all accessor variety was computed # words = 110660\n",
      "'중대발표'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110673\n",
      "all accessor variety was computed # words = 110673\n",
      "'전자발찌'\n",
      "training was done. used memory 1.105 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110676\n",
      "all accessor variety was computed # words = 110676\n",
      "'혜선엄마'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110682\n",
      "all accessor variety was computed # words = 110682\n",
      "'폐암게이'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110682\n",
      "all accessor variety was computed # words = 110682\n",
      "'주갤성님'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110687\n",
      "all accessor variety was computed # words = 110687\n",
      "'스폰지밥'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110689\n",
      "all accessor variety was computed # words = 110689\n",
      "'킹스맨'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110697\n",
      "all accessor variety was computed # words = 110697\n",
      "'오유똥'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110711\n",
      "all accessor variety was computed # words = 110711\n",
      "'인물들'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110715\n",
      "all accessor variety was computed # words = 110715\n",
      "'백청강'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3677\n",
      "all branching entropies was computed # words = 110729\n",
      "all accessor variety was computed # words = 110729\n",
      "'하는일'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3678\n",
      "all branching entropies was computed # words = 110736\n",
      "all accessor variety was computed # words = 110736\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3678\n",
      "all branching entropies was computed # words = 110742\n",
      "all accessor variety was computed # words = 110742\n",
      "'타진요'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3678\n",
      "all branching entropies was computed # words = 110743\n",
      "all accessor variety was computed # words = 110743\n",
      "'딸치고'\n",
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110754\n",
      "all accessor variety was computed # words = 110754\n",
      "training was done. used memory 1.108 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110754\n",
      "all accessor variety was computed # words = 110754\n",
      "'좆노잼'\n",
      "training was done. used memory 1.105 Gb1.081 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110758\n",
      "all accessor variety was computed # words = 110758\n",
      "'잘한거'\n",
      "training was done. used memory 1.109 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110759\n",
      "all accessor variety was computed # words = 110759\n",
      "'경호게'\n",
      "training was done. used memory 1.105 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110762\n",
      "all accessor variety was computed # words = 110762\n",
      "'좌효리'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110764\n",
      "all accessor variety was computed # words = 110764\n",
      "'이병장'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110771\n",
      "all accessor variety was computed # words = 110771\n",
      "'최문순'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110771\n",
      "all accessor variety was computed # words = 110771\n",
      "'으리저장소화덕게'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110771\n",
      "all accessor variety was computed # words = 110771\n",
      "'노알라무쿵현따'\n",
      "training was done. used memory 1.109 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110771\n",
      "all accessor variety was computed # words = 110771\n",
      "'카카오스토리'\n",
      "training was done. used memory 1.107 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110771\n",
      "all accessor variety was computed # words = 110771\n",
      "'정성산감독님'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110774\n",
      "all accessor variety was computed # words = 110774\n",
      "'상황정리'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110774\n",
      "all accessor variety was computed # words = 110774\n",
      "'페북상황'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110790\n",
      "all accessor variety was computed # words = 110790\n",
      "'영어실력'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110791\n",
      "all accessor variety was computed # words = 110791\n",
      "'논문표절'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110798\n",
      "all accessor variety was computed # words = 110798\n",
      "'성재기님'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110800\n",
      "all accessor variety was computed # words = 110800\n",
      "'네이마르'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110800\n",
      "all accessor variety was computed # words = 110800\n",
      "'일베충설'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110800\n",
      "all accessor variety was computed # words = 110800\n",
      "'막국수게'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110803\n",
      "all accessor variety was computed # words = 110803\n",
      "'유식대장'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110803\n",
      "all accessor variety was computed # words = 110803\n",
      "'네임드화'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110810\n",
      "all accessor variety was computed # words = 110810\n",
      "'워킹데드'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110818\n",
      "all accessor variety was computed # words = 110818\n",
      "'한마디만'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110827\n",
      "all accessor variety was computed # words = 110827\n",
      "'이상황'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110827\n",
      "all accessor variety was computed # words = 110827\n",
      "'물회게'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110829\n",
      "all accessor variety was computed # words = 110829\n",
      "'개념글'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110829\n",
      "all accessor variety was computed # words = 110829\n",
      "'빨개요'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110831\n",
      "all accessor variety was computed # words = 110831\n",
      "'김신욱'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110836\n",
      "all accessor variety was computed # words = 110836\n",
      "'죽은거'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110840\n",
      "all accessor variety was computed # words = 110840\n",
      "'시간뒤'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110843\n",
      "all accessor variety was computed # words = 110843\n",
      "'축구계'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3680\n",
      "all branching entropies was computed # words = 110847\n",
      "all accessor variety was computed # words = 110847\n",
      "'디스전'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110859\n",
      "all accessor variety was computed # words = 110859\n",
      "training was done. used memory 1.109 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110859\n",
      "all accessor variety was computed # words = 110859\n",
      "'협박게'\n",
      "training was done. used memory 1.108 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110866\n",
      "all accessor variety was computed # words = 110866\n",
      "'로드맨'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110866\n",
      "all accessor variety was computed # words = 110866\n",
      "'일밍을'\n",
      "training was done. used memory 1.104 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110868\n",
      "all accessor variety was computed # words = 110868\n",
      "'문지애'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110872\n",
      "all accessor variety was computed # words = 110872\n",
      "'하마스'\n",
      "training was done. used memory 1.105 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110878\n",
      "all accessor variety was computed # words = 110878\n",
      "'별창'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110883\n",
      "all accessor variety was computed # words = 110883\n",
      "'류뚱'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110887\n",
      "all accessor variety was computed # words = 110887\n",
      "'민주통합당'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110889\n",
      "all accessor variety was computed # words = 110889\n",
      "'정글의법칙'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110899\n",
      "all accessor variety was computed # words = 110899\n",
      "'유해사이트'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110913\n",
      "all accessor variety was computed # words = 110913\n",
      "'들어왔다'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110917\n",
      "all accessor variety was computed # words = 110917\n",
      "'조별과제'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110921\n",
      "all accessor variety was computed # words = 110921\n",
      "'세레모니'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110923\n",
      "all accessor variety was computed # words = 110923\n",
      "'인증짤'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110924\n",
      "all accessor variety was computed # words = 110924\n",
      "'관진찡'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110936\n",
      "all accessor variety was computed # words = 110936\n",
      "'위대함'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110936\n",
      "all accessor variety was computed # words = 110936\n",
      "'병신력'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110946\n",
      "all accessor variety was computed # words = 110946\n",
      "'죽을뻔'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110955\n",
      "all accessor variety was computed # words = 110955\n",
      "'간짜장'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110957\n",
      "all accessor variety was computed # words = 110957\n",
      "'금칙어'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110958\n",
      "all accessor variety was computed # words = 110958\n",
      "'군필자'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110963\n",
      "all accessor variety was computed # words = 110963\n",
      "'구럼비'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110970\n",
      "all accessor variety was computed # words = 110970\n",
      "'조세호'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110972\n",
      "all accessor variety was computed # words = 110972\n",
      "'흑누나'\n",
      "training was done. used memory 1.103 Gb1.081 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110980\n",
      "all accessor variety was computed # words = 110980\n",
      "'유느님'\n",
      "training was done. used memory 1.107 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110982\n",
      "all accessor variety was computed # words = 110982\n",
      "'노시계'\n",
      "training was done. used memory 1.105 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3682\n",
      "all branching entropies was computed # words = 110985\n",
      "all accessor variety was computed # words = 110985\n",
      "'자기야'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3683\n",
      "all branching entropies was computed # words = 111026\n",
      "all accessor variety was computed # words = 111026\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111044\n",
      "all accessor variety was computed # words = 111044\n",
      "'드뎌'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111047\n",
      "all accessor variety was computed # words = 111047\n",
      "'좀머'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111047\n",
      "all accessor variety was computed # words = 111047\n",
      "'창문형에어컨'\n",
      "training was done. used memory 1.107 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111055\n",
      "all accessor variety was computed # words = 111055\n",
      "'지하주차장'\n",
      "training was done. used memory 1.104 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111063\n",
      "all accessor variety was computed # words = 111063\n",
      "'대장내시경'\n",
      "training was done. used memory 1.105 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111065\n",
      "all accessor variety was computed # words = 111065\n",
      "'스포티파이'\n",
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111065\n",
      "all accessor variety was computed # words = 111065\n",
      "'비슷한거'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.105 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3684\n",
      "all branching entropies was computed # words = 111068\n",
      "all accessor variety was computed # words = 111068\n",
      "'결혼식장'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111073\n",
      "all accessor variety was computed # words = 111073\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111081\n",
      "all accessor variety was computed # words = 111081\n",
      "'영어공부'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111087\n",
      "all accessor variety was computed # words = 111087\n",
      "'혼인신고'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111087\n",
      "all accessor variety was computed # words = 111087\n",
      "'티오피즈'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111104\n",
      "all accessor variety was computed # words = 111104\n",
      "'집단폭행'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111106\n",
      "all accessor variety was computed # words = 111106\n",
      "'보이그룹'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111109\n",
      "all accessor variety was computed # words = 111109\n",
      "'솔로곡'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111115\n",
      "all accessor variety was computed # words = 111115\n",
      "'모은돈'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111117\n",
      "all accessor variety was computed # words = 111117\n",
      "'파는거'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111119\n",
      "all accessor variety was computed # words = 111119\n",
      "'코수술'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111121\n",
      "all accessor variety was computed # words = 111121\n",
      "'비소식'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111127\n",
      "all accessor variety was computed # words = 111127\n",
      "'일진들'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111138\n",
      "all accessor variety was computed # words = 111138\n",
      "'내방역'\n",
      "training was done. used memory 1.107 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111139\n",
      "all accessor variety was computed # words = 111139\n",
      "'관계성'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111149\n",
      "all accessor variety was computed # words = 111149\n",
      "'힘들때'\n",
      "training was done. used memory 1.104 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111152\n",
      "all accessor variety was computed # words = 111152\n",
      "'존맛'\n",
      "training was done. used memory 1.108 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111158\n",
      "all accessor variety was computed # words = 111158\n",
      "'뭐뭐'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111173\n",
      "all accessor variety was computed # words = 111173\n",
      "'맥북'\n",
      "training was done. used memory 1.106 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111180\n",
      "all accessor variety was computed # words = 111180\n",
      "'같긴'\n",
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111188\n",
      "all accessor variety was computed # words = 111188\n",
      "'저랑'\n",
      "training was done. used memory 1.107 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111191\n",
      "all accessor variety was computed # words = 111191\n",
      "'메보'\n",
      "training was done. used memory 1.104 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111199\n",
      "all accessor variety was computed # words = 111199\n",
      "'탐폰'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111199\n",
      "all accessor variety was computed # words = 111199\n",
      "'조녜'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111206\n",
      "all accessor variety was computed # words = 111206\n",
      "'순삭'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111210\n",
      "all accessor variety was computed # words = 111210\n",
      "'번씩'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111213\n",
      "all accessor variety was computed # words = 111213\n",
      "'갠적'\n",
      "training was done. used memory 1.105 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111223\n",
      "all accessor variety was computed # words = 111223\n",
      "'산후조리원'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111223\n",
      "all accessor variety was computed # words = 111223\n",
      "'결혼준비중'\n",
      "training was done. used memory 1.107 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111228\n",
      "all accessor variety was computed # words = 111228\n",
      "'전여자친구'\n",
      "training was done. used memory 1.104 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111235\n",
      "all accessor variety was computed # words = 111235\n",
      "'아이돌계'\n",
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111239\n",
      "all accessor variety was computed # words = 111239\n",
      "'동물병원'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111253\n",
      "all accessor variety was computed # words = 111253\n",
      "'부모님들'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111256\n",
      "all accessor variety was computed # words = 111256\n",
      "'시댁제사'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111264\n",
      "all accessor variety was computed # words = 111264\n",
      "'힘들다고'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111268\n",
      "all accessor variety was computed # words = 111268\n",
      "'누가잘못'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111271\n",
      "all accessor variety was computed # words = 111271\n",
      "'인성논란'\n",
      "training was done. used memory 1.104 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111276\n",
      "all accessor variety was computed # words = 111276\n",
      "'공개연애'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111276\n",
      "all accessor variety was computed # words = 111276\n",
      "'싱크로율'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111277\n",
      "all accessor variety was computed # words = 111277\n",
      "'방탄지민'\n",
      "training was done. used memory 1.104 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111279\n",
      "all accessor variety was computed # words = 111279\n",
      "'이용자수'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111281\n",
      "all accessor variety was computed # words = 111281\n",
      "'안부전화'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3687\n",
      "all branching entropies was computed # words = 111284\n",
      "all accessor variety was computed # words = 111284\n",
      "'추가신랑'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111291\n",
      "all accessor variety was computed # words = 111291\n",
      "'나왔으면'\n",
      "training was done. used memory 1.105 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111294\n",
      "all accessor variety was computed # words = 111294\n",
      "'엔터톡'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111298\n",
      "all accessor variety was computed # words = 111298\n",
      "'친척들'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111301\n",
      "all accessor variety was computed # words = 111301\n",
      "'같은반'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111314\n",
      "all accessor variety was computed # words = 111314\n",
      "'대초반'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111318\n",
      "all accessor variety was computed # words = 111318\n",
      "'말도안'\n",
      "training was done. used memory 1.104 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111332\n",
      "all accessor variety was computed # words = 111332\n",
      "'넘사벽'\n",
      "training was done. used memory 1.106 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111340\n",
      "all accessor variety was computed # words = 111340\n",
      "'저때문'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111342\n",
      "all accessor variety was computed # words = 111342\n",
      "'김청하'\n",
      "training was done. used memory 1.106 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111345\n",
      "all accessor variety was computed # words = 111345\n",
      "'받는거'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111353\n",
      "all accessor variety was computed # words = 111353\n",
      "'추가저'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111360\n",
      "all accessor variety was computed # words = 111360\n",
      "'불륜녀'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111368\n",
      "all accessor variety was computed # words = 111368\n",
      "'주지훈'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111368\n",
      "all accessor variety was computed # words = 111368\n",
      "'봇노잼'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111369\n",
      "all accessor variety was computed # words = 111369\n",
      "'베레모'\n",
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111374\n",
      "all accessor variety was computed # words = 111374\n",
      "'맴찢'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111377\n",
      "all accessor variety was computed # words = 111377\n",
      "'풀메'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111378\n",
      "all accessor variety was computed # words = 111378\n",
      "'립밤'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111382\n",
      "all accessor variety was computed # words = 111382\n",
      "'팬질'\n",
      "training was done. used memory 1.104 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111385\n",
      "all accessor variety was computed # words = 111385\n",
      "'상메'\n",
      "training was done. used memory 1.104 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111385\n",
      "all accessor variety was computed # words = 111385\n",
      "'다녤'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111387\n",
      "all accessor variety was computed # words = 111387\n",
      "'옹녤'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111394\n",
      "all accessor variety was computed # words = 111394\n",
      "'애프터스쿨'\n",
      "training was done. used memory 1.104 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111404\n",
      "all accessor variety was computed # words = 111404\n",
      "'시페이지급'\n",
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111409\n",
      "all accessor variety was computed # words = 111409\n",
      "'헤어졌는데'\n",
      "training was done. used memory 1.107 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111418\n",
      "all accessor variety was computed # words = 111418\n",
      "'말라뮤트'\n",
      "training was done. used memory 1.103 Gb1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 3689\n",
      "all branching entropies was computed # words = 111418\n",
      "all accessor variety was computed # words = 111418\n",
      "'카톡내용'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111436\n",
      "all accessor variety was computed # words = 111436\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111437\n",
      "all accessor variety was computed # words = 111437\n",
      "'얼짱도전'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111437\n",
      "all accessor variety was computed # words = 111437\n",
      "'우리카드'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111442\n",
      "all accessor variety was computed # words = 111442\n",
      "'셀카봉'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111443\n",
      "all accessor variety was computed # words = 111443\n",
      "'윤아라'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111476\n",
      "all accessor variety was computed # words = 111476\n",
      "'당할뻔'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.103 Gb1.076 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111483\n",
      "all accessor variety was computed # words = 111483\n",
      "'제품들'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111491\n",
      "all accessor variety was computed # words = 111491\n",
      "'먹는게'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111494\n",
      "all accessor variety was computed # words = 111494\n",
      "'쌍용차'\n",
      "training was done. used memory 1.103 Gb1.080 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111496\n",
      "all accessor variety was computed # words = 111496\n",
      "'양홍석'\n",
      "training was done. used memory 1.103 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111497\n",
      "all accessor variety was computed # words = 111497\n",
      "'윤진이'\n",
      "training was done. used memory 1.103 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111507\n",
      "all accessor variety was computed # words = 111507\n",
      "'웃게'\n",
      "training was done. used memory 1.108 Gb1.078 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111507\n",
      "all accessor variety was computed # words = 111507\n",
      "'뽐뿌게시판'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111509\n",
      "all accessor variety was computed # words = 111509\n",
      "'코로나대응'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111519\n",
      "all accessor variety was computed # words = 111519\n",
      "'현금영수증'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111524\n",
      "all accessor variety was computed # words = 111524\n",
      "'뭉쳐야찬다'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111528\n",
      "all accessor variety was computed # words = 111528\n",
      "'열린민주당'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111531\n",
      "all accessor variety was computed # words = 111531\n",
      "'익스플로러'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111535\n",
      "all accessor variety was computed # words = 111535\n",
      "'중국정부'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111537\n",
      "all accessor variety was computed # words = 111537\n",
      "'쇼챔피언'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111543\n",
      "all accessor variety was computed # words = 111543\n",
      "'무급휴가'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111545\n",
      "all accessor variety was computed # words = 111545\n",
      "'판매시간'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111545\n",
      "all accessor variety was computed # words = 111545\n",
      "'스마트워'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111554\n",
      "all accessor variety was computed # words = 111554\n",
      "'만들어야'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111554\n",
      "all accessor variety was computed # words = 111554\n",
      "'스맥다운'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111562\n",
      "all accessor variety was computed # words = 111562\n",
      "'재림예수'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111587\n",
      "all accessor variety was computed # words = 111587\n",
      "'간호사들'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111587\n",
      "all accessor variety was computed # words = 111587\n",
      "'일본놈들'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111590\n",
      "all accessor variety was computed # words = 111590\n",
      "'본부장님'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3691\n",
      "all branching entropies was computed # words = 111593\n",
      "all accessor variety was computed # words = 111593\n",
      "'아동수당'\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111617\n",
      "all accessor variety was computed # words = 111617\n",
      "training was done. used memory 1.108 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111624\n",
      "all accessor variety was computed # words = 111624\n",
      "'좋은것'\n",
      "training was done. used memory 1.107 Gb1.079 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111628\n",
      "all accessor variety was computed # words = 111628\n",
      "'사람중'\n",
      "training was done. used memory 1.107 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111628\n",
      "all accessor variety was computed # words = 111628\n",
      "'같기도'\n",
      "training was done. used memory 1.103 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111639\n",
      "all accessor variety was computed # words = 111639\n",
      "'정부탓'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111649\n",
      "all accessor variety was computed # words = 111649\n",
      "'돈가스'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111651\n",
      "all accessor variety was computed # words = 111651\n",
      "'판매처'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111652\n",
      "all accessor variety was computed # words = 111652\n",
      "'최강욱'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111657\n",
      "all accessor variety was computed # words = 111657\n",
      "'사용한'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111665\n",
      "all accessor variety was computed # words = 111665\n",
      "'나올때'\n",
      "training was done. used memory 1.104 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111665\n",
      "all accessor variety was computed # words = 111665\n",
      "'이원일'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111669\n",
      "all accessor variety was computed # words = 111669\n",
      "'천만명'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111669\n",
      "all accessor variety was computed # words = 111669\n",
      "'오수아'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111672\n",
      "all accessor variety was computed # words = 111672\n",
      "'삼다수'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111675\n",
      "all accessor variety was computed # words = 111675\n",
      "'미증시'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111675\n",
      "all accessor variety was computed # words = 111675\n",
      "'갤탭'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111683\n",
      "all accessor variety was computed # words = 111683\n",
      "'마윈'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111691\n",
      "all accessor variety was computed # words = 111691\n",
      "'맥맨'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111694\n",
      "all accessor variety was computed # words = 111694\n",
      "'폼페이오'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111695\n",
      "all accessor variety was computed # words = 111695\n",
      "'여사님'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111699\n",
      "all accessor variety was computed # words = 111699\n",
      "'텐프로'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111700\n",
      "all accessor variety was computed # words = 111700\n",
      "'정베'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111703\n",
      "all accessor variety was computed # words = 111703\n",
      "'노로바이러스'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111704\n",
      "all accessor variety was computed # words = 111704\n",
      "'아시안컵'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111714\n",
      "all accessor variety was computed # words = 111714\n",
      "'순대국'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111714\n",
      "all accessor variety was computed # words = 111714\n",
      "'엠바고'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3694\n",
      "all branching entropies was computed # words = 111715\n",
      "all accessor variety was computed # words = 111715\n",
      "'놀랐다'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111758\n",
      "all accessor variety was computed # words = 111758\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111760\n",
      "all accessor variety was computed # words = 111760\n",
      "'코끼리게이'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111763\n",
      "all accessor variety was computed # words = 111763\n",
      "'섹스할때'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111785\n",
      "all accessor variety was computed # words = 111785\n",
      "'총궐기'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111790\n",
      "all accessor variety was computed # words = 111790\n",
      "'이말년'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3697\n",
      "all branching entropies was computed # words = 111790\n",
      "all accessor variety was computed # words = 111790\n",
      "'로리물'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3699\n",
      "all branching entropies was computed # words = 111818\n",
      "all accessor variety was computed # words = 111818\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3699\n",
      "all branching entropies was computed # words = 111819\n",
      "all accessor variety was computed # words = 111819\n",
      "'원세훈'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3699\n",
      "all branching entropies was computed # words = 111823\n",
      "all accessor variety was computed # words = 111823\n",
      "'속궁합'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3700\n",
      "all branching entropies was computed # words = 111828\n",
      "all accessor variety was computed # words = 111828\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111894\n",
      "all accessor variety was computed # words = 111894\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111894\n",
      "all accessor variety was computed # words = 111894\n",
      "'로늘의류머'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111896\n",
      "all accessor variety was computed # words = 111896\n",
      "'보픈카'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111896\n",
      "all accessor variety was computed # words = 111896\n",
      "'엉덩국'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111898\n",
      "all accessor variety was computed # words = 111898\n",
      "'보고쿠폰'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111900\n",
      "all accessor variety was computed # words = 111900\n",
      "'좋아하는게'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111913\n",
      "all accessor variety was computed # words = 111913\n",
      "'생겼습니다'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111914\n",
      "all accessor variety was computed # words = 111914\n",
      "'남주혁'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111918\n",
      "all accessor variety was computed # words = 111918\n",
      "'결혼자금'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111920\n",
      "all accessor variety was computed # words = 111920\n",
      "'퇴치법'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111930\n",
      "all accessor variety was computed # words = 111930\n",
      "'코숏'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111936\n",
      "all accessor variety was computed # words = 111936\n",
      "'한화손해보험'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111936\n",
      "all accessor variety was computed # words = 111936\n",
      "'경기재난소득'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111945\n",
      "all accessor variety was computed # words = 111945\n",
      "'강남구청장'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111950\n",
      "all accessor variety was computed # words = 111950\n",
      "'새벽배송'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111950\n",
      "all accessor variety was computed # words = 111950\n",
      "'유료회원'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111954\n",
      "all accessor variety was computed # words = 111954\n",
      "'촉법소년'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111958\n",
      "all accessor variety was computed # words = 111958\n",
      "'선거문자'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111961\n",
      "all accessor variety was computed # words = 111961\n",
      "'미스터주'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111964\n",
      "all accessor variety was computed # words = 111964\n",
      "'썼는데'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111967\n",
      "all accessor variety was computed # words = 111967\n",
      "'이기고'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3704\n",
      "all branching entropies was computed # words = 111967\n",
      "all accessor variety was computed # words = 111967\n",
      "'짱개새끼들'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3707\n",
      "all branching entropies was computed # words = 111980\n",
      "all accessor variety was computed # words = 111980\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3707\n",
      "all branching entropies was computed # words = 111980\n",
      "all accessor variety was computed # words = 111980\n",
      "'곰탕집'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3707\n",
      "all branching entropies was computed # words = 111986\n",
      "all accessor variety was computed # words = 111986\n",
      "'몸파는'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 111998\n",
      "all accessor variety was computed # words = 111998\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112008\n",
      "all accessor variety was computed # words = 112008\n",
      "'추천수'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112010\n",
      "all accessor variety was computed # words = 112010\n",
      "'존내'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112017\n",
      "all accessor variety was computed # words = 112017\n",
      "'한미연합훈련'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112019\n",
      "all accessor variety was computed # words = 112019\n",
      "'핵포기'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112027\n",
      "all accessor variety was computed # words = 112027\n",
      "'도와주지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112032\n",
      "all accessor variety was computed # words = 112032\n",
      "'아침식사'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112038\n",
      "all accessor variety was computed # words = 112038\n",
      "'조추첨'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112054\n",
      "all accessor variety was computed # words = 112054\n",
      "'공포영화'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112060\n",
      "all accessor variety was computed # words = 112060\n",
      "'건들지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112067\n",
      "all accessor variety was computed # words = 112067\n",
      "'어디다'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112073\n",
      "all accessor variety was computed # words = 112073\n",
      "'바꾸고'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112080\n",
      "all accessor variety was computed # words = 112080\n",
      "'쓴거'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112086\n",
      "all accessor variety was computed # words = 112086\n",
      "'월급관리'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112093\n",
      "all accessor variety was computed # words = 112093\n",
      "'아픈데'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112095\n",
      "all accessor variety was computed # words = 112095\n",
      "'눕방'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112095\n",
      "all accessor variety was computed # words = 112095\n",
      "'제주여행'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112101\n",
      "all accessor variety was computed # words = 112101\n",
      "'이덴트'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112111\n",
      "all accessor variety was computed # words = 112111\n",
      "'아니랑'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112111\n",
      "all accessor variety was computed # words = 112111\n",
      "'김상교'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112114\n",
      "all accessor variety was computed # words = 112114\n",
      "'부산여중생'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112118\n",
      "all accessor variety was computed # words = 112118\n",
      "'황다건'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112136\n",
      "all accessor variety was computed # words = 112136\n",
      "'모링'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112139\n",
      "all accessor variety was computed # words = 112139\n",
      "'너네학교'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112142\n",
      "all accessor variety was computed # words = 112142\n",
      "'임신개월'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112145\n",
      "all accessor variety was computed # words = 112145\n",
      "'교복치마'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112157\n",
      "all accessor variety was computed # words = 112157\n",
      "'사귀기'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112157\n",
      "all accessor variety was computed # words = 112157\n",
      "'틴탑이'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112159\n",
      "all accessor variety was computed # words = 112159\n",
      "'똘끼'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112159\n",
      "all accessor variety was computed # words = 112159\n",
      "'대북송금'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112165\n",
      "all accessor variety was computed # words = 112165\n",
      "'동근이'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112165\n",
      "all accessor variety was computed # words = 112165\n",
      "'카톡대화'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112169\n",
      "all accessor variety was computed # words = 112169\n",
      "'말빨'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112174\n",
      "all accessor variety was computed # words = 112174\n",
      "'마스크사러'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112180\n",
      "all accessor variety was computed # words = 112180\n",
      "'마스크들'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112198\n",
      "all accessor variety was computed # words = 112198\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3710\n",
      "all branching entropies was computed # words = 112198\n",
      "all accessor variety was computed # words = 112198\n",
      "'저장용'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3712\n",
      "all branching entropies was computed # words = 112219\n",
      "all accessor variety was computed # words = 112219\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3712\n",
      "all branching entropies was computed # words = 112227\n",
      "all accessor variety was computed # words = 112227\n",
      "'집합금지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3712\n",
      "all branching entropies was computed # words = 112236\n",
      "all accessor variety was computed # words = 112236\n",
      "'이런사람'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3713\n",
      "all branching entropies was computed # words = 112257\n",
      "all accessor variety was computed # words = 112257\n",
      "'좋지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112263\n",
      "all accessor variety was computed # words = 112263\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112264\n",
      "all accessor variety was computed # words = 112264\n",
      "'구글트렌드'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112280\n",
      "all accessor variety was computed # words = 112280\n",
      "'성추행범'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112285\n",
      "all accessor variety was computed # words = 112285\n",
      "'여중딩'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112289\n",
      "all accessor variety was computed # words = 112289\n",
      "'선거법위반'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3714\n",
      "all branching entropies was computed # words = 112289\n",
      "all accessor variety was computed # words = 112289\n",
      "'섬짱깨'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112299\n",
      "all accessor variety was computed # words = 112299\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112301\n",
      "all accessor variety was computed # words = 112301\n",
      "'석천이형'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112306\n",
      "all accessor variety was computed # words = 112306\n",
      "'동암역'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112306\n",
      "all accessor variety was computed # words = 112306\n",
      "'잘생긴거'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112307\n",
      "all accessor variety was computed # words = 112307\n",
      "'타팬들'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112313\n",
      "all accessor variety was computed # words = 112313\n",
      "'어린이보호구역'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112313\n",
      "all accessor variety was computed # words = 112313\n",
      "'번확진자'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112313\n",
      "all accessor variety was computed # words = 112313\n",
      "'잘생긴남자'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3716\n",
      "all branching entropies was computed # words = 112315\n",
      "all accessor variety was computed # words = 112315\n",
      "'좆같지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3717\n",
      "all branching entropies was computed # words = 112343\n",
      "all accessor variety was computed # words = 112343\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3719\n",
      "all branching entropies was computed # words = 112384\n",
      "all accessor variety was computed # words = 112384\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3719\n",
      "all branching entropies was computed # words = 112388\n",
      "all accessor variety was computed # words = 112388\n",
      "'원정녀'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112431\n",
      "all accessor variety was computed # words = 112431\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112444\n",
      "all accessor variety was computed # words = 112444\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112454\n",
      "all accessor variety was computed # words = 112454\n",
      "'트랜스젠더'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112464\n",
      "all accessor variety was computed # words = 112464\n",
      "'쫄보'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112466\n",
      "all accessor variety was computed # words = 112466\n",
      "'좌파친구'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112467\n",
      "all accessor variety was computed # words = 112467\n",
      "'언론통제'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112473\n",
      "all accessor variety was computed # words = 112473\n",
      "'남교사'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 112477\n",
      "all accessor variety was computed # words = 112477\n",
      "'무도갤'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112477\n",
      "all accessor variety was computed # words = 112477\n",
      "'시간후'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112477\n",
      "all accessor variety was computed # words = 112477\n",
      "'레이디갓카'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112483\n",
      "all accessor variety was computed # words = 112483\n",
      "'하라보지'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112483\n",
      "all accessor variety was computed # words = 112483\n",
      "'중고딩들'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112488\n",
      "all accessor variety was computed # words = 112488\n",
      "'개오바'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112498\n",
      "all accessor variety was computed # words = 112498\n",
      "'나온거'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112508\n",
      "all accessor variety was computed # words = 112508\n",
      "'어린여자'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112509\n",
      "all accessor variety was computed # words = 112509\n",
      "'여자키'\n",
      "training was done. used memory 1.103 Gb1.074 Gb\n",
      "all cohesion probabilities was computed. # words = 3720\n",
      "all branching entropies was computed # words = 112512\n",
      "all accessor variety was computed # words = 112512\n",
      "'잊고'\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8526 from 1 sents. mem=1.061 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27760, mem=1.003 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2446 words\n",
      "[Noun Extractor] checked compounds. discovered 468 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1213 -> 1193\n",
      "[Noun Extractor] postprocessing ignore_features : 1193 -> 1170\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1170 -> 1165\n",
      "[Noun Extractor] 1165 nouns (468 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.995 Gb                    \n",
      "[Noun Extractor] 66.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 25683 from 1 sents. mem=0.987 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=65483, mem=0.987 Gb\n",
      "[Noun Extractor] batch prediction was completed for 12803 words\n",
      "[Noun Extractor] checked compounds. discovered 2935 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 6229 -> 5996\n",
      "[Noun Extractor] postprocessing ignore_features : 5996 -> 5931\n",
      "[Noun Extractor] postprocessing ignore_NJ : 5931 -> 5929\n",
      "[Noun Extractor] 5929 nouns (2935 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.987 Gb                    \n",
      "[Noun Extractor] 72.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7122 from 1 sents. mem=0.987 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14256, mem=0.970 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2049 words\n",
      "[Noun Extractor] checked compounds. discovered 245 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 864 -> 809\n",
      "[Noun Extractor] postprocessing ignore_features : 809 -> 781\n",
      "[Noun Extractor] postprocessing ignore_NJ : 781 -> 781\n",
      "[Noun Extractor] 781 nouns (245 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.967 Gb                    \n",
      "[Noun Extractor] 51.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5786 from 1 sents. mem=0.967 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13089, mem=0.965 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1671 words\n",
      "[Noun Extractor] checked compounds. discovered 99 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 563 -> 554\n",
      "[Noun Extractor] postprocessing ignore_features : 554 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (99 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.964 Gb                    \n",
      "[Noun Extractor] 47.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 30256 from 1 sents. mem=0.962 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=73836, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 11032 words\n",
      "[Noun Extractor] checked compounds. discovered 2555 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 5447 -> 5060\n",
      "[Noun Extractor] postprocessing ignore_features : 5060 -> 4993\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4993 -> 4989\n",
      "[Noun Extractor] 4989 nouns (2555 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 55.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11194 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=39393, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3066 words\n",
      "[Noun Extractor] checked compounds. discovered 710 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1557 -> 1528\n",
      "[Noun Extractor] postprocessing ignore_features : 1528 -> 1501\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1501 -> 1494\n",
      "[Noun Extractor] 1494 nouns (710 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 67.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2772 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5284, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 865 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 305\n",
      "[Noun Extractor] postprocessing ignore_features : 305 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2348 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4341, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 674 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 221\n",
      "[Noun Extractor] 221 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3237 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6634, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1056 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 379 -> 379\n",
      "[Noun Extractor] postprocessing ignore_features : 379 -> 363\n",
      "[Noun Extractor] postprocessing ignore_NJ : 363 -> 363\n",
      "[Noun Extractor] 363 nouns (64 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3480 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6911, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1059 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 368 -> 363\n",
      "[Noun Extractor] postprocessing ignore_features : 363 -> 354\n",
      "[Noun Extractor] postprocessing ignore_NJ : 354 -> 353\n",
      "[Noun Extractor] 353 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14048 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27950, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3977 words\n",
      "[Noun Extractor] checked compounds. discovered 633 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1847 -> 1787\n",
      "[Noun Extractor] postprocessing ignore_features : 1787 -> 1759\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1759 -> 1759\n",
      "[Noun Extractor] 1759 nouns (633 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3318 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6304, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1059 words\n",
      "[Noun Extractor] checked compounds. discovered 61 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 387 -> 379\n",
      "[Noun Extractor] postprocessing ignore_features : 379 -> 361\n",
      "[Noun Extractor] postprocessing ignore_NJ : 361 -> 361\n",
      "[Noun Extractor] 361 nouns (61 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5484 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11171, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1490 words\n",
      "[Noun Extractor] checked compounds. discovered 93 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 473 -> 469\n",
      "[Noun Extractor] postprocessing ignore_features : 469 -> 456\n",
      "[Noun Extractor] postprocessing ignore_NJ : 456 -> 456\n",
      "[Noun Extractor] 456 nouns (93 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2430 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4525, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 600 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 185\n",
      "[Noun Extractor] postprocessing ignore_features : 185 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7266 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14094, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2802 words\n",
      "[Noun Extractor] checked compounds. discovered 107 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 678 -> 666\n",
      "[Noun Extractor] postprocessing ignore_features : 666 -> 648\n",
      "[Noun Extractor] postprocessing ignore_NJ : 648 -> 647\n",
      "[Noun Extractor] 647 nouns (107 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1452 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2594, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 150 -> 148\n",
      "[Noun Extractor] postprocessing ignore_features : 148 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1297 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2255, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4693 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8084, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1476 words\n",
      "[Noun Extractor] checked compounds. discovered 82 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 512 -> 506\n",
      "[Noun Extractor] postprocessing ignore_features : 506 -> 491\n",
      "[Noun Extractor] postprocessing ignore_NJ : 491 -> 490\n",
      "[Noun Extractor] 490 nouns (82 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7620 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16636, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2070 words\n",
      "[Noun Extractor] checked compounds. discovered 211 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 808 -> 803\n",
      "[Noun Extractor] postprocessing ignore_features : 803 -> 784\n",
      "[Noun Extractor] postprocessing ignore_NJ : 784 -> 784\n",
      "[Noun Extractor] 784 nouns (211 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7367 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12735, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2849 words\n",
      "[Noun Extractor] checked compounds. discovered 205 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 859 -> 855\n",
      "[Noun Extractor] postprocessing ignore_features : 855 -> 838\n",
      "[Noun Extractor] postprocessing ignore_NJ : 838 -> 836\n",
      "[Noun Extractor] 836 nouns (205 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2298 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 658 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 242 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 229\n",
      "[Noun Extractor] 229 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5278 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10607, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1498 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 565 -> 550\n",
      "[Noun Extractor] postprocessing ignore_features : 550 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1107 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1904, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 380 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1080 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1940, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 115\n",
      "[Noun Extractor] postprocessing ignore_features : 115 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10143 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27309, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2588 words\n",
      "[Noun Extractor] checked compounds. discovered 248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1044 -> 1035\n",
      "[Noun Extractor] postprocessing ignore_features : 1035 -> 1011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1011 -> 1011\n",
      "[Noun Extractor] 1011 nouns (248 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 961 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1665, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 296 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3591 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6698, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1170 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 443 -> 436\n",
      "[Noun Extractor] postprocessing ignore_features : 436 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2119, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 141\n",
      "[Noun Extractor] postprocessing ignore_NJ : 141 -> 141\n",
      "[Noun Extractor] 141 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6533 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10913, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2429 words\n",
      "[Noun Extractor] checked compounds. discovered 199 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 784 -> 775\n",
      "[Noun Extractor] postprocessing ignore_features : 775 -> 749\n",
      "[Noun Extractor] postprocessing ignore_NJ : 749 -> 747\n",
      "[Noun Extractor] 747 nouns (199 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2628 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4504, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1465 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 291 -> 288\n",
      "[Noun Extractor] postprocessing ignore_features : 288 -> 269\n",
      "[Noun Extractor] postprocessing ignore_NJ : 269 -> 269\n",
      "[Noun Extractor] 269 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2101 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3806, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 652 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 235 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 208\n",
      "[Noun Extractor] postprocessing ignore_NJ : 208 -> 208\n",
      "[Noun Extractor] 208 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2018, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 385 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.51 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 711 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1235, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1737 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3279, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 504 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 165 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 151\n",
      "[Noun Extractor] 151 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1524 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2434, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 540 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 148 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2065, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4747 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8668, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1541 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 480 -> 468\n",
      "[Noun Extractor] postprocessing ignore_features : 468 -> 452\n",
      "[Noun Extractor] postprocessing ignore_NJ : 452 -> 452\n",
      "[Noun Extractor] 452 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3401 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6749, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1048 words\n",
      "[Noun Extractor] checked compounds. discovered 96 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 435 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 417\n",
      "[Noun Extractor] postprocessing ignore_NJ : 417 -> 417\n",
      "[Noun Extractor] 417 nouns (96 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 58.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1437 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2403, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 537 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5727 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12231, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1820 words\n",
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 631\n",
      "[Noun Extractor] postprocessing ignore_features : 631 -> 618\n",
      "[Noun Extractor] postprocessing ignore_NJ : 618 -> 618\n",
      "[Noun Extractor] 618 nouns (158 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 53.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2920 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5984, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 910 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 327 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 314\n",
      "[Noun Extractor] postprocessing ignore_NJ : 314 -> 312\n",
      "[Noun Extractor] 312 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3506 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6208, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1202 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 397 -> 369\n",
      "[Noun Extractor] postprocessing ignore_features : 369 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3182 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5937, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 964 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 328 -> 322\n",
      "[Noun Extractor] postprocessing ignore_features : 322 -> 306\n",
      "[Noun Extractor] postprocessing ignore_NJ : 306 -> 305\n",
      "[Noun Extractor] 305 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1529, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1093 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1898, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 960 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1592, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 915 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1521, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 627 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1035, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1333 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2357, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 453 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 114\n",
      "[Noun Extractor] postprocessing ignore_NJ : 114 -> 114\n",
      "[Noun Extractor] 114 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9338 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=17634, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3911 words\n",
      "[Noun Extractor] checked compounds. discovered 467 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1146 -> 1120\n",
      "[Noun Extractor] postprocessing ignore_features : 1120 -> 1093\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1093 -> 1093\n",
      "[Noun Extractor] 1093 nouns (467 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1025 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1731, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9858 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20851, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3335 words\n",
      "[Noun Extractor] checked compounds. discovered 567 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1530 -> 1345\n",
      "[Noun Extractor] postprocessing ignore_features : 1345 -> 1311\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1311 -> 1306\n",
      "[Noun Extractor] 1306 nouns (567 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 58.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 861 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1740, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2911 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4942, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1289 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 302 -> 301\n",
      "[Noun Extractor] postprocessing ignore_features : 301 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6695 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12515, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2134 words\n",
      "[Noun Extractor] checked compounds. discovered 260 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 779 -> 691\n",
      "[Noun Extractor] postprocessing ignore_features : 691 -> 666\n",
      "[Noun Extractor] postprocessing ignore_NJ : 666 -> 666\n",
      "[Noun Extractor] 666 nouns (260 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2058 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3747, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 695 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 215\n",
      "[Noun Extractor] postprocessing ignore_NJ : 215 -> 215\n",
      "[Noun Extractor] 215 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1143 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2132, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 324 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1312 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2387, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1564 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2776, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 169\n",
      "[Noun Extractor] postprocessing ignore_features : 169 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 160\n",
      "[Noun Extractor] 160 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4415 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8218, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1286 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 370 -> 366\n",
      "[Noun Extractor] postprocessing ignore_features : 366 -> 356\n",
      "[Noun Extractor] postprocessing ignore_NJ : 356 -> 356\n",
      "[Noun Extractor] 356 nouns (32 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 23.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1927 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3650, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 561 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 181 -> 179\n",
      "[Noun Extractor] postprocessing ignore_features : 179 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1043 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1937, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5038 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10506, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1403 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 485 -> 482\n",
      "[Noun Extractor] postprocessing ignore_features : 482 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 470\n",
      "[Noun Extractor] 470 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1399 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2424, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 512 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 737 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1236, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 65\n",
      "[Noun Extractor] 65 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 407 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=815, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2841, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=993, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1357 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2475, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 442 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 840 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1198, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1048 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1771, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1865 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3259, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 583 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 144\n",
      "[Noun Extractor] postprocessing ignore_NJ : 144 -> 144\n",
      "[Noun Extractor] 144 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 659 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1024, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 775 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1285, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1143, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2281, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 427 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 134\n",
      "[Noun Extractor] 134 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 952 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1457, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4988 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9140, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1496 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 412 -> 403\n",
      "[Noun Extractor] postprocessing ignore_features : 403 -> 384\n",
      "[Noun Extractor] postprocessing ignore_NJ : 384 -> 384\n",
      "[Noun Extractor] 384 nouns (44 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 23.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1824, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 387 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6448 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12676, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2077 words\n",
      "[Noun Extractor] checked compounds. discovered 81 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 637 -> 628\n",
      "[Noun Extractor] postprocessing ignore_features : 628 -> 603\n",
      "[Noun Extractor] postprocessing ignore_NJ : 603 -> 602\n",
      "[Noun Extractor] 602 nouns (81 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 540 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=897, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1158 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2085, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 348 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 110\n",
      "[Noun Extractor] 110 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 414 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=706, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1282 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2048, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 128 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1292, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5640 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10126, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2107 words\n",
      "[Noun Extractor] checked compounds. discovered 111 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 553 -> 545\n",
      "[Noun Extractor] postprocessing ignore_features : 545 -> 525\n",
      "[Noun Extractor] postprocessing ignore_NJ : 525 -> 525\n",
      "[Noun Extractor] 525 nouns (111 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 998 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1518, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4578, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 802 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 303 -> 296\n",
      "[Noun Extractor] postprocessing ignore_features : 296 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 282\n",
      "[Noun Extractor] 282 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1316 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2414, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1092 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2000, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 76\n",
      "[Noun Extractor] 76 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 789 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1214, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=949, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 639 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1076, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2116 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3757, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 702 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 219 -> 219\n",
      "[Noun Extractor] postprocessing ignore_features : 219 -> 212\n",
      "[Noun Extractor] postprocessing ignore_NJ : 212 -> 211\n",
      "[Noun Extractor] 211 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 891 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1467, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1033 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1537, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 719 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1029, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2730, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 341 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2435 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4039, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 789 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 228\n",
      "[Noun Extractor] postprocessing ignore_NJ : 228 -> 227\n",
      "[Noun Extractor] 227 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 815 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1499, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1886 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3428, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 587 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 211 -> 207\n",
      "[Noun Extractor] postprocessing ignore_features : 207 -> 193\n",
      "[Noun Extractor] postprocessing ignore_NJ : 193 -> 193\n",
      "[Noun Extractor] 193 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1608 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2632, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 448 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (9 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4991 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8667, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1572 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 417 -> 412\n",
      "[Noun Extractor] postprocessing ignore_features : 412 -> 394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 394 -> 394\n",
      "[Noun Extractor] 394 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1200 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2123, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 404 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=733, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 100 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 681 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1161, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1073, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 210 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1796, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 853 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1327, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 273 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1569 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2518, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 488 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=941, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 610 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=953, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 153 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1453 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2367, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 515 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1987 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3336, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 762 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 184\n",
      "[Noun Extractor] postprocessing ignore_features : 184 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 173 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4321 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8200, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1695 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 454 -> 454\n",
      "[Noun Extractor] postprocessing ignore_features : 454 -> 440\n",
      "[Noun Extractor] postprocessing ignore_NJ : 440 -> 440\n",
      "[Noun Extractor] 440 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2270, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 940 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 347 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2410, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 716 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 569 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7179 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14143, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2397 words\n",
      "[Noun Extractor] checked compounds. discovered 397 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1056 -> 1031\n",
      "[Noun Extractor] postprocessing ignore_features : 1031 -> 1008\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1008 -> 1000\n",
      "[Noun Extractor] 1000 nouns (397 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 58.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4323 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8442, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1552 words\n",
      "[Noun Extractor] checked compounds. discovered 86 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 499 -> 486\n",
      "[Noun Extractor] postprocessing ignore_features : 486 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 469\n",
      "[Noun Extractor] 469 nouns (86 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1986 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3945, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 645 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 230 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 220\n",
      "[Noun Extractor] 220 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1658 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2790, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 489 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 133\n",
      "[Noun Extractor] 133 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5238 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12604, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1419 words\n",
      "[Noun Extractor] checked compounds. discovered 132 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 575 -> 563\n",
      "[Noun Extractor] postprocessing ignore_features : 563 -> 542\n",
      "[Noun Extractor] postprocessing ignore_NJ : 542 -> 542\n",
      "[Noun Extractor] 542 nouns (132 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=840, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 364 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=572, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 501 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=727, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4938 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9814, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1722 words\n",
      "[Noun Extractor] checked compounds. discovered 161 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 651 -> 582\n",
      "[Noun Extractor] postprocessing ignore_features : 582 -> 566\n",
      "[Noun Extractor] postprocessing ignore_NJ : 566 -> 565\n",
      "[Noun Extractor] 565 nouns (161 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 53.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1907, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1062 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1632, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 331 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1259, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1041 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1829, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1610 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2739, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 544 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 191\n",
      "[Noun Extractor] postprocessing ignore_features : 191 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1354 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2282, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=667, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 494 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=855, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1620, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1147 from 1 sents. mem=0.961 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2039, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 354 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 594 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2864 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5158, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1065 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 293 -> 285\n",
      "[Noun Extractor] postprocessing ignore_features : 285 -> 270\n",
      "[Noun Extractor] postprocessing ignore_NJ : 270 -> 270\n",
      "[Noun Extractor] 270 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1206, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1161 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1889, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=900, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1133 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1965, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 552 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=831, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 660 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1002, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2616 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4803, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 776 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 238 -> 236\n",
      "[Noun Extractor] postprocessing ignore_features : 236 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 672 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1017, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 308 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=496, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 87 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 339 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=530, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 115 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 935 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1552, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 644 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1002, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 462 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1365, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 82 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=692, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 151 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1905 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3962, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 208\n",
      "[Noun Extractor] postprocessing ignore_features : 208 -> 197\n",
      "[Noun Extractor] postprocessing ignore_NJ : 197 -> 197\n",
      "[Noun Extractor] 197 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 418 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=630, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 130 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 344 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=519, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 99 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 433 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=683, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 126 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=848, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1189 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2420, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 407 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 110 -> 109\n",
      "[Noun Extractor] postprocessing ignore_features : 109 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 103\n",
      "[Noun Extractor] 103 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 853 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1496, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3709 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7036, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1215 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 430 -> 427\n",
      "[Noun Extractor] postprocessing ignore_features : 427 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 301 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=500, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 86 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1477, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 317 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 642 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1093, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 509 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=777, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 152 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 665 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1067, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3815 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7830, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1295 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 488 -> 447\n",
      "[Noun Extractor] postprocessing ignore_features : 447 -> 429\n",
      "[Noun Extractor] postprocessing ignore_NJ : 429 -> 425\n",
      "[Noun Extractor] 425 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1870 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3267, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 474 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 341 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=502, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 110 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3311 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5875, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1223 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 353 -> 338\n",
      "[Noun Extractor] postprocessing ignore_features : 338 -> 326\n",
      "[Noun Extractor] postprocessing ignore_NJ : 326 -> 326\n",
      "[Noun Extractor] 326 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 677 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1089, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 217 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 623 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1030, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 507 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=996, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1435 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2444, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 392 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=584, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 111 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1340 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2322, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2150 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3620, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 639 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 175 -> 171\n",
      "[Noun Extractor] postprocessing ignore_features : 171 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 163\n",
      "[Noun Extractor] 163 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 429 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=633, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 704 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1188, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1965 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3424, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 653 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 200 -> 199\n",
      "[Noun Extractor] postprocessing ignore_features : 199 -> 192\n",
      "[Noun Extractor] postprocessing ignore_NJ : 192 -> 192\n",
      "[Noun Extractor] 192 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1219 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1971, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 391 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 347 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=508, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 106 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1462 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2357, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 708 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 158\n",
      "[Noun Extractor] postprocessing ignore_features : 158 -> 149\n",
      "[Noun Extractor] postprocessing ignore_NJ : 149 -> 149\n",
      "[Noun Extractor] 149 nouns (34 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 527 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=739, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 528 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=759, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 144 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1167 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2306, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 476 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 130 -> 130\n",
      "[Noun Extractor] postprocessing ignore_features : 130 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1918 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3982, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 598 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 140\n",
      "[Noun Extractor] postprocessing ignore_NJ : 140 -> 140\n",
      "[Noun Extractor] 140 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1038 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1482, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1030 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1564, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 72\n",
      "[Noun Extractor] 72 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 906 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1600, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 245 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 462 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=931, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 483 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=830, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 133 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 524 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=710, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 224 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 956 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1332, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 305 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2147, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1091 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2289, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 446 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2581 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4731, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 649 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 174\n",
      "[Noun Extractor] 174 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 396 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=591, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2016 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3467, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 683 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 195 -> 194\n",
      "[Noun Extractor] postprocessing ignore_features : 194 -> 181\n",
      "[Noun Extractor] postprocessing ignore_NJ : 181 -> 181\n",
      "[Noun Extractor] 181 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 734 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1172, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=958, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 415 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=630, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 118 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 920 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1454, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 753 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1259, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2508 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4128, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 880 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 287 -> 286\n",
      "[Noun Extractor] postprocessing ignore_features : 286 -> 268\n",
      "[Noun Extractor] postprocessing ignore_NJ : 268 -> 268\n",
      "[Noun Extractor] 268 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 988 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1779, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 366 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1254 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2740, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 397 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 96\n",
      "[Noun Extractor] 96 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 472 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=726, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 112 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1032 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1725, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 315 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 713 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1014, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 62\n",
      "[Noun Extractor] 62 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 608 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1050, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 884 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1301, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 697 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1224, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1800 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4594, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 505 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 438 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=620, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 936 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1487, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 270 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 504 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=724, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 176 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1099 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2399, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 483 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9713 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21610, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2853 words\n",
      "[Noun Extractor] checked compounds. discovered 644 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1583 -> 1403\n",
      "[Noun Extractor] postprocessing ignore_features : 1403 -> 1367\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1367 -> 1360\n",
      "[Noun Extractor] 1360 nouns (644 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 61.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1079 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1715, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (10 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1524 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2255, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 488 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 129 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1134 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1666, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 327 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 748 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1222, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 242 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 695 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1465, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 256 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1027, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1874 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3930, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 646 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 202 -> 198\n",
      "[Noun Extractor] postprocessing ignore_features : 198 -> 185\n",
      "[Noun Extractor] postprocessing ignore_NJ : 185 -> 185\n",
      "[Noun Extractor] 185 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 455 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=843, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 53.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 687 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1079, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2964, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 597 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 578 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=924, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1991 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3368, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 618 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 192\n",
      "[Noun Extractor] postprocessing ignore_features : 192 -> 178\n",
      "[Noun Extractor] postprocessing ignore_NJ : 178 -> 178\n",
      "[Noun Extractor] 178 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 537 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=813, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4137 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8282, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1504 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 441 -> 437\n",
      "[Noun Extractor] postprocessing ignore_features : 437 -> 424\n",
      "[Noun Extractor] postprocessing ignore_NJ : 424 -> 423\n",
      "[Noun Extractor] 423 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 667 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1021, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1392 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2288, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 440 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 103 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6882 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13766, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1976 words\n",
      "[Noun Extractor] checked compounds. discovered 213 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 748 -> 731\n",
      "[Noun Extractor] postprocessing ignore_features : 731 -> 709\n",
      "[Noun Extractor] postprocessing ignore_NJ : 709 -> 708\n",
      "[Noun Extractor] 708 nouns (213 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 584 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=910, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 805 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1904, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 197 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 204 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=885, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 7 -> 7\n",
      "[Noun Extractor] postprocessing ignore_features : 7 -> 7\n",
      "[Noun Extractor] postprocessing ignore_NJ : 7 -> 7\n",
      "[Noun Extractor] 7 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2779 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5223, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 883 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 257 -> 256\n",
      "[Noun Extractor] postprocessing ignore_features : 256 -> 248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 248 -> 247\n",
      "[Noun Extractor] 247 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 381 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=573, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 138 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 605 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1687, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1106 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1712, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 368 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2979 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5350, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1163 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 265 -> 264\n",
      "[Noun Extractor] postprocessing ignore_features : 264 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 257\n",
      "[Noun Extractor] 257 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1477 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2973, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 333 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 839 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1376, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 306 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2033 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2885, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 762 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 187\n",
      "[Noun Extractor] postprocessing ignore_features : 187 -> 178\n",
      "[Noun Extractor] postprocessing ignore_NJ : 178 -> 178\n",
      "[Noun Extractor] 178 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2088 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3298, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 607 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 168\n",
      "[Noun Extractor] postprocessing ignore_NJ : 168 -> 168\n",
      "[Noun Extractor] 168 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 511 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=758, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1109 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1864, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 283 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=882, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 195 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 472 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=771, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 869 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1322, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4912 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11157, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1521 words\n",
      "[Noun Extractor] checked compounds. discovered 117 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 587 -> 569\n",
      "[Noun Extractor] postprocessing ignore_features : 569 -> 543\n",
      "[Noun Extractor] postprocessing ignore_NJ : 543 -> 542\n",
      "[Noun Extractor] 542 nouns (117 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 895 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1409, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 724 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1117, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 246 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 461 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=729, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4340 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7030, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1474 words\n",
      "[Noun Extractor] checked compounds. discovered 109 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 530 -> 525\n",
      "[Noun Extractor] postprocessing ignore_features : 525 -> 513\n",
      "[Noun Extractor] postprocessing ignore_NJ : 513 -> 511\n",
      "[Noun Extractor] 511 nouns (109 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1434 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2380, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 525 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 922 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1757, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 291 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1946 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2993, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 663 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 180 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1915 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4153, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 639 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 187\n",
      "[Noun Extractor] postprocessing ignore_features : 187 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 376 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=547, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 115 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=586, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 121 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5994 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10816, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2244 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 621 -> 614\n",
      "[Noun Extractor] postprocessing ignore_features : 614 -> 600\n",
      "[Noun Extractor] postprocessing ignore_NJ : 600 -> 600\n",
      "[Noun Extractor] 600 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 458 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=641, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 150 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 490 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=748, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 151 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2514, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1134 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1811, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1603 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2813, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 440 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 736 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1224, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 894 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1494, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 256 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 723 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1103, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1962 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3256, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 719 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 635 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1032, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1074 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1921, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 463 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=895, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 136 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=696, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 800 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1435, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 158 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 626 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1007, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 536 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=921, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1963 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3098, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 658 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 169 -> 167\n",
      "[Noun Extractor] postprocessing ignore_features : 167 -> 155\n",
      "[Noun Extractor] postprocessing ignore_NJ : 155 -> 155\n",
      "[Noun Extractor] 155 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8371 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16218, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2851 words\n",
      "[Noun Extractor] checked compounds. discovered 276 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1087 -> 1014\n",
      "[Noun Extractor] postprocessing ignore_features : 1014 -> 983\n",
      "[Noun Extractor] postprocessing ignore_NJ : 983 -> 981\n",
      "[Noun Extractor] 981 nouns (276 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 556 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=826, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 6.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 413 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=620, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 136 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1627 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2677, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 599 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 117\n",
      "[Noun Extractor] 117 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3646 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5808, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1264 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 359 -> 354\n",
      "[Noun Extractor] postprocessing ignore_features : 354 -> 340\n",
      "[Noun Extractor] postprocessing ignore_NJ : 340 -> 340\n",
      "[Noun Extractor] 340 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1653 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2582, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 613 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 122\n",
      "[Noun Extractor] postprocessing ignore_features : 122 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1144 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1958, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 682 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1107, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (14 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1332 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1891, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 471 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 149 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 356 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=669, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 93 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1006 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1531, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 60\n",
      "[Noun Extractor] 60 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2739 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5114, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 849 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 263\n",
      "[Noun Extractor] postprocessing ignore_features : 263 -> 252\n",
      "[Noun Extractor] postprocessing ignore_NJ : 252 -> 252\n",
      "[Noun Extractor] 252 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=611, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 127 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 347 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 100 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1077 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1844, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 412 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1342 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2310, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 533 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=901, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 751 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1181, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 991 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1476, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 293 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 63\n",
      "[Noun Extractor] 63 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 558 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=867, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 173 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1367 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2681, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 154\n",
      "[Noun Extractor] postprocessing ignore_features : 154 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 57.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1439 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2148, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 527 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 129 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 422 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=744, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 140 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 846 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1358, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 290 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 820 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1181, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1060 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1585, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 336 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 90\n",
      "[Noun Extractor] 90 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1875 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3130, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 188 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2294, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 123 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 418 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 124 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 872 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1307, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 303 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 70\n",
      "[Noun Extractor] 70 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=816, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 650 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=917, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1907 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3371, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 720 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 231 -> 203\n",
      "[Noun Extractor] postprocessing ignore_features : 203 -> 186\n",
      "[Noun Extractor] postprocessing ignore_NJ : 186 -> 186\n",
      "[Noun Extractor] 186 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 393 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=657, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 122 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 26\n",
      "[Noun Extractor] 26 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 646 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=999, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 407 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=611, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 133 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1193, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1200 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1985, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 387 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 817 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1323, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 716 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1122, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 438 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=820, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 182 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 477 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=750, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1618 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2242, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 710 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 141 -> 141\n",
      "[Noun Extractor] postprocessing ignore_features : 141 -> 135\n",
      "[Noun Extractor] postprocessing ignore_NJ : 135 -> 135\n",
      "[Noun Extractor] 135 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 692 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=971, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 197 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1522 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2143, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 552 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 122\n",
      "[Noun Extractor] postprocessing ignore_features : 122 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 392 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=765, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 159 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 53.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2038 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3193, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 684 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 175 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 527 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 144 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 413 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=625, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 113 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 477 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=634, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3139 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5794, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 348 -> 345\n",
      "[Noun Extractor] postprocessing ignore_features : 345 -> 331\n",
      "[Noun Extractor] postprocessing ignore_NJ : 331 -> 331\n",
      "[Noun Extractor] 331 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 461 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=679, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 713 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1101, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 243 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2523 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4566, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 770 words\n",
      "[Noun Extractor] checked compounds. discovered 60 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 252\n",
      "[Noun Extractor] postprocessing ignore_features : 252 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (60 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 554 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=799, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 188 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1534 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2756, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (41 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 358 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=542, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2112, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 438 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 682 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=973, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 370 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=653, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 298 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=535, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 101 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1047 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1492, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 849 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1314, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 448 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=655, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 908 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1571, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 241 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1363 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2195, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 429 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 122\n",
      "[Noun Extractor] 122 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 925 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1931 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3512, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 632 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (34 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2360 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3895, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 805 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 243 -> 231\n",
      "[Noun Extractor] postprocessing ignore_features : 231 -> 215\n",
      "[Noun Extractor] postprocessing ignore_NJ : 215 -> 214\n",
      "[Noun Extractor] 214 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 25.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3311 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5009, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1221 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 275 -> 267\n",
      "[Noun Extractor] postprocessing ignore_features : 267 -> 248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 248 -> 248\n",
      "[Noun Extractor] 248 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 21.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 917 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1383, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 832 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1239, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 890 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1364, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 333 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 820 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1134, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 457 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=692, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 176 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 359 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=566, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 111 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2822 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5142, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1042 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 332 -> 311\n",
      "[Noun Extractor] postprocessing ignore_features : 311 -> 293\n",
      "[Noun Extractor] postprocessing ignore_NJ : 293 -> 292\n",
      "[Noun Extractor] 292 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 651 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=943, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 582 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1066, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 160 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1126 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1928, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 394 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (13 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 281 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=409, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 86 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1968, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 915 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1425, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 293 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 59\n",
      "[Noun Extractor] 59 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2628 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3852, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 878 words\n",
      "[Noun Extractor] checked compounds. discovered 63 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 298 -> 296\n",
      "[Noun Extractor] postprocessing ignore_features : 296 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (63 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1083 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1865, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 294 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 623 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1194, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7420 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12810, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2466 words\n",
      "[Noun Extractor] checked compounds. discovered 138 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 792 -> 747\n",
      "[Noun Extractor] postprocessing ignore_features : 747 -> 722\n",
      "[Noun Extractor] postprocessing ignore_NJ : 722 -> 722\n",
      "[Noun Extractor] 722 nouns (138 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6604 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12873, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1783 words\n",
      "[Noun Extractor] checked compounds. discovered 131 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 669 -> 644\n",
      "[Noun Extractor] postprocessing ignore_features : 644 -> 619\n",
      "[Noun Extractor] postprocessing ignore_NJ : 619 -> 618\n",
      "[Noun Extractor] 618 nouns (131 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3217 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5608, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1134 words\n",
      "[Noun Extractor] checked compounds. discovered 113 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 395 -> 385\n",
      "[Noun Extractor] postprocessing ignore_features : 385 -> 368\n",
      "[Noun Extractor] postprocessing ignore_NJ : 368 -> 368\n",
      "[Noun Extractor] 368 nouns (113 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 829 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1309, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 261 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 759 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1184, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 970 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1493, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 472 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=773, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 109 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 893 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1379, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 331 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2648, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 443 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=601, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 150 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1350 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2141, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 536 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 122\n",
      "[Noun Extractor] postprocessing ignore_NJ : 122 -> 122\n",
      "[Noun Extractor] 122 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2399 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3832, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 778 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 247 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 238\n",
      "[Noun Extractor] postprocessing ignore_NJ : 238 -> 238\n",
      "[Noun Extractor] 238 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3635 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7104, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1061 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 340 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 309\n",
      "[Noun Extractor] postprocessing ignore_NJ : 309 -> 309\n",
      "[Noun Extractor] 309 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1114 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1658, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 407 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 77\n",
      "[Noun Extractor] 77 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 775 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1142, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 10.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1427 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2310, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 535 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 144 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 136\n",
      "[Noun Extractor] postprocessing ignore_NJ : 136 -> 136\n",
      "[Noun Extractor] 136 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2077 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2908, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 685 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 205 -> 202\n",
      "[Noun Extractor] postprocessing ignore_features : 202 -> 187\n",
      "[Noun Extractor] postprocessing ignore_NJ : 187 -> 187\n",
      "[Noun Extractor] 187 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2235 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3618, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 892 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 209 -> 207\n",
      "[Noun Extractor] postprocessing ignore_features : 207 -> 190\n",
      "[Noun Extractor] postprocessing ignore_NJ : 190 -> 190\n",
      "[Noun Extractor] 190 nouns (15 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 596 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=877, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 571 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=982, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 964 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1489, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 414 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 468 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=674, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1118 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2022, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=790, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 191 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 425 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=653, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 135 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 837 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1222, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1133, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1024 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1569, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 365 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 88\n",
      "[Noun Extractor] 88 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 686 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1093, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1947 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3436, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 682 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 196 -> 185\n",
      "[Noun Extractor] postprocessing ignore_NJ : 185 -> 185\n",
      "[Noun Extractor] 185 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 376 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=597, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 423 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=631, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 129 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1174 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2000, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 437 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1222 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1827, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 584 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 540 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=791, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=641, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 617 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=961, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 166 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1149 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1751, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 359 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 771 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1151, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1702 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2780, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 600 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 568 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=776, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1400 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2283, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 541 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1053, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 167 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1485 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2690, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 782 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1185, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 282 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2126 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3724, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 804 words\n",
      "[Noun Extractor] checked compounds. discovered 46 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 245 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 216\n",
      "[Noun Extractor] 216 nouns (46 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 520 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=780, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 581 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1001, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 902 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1412, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1345 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2185, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 519 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2551 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4678, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 737 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 225 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 207\n",
      "[Noun Extractor] 207 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1713 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2620, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 570 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 135 -> 135\n",
      "[Noun Extractor] postprocessing ignore_features : 135 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2296 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4018, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 705 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 209\n",
      "[Noun Extractor] postprocessing ignore_features : 209 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1513 from 1 sents. mem=0.961 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2371, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 489 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 94\n",
      "[Noun Extractor] postprocessing ignore_NJ : 94 -> 94\n",
      "[Noun Extractor] 94 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1274, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 311 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 941 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1434, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 360 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=831, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 130 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2358 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 708 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 221 -> 221\n",
      "[Noun Extractor] postprocessing ignore_features : 221 -> 213\n",
      "[Noun Extractor] postprocessing ignore_NJ : 213 -> 213\n",
      "[Noun Extractor] 213 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1047 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1487, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 85\n",
      "[Noun Extractor] postprocessing ignore_features : 85 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 79\n",
      "[Noun Extractor] 79 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 787 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1068, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 52\n",
      "[Noun Extractor] 52 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1691 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2573, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 617 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 173 -> 171\n",
      "[Noun Extractor] postprocessing ignore_features : 171 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 162\n",
      "[Noun Extractor] 162 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1288 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2234, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 371 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 522 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=850, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1332 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2140, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 413 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 100\n",
      "[Noun Extractor] postprocessing ignore_features : 100 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 562 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=898, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 591 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=894, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 252 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 796 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1164, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 399 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=676, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 104 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 293 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=575, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 70 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 15 -> 15\n",
      "[Noun Extractor] postprocessing ignore_features : 15 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1142 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2022, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4042 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8425, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1248 words\n",
      "[Noun Extractor] checked compounds. discovered 116 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 441 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 410\n",
      "[Noun Extractor] postprocessing ignore_NJ : 410 -> 409\n",
      "[Noun Extractor] 409 nouns (116 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 55.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1756 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2861, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 539 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 748 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1108, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 257 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 471 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=720, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 542 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=809, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1434 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2415, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 430 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1396 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2096, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 502 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 123 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (19 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1383, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1089 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1845, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 346 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 632 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=924, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 190 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 645 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=982, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 162 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1603, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 501 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=603, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 940 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1715, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 338 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 380 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=601, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 108 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3611 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5885, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1085 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 295 -> 292\n",
      "[Noun Extractor] postprocessing ignore_features : 292 -> 277\n",
      "[Noun Extractor] postprocessing ignore_NJ : 277 -> 277\n",
      "[Noun Extractor] 277 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1411 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2034, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 399 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1066, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 734 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1161, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 352 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=507, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 110 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2482 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4023, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1073 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 194 -> 194\n",
      "[Noun Extractor] postprocessing ignore_features : 194 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1512 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2352, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 522 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1139 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1942, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 548 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=767, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 202 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1853 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 631 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 162\n",
      "[Noun Extractor] 162 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1157 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1788, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 356 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1221, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 73\n",
      "[Noun Extractor] 73 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 795 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1050, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 270 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=958, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 365 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=544, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 33\n",
      "[Noun Extractor] 33 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 694 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=993, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 301 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing detaching_features : 79 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 515 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=808, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 394 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=613, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 787 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1154, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 543 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=824, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 542 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=870, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=727, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2730 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4452, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1043 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 279 -> 279\n",
      "[Noun Extractor] postprocessing ignore_features : 279 -> 271\n",
      "[Noun Extractor] postprocessing ignore_NJ : 271 -> 271\n",
      "[Noun Extractor] 271 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 389 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=607, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 96 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 724 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1043, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 301 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 677 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1038, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1487 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2243, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 533 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 175 -> 172\n",
      "[Noun Extractor] postprocessing ignore_features : 172 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 159\n",
      "[Noun Extractor] 159 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 478 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=681, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 495 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 402 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=670, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 724 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1100, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1159 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1890, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 479 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 473 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=708, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1329 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1690, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 484 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 122\n",
      "[Noun Extractor] postprocessing ignore_features : 122 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1834 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2966, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 779 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 169 -> 168\n",
      "[Noun Extractor] postprocessing ignore_features : 168 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1001 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1348, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 385 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 244 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=554, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 94 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 429 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=682, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 126 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 464 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=707, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 154 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 339 from 1 sents. mem=0.961 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=548, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 355 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=559, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 115 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 15 -> 15\n",
      "[Noun Extractor] postprocessing ignore_features : 15 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 360 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=577, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 120 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1413 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2310, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 476 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 136\n",
      "[Noun Extractor] postprocessing ignore_NJ : 136 -> 136\n",
      "[Noun Extractor] 136 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6659 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11505, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2168 words\n",
      "[Noun Extractor] checked compounds. discovered 198 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 707 -> 686\n",
      "[Noun Extractor] postprocessing ignore_features : 686 -> 663\n",
      "[Noun Extractor] postprocessing ignore_NJ : 663 -> 662\n",
      "[Noun Extractor] 662 nouns (198 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2830 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5334, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 890 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 271 -> 269\n",
      "[Noun Extractor] postprocessing ignore_features : 269 -> 253\n",
      "[Noun Extractor] postprocessing ignore_NJ : 253 -> 253\n",
      "[Noun Extractor] 253 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 583 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=882, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1219 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1818, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5164 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8252, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1807 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 474 -> 473\n",
      "[Noun Extractor] postprocessing ignore_features : 473 -> 458\n",
      "[Noun Extractor] postprocessing ignore_NJ : 458 -> 458\n",
      "[Noun Extractor] 458 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 24.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 891 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1200, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 537 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=873, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 162 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2316 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3770, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 750 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 231 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 214\n",
      "[Noun Extractor] postprocessing ignore_NJ : 214 -> 214\n",
      "[Noun Extractor] 214 nouns (26 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 938 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1428, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2619 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5074, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 759 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 236 -> 234\n",
      "[Noun Extractor] postprocessing ignore_features : 234 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 495 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=702, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 743 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1203, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1454, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 339 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 573 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=885, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 306 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=450, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 90 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 644 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1028, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 188 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 959 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1482, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 407 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=629, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 131 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 391 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=632, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 246 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1178 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2064, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 365 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 9.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 368 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=667, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 111 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 780 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1202, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2065 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3340, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 661 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 156 -> 156\n",
      "[Noun Extractor] postprocessing ignore_features : 156 -> 144\n",
      "[Noun Extractor] postprocessing ignore_NJ : 144 -> 144\n",
      "[Noun Extractor] 144 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1091 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1642, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 389 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 651 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1063, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 190 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 672 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=986, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 272 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1282 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1777, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 414 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1005 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1549, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 493 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=640, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 156 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1205 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2383, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 700 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1090, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 736 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1067, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 216 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 406 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=681, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 116 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1125 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1940, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 334 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 542 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=797, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 156 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3843 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7174, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1305 words\n",
      "[Noun Extractor] checked compounds. discovered 134 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 434 -> 423\n",
      "[Noun Extractor] postprocessing ignore_features : 423 -> 412\n",
      "[Noun Extractor] postprocessing ignore_NJ : 412 -> 412\n",
      "[Noun Extractor] 412 nouns (134 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=543, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 125 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 381 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=573, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 117 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 388 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=625, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 118 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 982 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1610, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 291 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1041 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1527, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 458 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=644, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 163 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1460, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 740 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1081, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 273 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1951 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3177, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 630 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 171\n",
      "[Noun Extractor] 171 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1019, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2108 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4285, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 683 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 220 -> 217\n",
      "[Noun Extractor] postprocessing ignore_features : 217 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 207\n",
      "[Noun Extractor] 207 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2852 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4557, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1055 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 275 -> 266\n",
      "[Noun Extractor] postprocessing ignore_features : 266 -> 254\n",
      "[Noun Extractor] postprocessing ignore_NJ : 254 -> 254\n",
      "[Noun Extractor] 254 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 711 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1069, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 296 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 283 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=492, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 79 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 480 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=722, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 494 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=795, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5494 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11507, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1688 words\n",
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 700 -> 675\n",
      "[Noun Extractor] postprocessing ignore_features : 675 -> 655\n",
      "[Noun Extractor] postprocessing ignore_NJ : 655 -> 655\n",
      "[Noun Extractor] 655 nouns (158 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 53.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 496 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=720, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 822 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1328, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 597 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1920 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3471, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 662 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 183\n",
      "[Noun Extractor] postprocessing ignore_features : 183 -> 175\n",
      "[Noun Extractor] postprocessing ignore_NJ : 175 -> 175\n",
      "[Noun Extractor] 175 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 531 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=756, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 161 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1087 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1667, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 333 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 577 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=823, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2034 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3336, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 697 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 175 -> 174\n",
      "[Noun Extractor] postprocessing ignore_features : 174 -> 161\n",
      "[Noun Extractor] postprocessing ignore_NJ : 161 -> 161\n",
      "[Noun Extractor] 161 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 25.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 498 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=786, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 148 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1352 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2450, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 414 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=550, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 209 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=472, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 51 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 55.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 809 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1343, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 320 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 77\n",
      "[Noun Extractor] 77 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1397 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2413, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 452 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 131\n",
      "[Noun Extractor] postprocessing ignore_features : 131 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 485 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=684, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 821 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1507, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 584 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=934, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2011 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3320, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 701 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 199 -> 185\n",
      "[Noun Extractor] postprocessing ignore_features : 185 -> 173\n",
      "[Noun Extractor] postprocessing ignore_NJ : 173 -> 173\n",
      "[Noun Extractor] 173 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 431 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=599, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1098 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1746, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 371 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 499 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=711, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 29\n",
      "[Noun Extractor] 29 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 319 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=510, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 440 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=696, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 127 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 740 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1064, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 254 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=579, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 117 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 900 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1489, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 306 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 578 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=787, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2784 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5337, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1015 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 247 -> 247\n",
      "[Noun Extractor] postprocessing ignore_features : 247 -> 235\n",
      "[Noun Extractor] postprocessing ignore_NJ : 235 -> 235\n",
      "[Noun Extractor] 235 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 545 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=820, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 830 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1030 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1480, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 424 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1003 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1487, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 325 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 14.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 699 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=940, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 609 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=941, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1705 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2907, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 594 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 168 -> 168\n",
      "[Noun Extractor] postprocessing ignore_features : 168 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 160\n",
      "[Noun Extractor] 160 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 507 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=703, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 398 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=606, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 148 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 485 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=952, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 500 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=792, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 580 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1106, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1216 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1977, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1306 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2063, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 122\n",
      "[Noun Extractor] postprocessing ignore_NJ : 122 -> 122\n",
      "[Noun Extractor] 122 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 837 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1131, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1036 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1464, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 355 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1942 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3129, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 582 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 437 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=617, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 930 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1612, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 71\n",
      "[Noun Extractor] 71 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 527 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=858, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 995 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1498, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 768 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1238, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 336 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1257 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2002, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 382 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 394 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=608, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 144 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3183 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6956, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 361 -> 360\n",
      "[Noun Extractor] postprocessing ignore_features : 360 -> 346\n",
      "[Noun Extractor] postprocessing ignore_NJ : 346 -> 346\n",
      "[Noun Extractor] 346 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1346 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2169, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 440 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 105\n",
      "[Noun Extractor] 105 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1130 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1776, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 70\n",
      "[Noun Extractor] 70 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 691 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=897, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 932 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1591, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 382 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=569, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 110 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 718 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=997, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 269 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 537 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=831, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 166 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2441 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3952, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1054 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 457 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=642, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 182 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 648 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=876, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 637 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=910, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 794 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1229, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 252 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 670 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1027, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1163, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 271 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1482 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2307, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 509 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 157 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 148\n",
      "[Noun Extractor] postprocessing ignore_NJ : 148 -> 148\n",
      "[Noun Extractor] 148 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 344 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=527, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 106 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 522 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=802, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 710 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1219, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 678 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1697, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1717 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3246, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 952 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 137 -> 137\n",
      "[Noun Extractor] postprocessing ignore_features : 137 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 734 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1323, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 473 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=755, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 126 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1433 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2215, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 479 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 622 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 239 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1898 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2664, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 131 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 21.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=571, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 112 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 612 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=851, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 754 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1112, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 386 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=633, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1083 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1695, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1404 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2018, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 566 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 135 -> 135\n",
      "[Noun Extractor] postprocessing ignore_features : 135 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 534 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=724, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 921 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1671, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1802, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 77\n",
      "[Noun Extractor] 77 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 382 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=569, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 110 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 602 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1050, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 710 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1129, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 270 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 598 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=937, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 636 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1019, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2039 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2891, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 763 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 189 -> 189\n",
      "[Noun Extractor] postprocessing ignore_features : 189 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14814 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=35354, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4414 words\n",
      "[Noun Extractor] checked compounds. discovered 803 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2243 -> 2008\n",
      "[Noun Extractor] postprocessing ignore_features : 2008 -> 1967\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1967 -> 1964\n",
      "[Noun Extractor] 1964 nouns (803 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 59.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7351 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15092, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2444 words\n",
      "[Noun Extractor] checked compounds. discovered 146 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 863 -> 854\n",
      "[Noun Extractor] postprocessing ignore_features : 854 -> 835\n",
      "[Noun Extractor] postprocessing ignore_NJ : 835 -> 834\n",
      "[Noun Extractor] 834 nouns (146 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3449, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 655 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=887, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1564 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2387, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 565 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 158 -> 158\n",
      "[Noun Extractor] postprocessing ignore_features : 158 -> 148\n",
      "[Noun Extractor] postprocessing ignore_NJ : 148 -> 148\n",
      "[Noun Extractor] 148 nouns (18 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 399 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=570, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 405 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=575, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 138 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 369 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=600, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 96 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1016 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1653, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 485 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=744, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 460 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=693, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1126 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1625, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1075 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1700, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 327 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=526, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 98 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 451 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=705, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1049 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1643, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 363 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 11.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 572 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 167 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 34\n",
      "[Noun Extractor] 34 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7336 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13456, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2333 words\n",
      "[Noun Extractor] checked compounds. discovered 283 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 947 -> 857\n",
      "[Noun Extractor] postprocessing ignore_features : 857 -> 833\n",
      "[Noun Extractor] postprocessing ignore_NJ : 833 -> 830\n",
      "[Noun Extractor] 830 nouns (283 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 50.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 319 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=488, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 95 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1053 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1684, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 417 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=634, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 148 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 14 -> 14\n",
      "[Noun Extractor] postprocessing ignore_features : 14 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 368 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=596, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 104 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1858 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2865, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 678 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 157 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 982 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1598, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1514 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2379, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 545 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 141\n",
      "[Noun Extractor] postprocessing ignore_features : 141 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 134\n",
      "[Noun Extractor] 134 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 721 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1027, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1045 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1782, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 338 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 10.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1253 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1956, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 319 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 974 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1647, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (7 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1179 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1924, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 546 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 107\n",
      "[Noun Extractor] postprocessing ignore_features : 107 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1688, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 402 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=612, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 131 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 455 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=667, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 159 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 488 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=695, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 161 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 605 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=893, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6074 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11840, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2163 words\n",
      "[Noun Extractor] checked compounds. discovered 308 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 857 -> 781\n",
      "[Noun Extractor] postprocessing ignore_features : 781 -> 751\n",
      "[Noun Extractor] postprocessing ignore_NJ : 751 -> 746\n",
      "[Noun Extractor] 746 nouns (308 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 54.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 351 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=502, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 110 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 867 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1209, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 459 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=630, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 478 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=787, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 151 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 740 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1098, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=706, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 113 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 874 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1418, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 843 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1237, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 600 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=889, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 11.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 441 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=682, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 417 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=612, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 512 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=687, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 694 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1000, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 456 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=713, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 143 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2290 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 728 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 267\n",
      "[Noun Extractor] postprocessing ignore_features : 267 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 256\n",
      "[Noun Extractor] 256 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 52.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 817 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1128, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 300 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 747 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1168, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 537 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=882, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 182 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1699, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1449 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2181, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 576 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 787 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1231, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 486 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=808, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 588 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=861, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 226 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 379 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=619, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 128 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 414 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 116 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 758 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1079, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 573 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=865, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 644 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1085, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 204 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2458, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 404 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=696, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 420 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=633, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 139 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=657, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 97 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1853 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3216, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 913 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 231 -> 213\n",
      "[Noun Extractor] postprocessing ignore_features : 213 -> 205\n",
      "[Noun Extractor] postprocessing ignore_NJ : 205 -> 205\n",
      "[Noun Extractor] 205 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 463 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 158 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4332, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 719 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1080 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1757, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 382 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=636, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 114 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1083, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1027 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1858, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 258 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 914 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1382, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 272 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 376 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=607, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 120 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 21 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1287 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2327, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 744 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1247, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 276 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1393, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 499 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=782, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 178 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 894 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1609, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 349 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 116 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 614 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=880, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 347 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=521, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 120 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 547 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=888, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1014 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1661, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1964 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3269, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 706 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 188 -> 188\n",
      "[Noun Extractor] postprocessing ignore_features : 188 -> 176\n",
      "[Noun Extractor] postprocessing ignore_NJ : 176 -> 176\n",
      "[Noun Extractor] 176 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 725 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1174, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 227 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 12.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 413 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=580, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 126 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 684 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1112, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 968 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1437, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 294 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 12.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 355 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=559, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 113 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 385 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=658, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 213 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=678, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 152 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=939, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1471 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2492, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 464 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 447 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=648, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 700 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1194, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 230 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 740 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1325, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 188 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 500 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=828, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 42 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 879 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1283, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 12.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2752 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4696, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 930 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 281 -> 281\n",
      "[Noun Extractor] postprocessing ignore_features : 281 -> 269\n",
      "[Noun Extractor] postprocessing ignore_NJ : 269 -> 269\n",
      "[Noun Extractor] 269 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 694 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1015, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 256 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 668 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=886, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 210 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 45\n",
      "[Noun Extractor] 45 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 468 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=725, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 141 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 697 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=996, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 796 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1047, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 290 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 521 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=763, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1125 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1643, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2524 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4439, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 937 words\n",
      "[Noun Extractor] checked compounds. discovered 61 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 322 -> 292\n",
      "[Noun Extractor] postprocessing ignore_features : 292 -> 275\n",
      "[Noun Extractor] postprocessing ignore_NJ : 275 -> 275\n",
      "[Noun Extractor] 275 nouns (61 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1025 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1476, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 375 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 876 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1272, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 266 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 10.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 500 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=714, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2220 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3929, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 700 words\n",
      "[Noun Extractor] checked compounds. discovered 51 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 235 -> 231\n",
      "[Noun Extractor] postprocessing ignore_features : 231 -> 220\n",
      "[Noun Extractor] postprocessing ignore_NJ : 220 -> 220\n",
      "[Noun Extractor] 220 nouns (51 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 386 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=641, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 122 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 499 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=887, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 774 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1201, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1549 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2397, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 632 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 154 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 827 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1356, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 270 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 966 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1454, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1141 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1738, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 470 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 434 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=621, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 433 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=876, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=550, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 364 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=634, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=920, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1652 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2363, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 498 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 23.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 21192 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=51210, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6491 words\n",
      "[Noun Extractor] checked compounds. discovered 1655 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3399 -> 3189\n",
      "[Noun Extractor] postprocessing ignore_features : 3189 -> 3130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3130 -> 3115\n",
      "[Noun Extractor] 3115 nouns (1655 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 57.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 138 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=493, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 35 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 10 -> 10\n",
      "[Noun Extractor] postprocessing ignore_features : 10 -> 10\n",
      "[Noun Extractor] postprocessing ignore_NJ : 10 -> 10\n",
      "[Noun Extractor] 10 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 71.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 854 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1206, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 274 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1004 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1579, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 329 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 75\n",
      "[Noun Extractor] 75 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1097, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 666 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=778, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 23.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 523 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=824, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 557 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=867, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1120 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1685, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 377 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 110 -> 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 110 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1277 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2001, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 507 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=900, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 453 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=644, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1357 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2414, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 867 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1190, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1086 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1832, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 107\n",
      "[Noun Extractor] postprocessing ignore_features : 107 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1293 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1992, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 465 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 107\n",
      "[Noun Extractor] postprocessing ignore_features : 107 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1137, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 942 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1623, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 338 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1335 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1970, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 450 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 770 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1335, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 320 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 77\n",
      "[Noun Extractor] 77 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3919 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6142, mem=0.961 Gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] batch prediction was completed for 1380 words\n",
      "[Noun Extractor] checked compounds. discovered 101 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 417 -> 411\n",
      "[Noun Extractor] postprocessing ignore_features : 411 -> 396\n",
      "[Noun Extractor] postprocessing ignore_NJ : 396 -> 396\n",
      "[Noun Extractor] 396 nouns (101 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 788 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1117, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1014 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1508, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 329 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 437 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=624, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 156 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 259 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=490, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 88 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 15 -> 15\n",
      "[Noun Extractor] postprocessing ignore_features : 15 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 590 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=888, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 819 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1211, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 257 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 502 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=782, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 162 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 463 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 118 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 393 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=580, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 111 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 416 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=624, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 11 -> 11\n",
      "[Noun Extractor] postprocessing ignore_features : 11 -> 10\n",
      "[Noun Extractor] postprocessing ignore_NJ : 10 -> 10\n",
      "[Noun Extractor] 10 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 25.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 502 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=837, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 367 from 1 sents. mem=0.961 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=542, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 95 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1085 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1492, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 355 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 601 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=896, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1155 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1890, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 439 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1190 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1601, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 88\n",
      "[Noun Extractor] 88 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 759 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1002, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 275 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=910, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 470 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=712, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 489 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=732, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 150 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1167, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 396 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=572, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 141 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 35.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 616 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=863, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1083, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1500 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2318, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 486 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 163 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 158 -> 158\n",
      "[Noun Extractor] 158 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3650 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5819, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1200 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 360 -> 356\n",
      "[Noun Extractor] postprocessing ignore_features : 356 -> 341\n",
      "[Noun Extractor] postprocessing ignore_NJ : 341 -> 341\n",
      "[Noun Extractor] 341 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 390 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=584, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 115 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 756 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1612, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 243 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1226 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1779, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 347 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 430 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=598, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 131 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 12.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1148, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 465 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=822, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 239 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1679 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2674, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 552 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2715, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (25 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 339 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=612, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 83 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 13\n",
      "[Noun Extractor] postprocessing ignore_NJ : 13 -> 13\n",
      "[Noun Extractor] 13 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1511 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2604, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 499 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1497 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2223, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 511 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 123\n",
      "[Noun Extractor] 123 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 931 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1311 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2527, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 460 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 120 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 475 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=644, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2011 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3459, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 708 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 215\n",
      "[Noun Extractor] postprocessing ignore_features : 215 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 207\n",
      "[Noun Extractor] 207 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 624 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=734, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 246 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 23.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1654 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3175, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 510 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1508, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 121 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 635 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=953, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6152 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11626, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2132 words\n",
      "[Noun Extractor] checked compounds. discovered 151 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 734 -> 700\n",
      "[Noun Extractor] postprocessing ignore_features : 700 -> 674\n",
      "[Noun Extractor] postprocessing ignore_NJ : 674 -> 672\n",
      "[Noun Extractor] 672 nouns (151 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1752 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2604, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 618 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 172\n",
      "[Noun Extractor] postprocessing ignore_features : 172 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 455 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=841, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 156 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 492 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=684, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 158 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 450 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=714, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 139 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 590 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1038, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 609 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=921, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2843 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4392, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 973 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 210\n",
      "[Noun Extractor] postprocessing ignore_NJ : 210 -> 210\n",
      "[Noun Extractor] 210 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 743 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1017, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 288 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1218 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2040, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 118 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 110\n",
      "[Noun Extractor] postprocessing ignore_NJ : 110 -> 110\n",
      "[Noun Extractor] 110 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 424 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=654, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1055 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1528, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 359 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 110\n",
      "[Noun Extractor] postprocessing ignore_NJ : 110 -> 110\n",
      "[Noun Extractor] 110 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 474 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=711, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 122 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 546 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=791, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 494 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=686, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1211 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1892, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 457 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 98\n",
      "[Noun Extractor] 98 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 916 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1472, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 850 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1356, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 291 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1074 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1445, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 339 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1031 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1678, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 383 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 103\n",
      "[Noun Extractor] 103 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=955, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 670 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=931, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1005 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1727, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 411 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 603 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=974, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 176 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1202, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 208 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1078 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1710, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2179 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4234, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 800 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 232 -> 232\n",
      "[Noun Extractor] postprocessing ignore_features : 232 -> 220\n",
      "[Noun Extractor] postprocessing ignore_NJ : 220 -> 220\n",
      "[Noun Extractor] 220 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 393 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=579, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 397 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=588, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1028, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 278 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 848 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1328, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 339 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 629 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=919, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 391 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=543, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 122 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1060, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 241 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1058 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1845, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 325 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1695 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3286, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 551 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 123\n",
      "[Noun Extractor] 123 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 21.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 477 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=722, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 160 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 885 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1336, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 299 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=539, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 80 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 872 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1155, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 360 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1355 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1717, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 455 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 633 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=996, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 226 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 777 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1498, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1558, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 241 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4265, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 749 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 229 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 215\n",
      "[Noun Extractor] 215 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 488 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=663, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 595 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=874, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 176 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 559 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=811, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=893, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1550 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2067, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 448 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 608 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=867, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 950 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1578, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 971 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2140, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 230 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 402 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=567, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1591 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2590, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 473 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 128 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 535 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=820, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 459 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=659, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 161 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 816 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1464, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 415 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=768, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 933 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1360, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 404 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11407 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=28223, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4493 words\n",
      "[Noun Extractor] checked compounds. discovered 705 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1739 -> 1653\n",
      "[Noun Extractor] postprocessing ignore_features : 1653 -> 1620\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1620 -> 1616\n",
      "[Noun Extractor] 1616 nouns (705 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 65.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 505 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=694, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 43\n",
      "[Noun Extractor] 43 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 884 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1453, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 275 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 387 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=630, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 140 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 763 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1130, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1242 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2160, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 366 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 333 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=780, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 25.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 846 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1217, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 675 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=981, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 776 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1113, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 537 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=671, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 29\n",
      "[Noun Extractor] 29 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 839 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1517, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 545 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=729, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 197 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 389 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=575, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 158 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 710 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1149, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 467 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=790, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 480 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=626, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 824 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1357, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1110 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1838, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 403 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 513 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=745, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 584 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=926, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 543 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=750, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 195 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 823 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1232, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1424 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2130, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 155\n",
      "[Noun Extractor] postprocessing ignore_features : 155 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 147\n",
      "[Noun Extractor] 147 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 442 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=615, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 151 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2000 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3570, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 700 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 251 -> 249\n",
      "[Noun Extractor] postprocessing ignore_features : 249 -> 242\n",
      "[Noun Extractor] postprocessing ignore_NJ : 242 -> 242\n",
      "[Noun Extractor] 242 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1324 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2046, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 498 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 139\n",
      "[Noun Extractor] postprocessing ignore_NJ : 139 -> 139\n",
      "[Noun Extractor] 139 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 614 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 45\n",
      "[Noun Extractor] 45 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 830 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1292, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 241 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 459 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=774, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 812 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1278, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1056 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1784, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 837 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1164, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 327 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 416 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=679, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 106 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 455 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=902, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 509 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=723, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 125 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 24.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 443 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=632, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 130 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 687 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 44\n",
      "[Noun Extractor] 44 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 406 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=680, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 120 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 810 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1241, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 391 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1222, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1345 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2035, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 472 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 28.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 494 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=717, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 166 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 646 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=937, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 195 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 781 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1250, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1257 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2300, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 111\n",
      "[Noun Extractor] 111 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 51.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 367 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=566, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 131 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 550 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=777, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 513 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=813, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 126 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 468 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=848, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 257 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 560 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=836, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2419 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4236, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 800 words\n",
      "[Noun Extractor] checked compounds. discovered 69 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 281 -> 278\n",
      "[Noun Extractor] postprocessing ignore_features : 278 -> 274\n",
      "[Noun Extractor] postprocessing ignore_NJ : 274 -> 274\n",
      "[Noun Extractor] 274 nouns (69 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2196 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2879, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 955 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 212 -> 210\n",
      "[Noun Extractor] postprocessing ignore_features : 210 -> 201\n",
      "[Noun Extractor] postprocessing ignore_NJ : 201 -> 201\n",
      "[Noun Extractor] 201 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=999, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 227 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 629 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=930, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2019 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3325, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 644 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 232 -> 232\n",
      "[Noun Extractor] postprocessing ignore_features : 232 -> 223\n",
      "[Noun Extractor] postprocessing ignore_NJ : 223 -> 222\n",
      "[Noun Extractor] 222 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 403 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=581, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 529 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=865, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 172 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 997 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1665, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 696 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=989, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1878 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3319, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (14 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1047 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1446, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 16366 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=37636, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5169 words\n",
      "[Noun Extractor] checked compounds. discovered 1401 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2799 -> 2443\n",
      "[Noun Extractor] postprocessing ignore_features : 2443 -> 2394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2394 -> 2391\n",
      "[Noun Extractor] 2391 nouns (1401 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 56.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1435, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 451 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=674, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 143 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 13 -> 13\n",
      "[Noun Extractor] postprocessing ignore_features : 13 -> 12\n",
      "[Noun Extractor] postprocessing ignore_NJ : 12 -> 12\n",
      "[Noun Extractor] 12 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 298 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=533, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 96 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 813 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1201, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 349 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=498, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 124 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 746 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=974, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 672 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2355, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 239 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 8.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 564 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=819, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 424 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=590, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 159 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 501 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=686, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 159 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 943 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1446, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 276 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 473 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=726, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 149 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 24.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 828 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1368, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 623 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=958, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1081, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1436 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2373, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 483 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 114\n",
      "[Noun Extractor] postprocessing ignore_NJ : 114 -> 113\n",
      "[Noun Extractor] 113 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 781 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1179, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 285 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 738 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1041, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2448 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3966, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 894 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 234\n",
      "[Noun Extractor] postprocessing ignore_features : 234 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 222\n",
      "[Noun Extractor] 222 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 882 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1396, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1203 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1961, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 384 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 110\n",
      "[Noun Extractor] postprocessing ignore_NJ : 110 -> 110\n",
      "[Noun Extractor] 110 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 593 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=969, mem=0.961 Gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] batch prediction was completed for 294 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10006 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21637, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3389 words\n",
      "[Noun Extractor] checked compounds. discovered 594 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1626 -> 1442\n",
      "[Noun Extractor] postprocessing ignore_features : 1442 -> 1408\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1408 -> 1402\n",
      "[Noun Extractor] 1402 nouns (594 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 59.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 762 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1202, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 471 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=788, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 429 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=642, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 112 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1505 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2349, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 144\n",
      "[Noun Extractor] postprocessing ignore_features : 144 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1277 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2145, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 408 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2238 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3922, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 783 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 240 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 368 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 138 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 402 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=610, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 112 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 14 -> 14\n",
      "[Noun Extractor] postprocessing ignore_features : 14 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 586 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=806, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2676 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3868, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 978 words\n",
      "[Noun Extractor] checked compounds. discovered 74 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 343 -> 287\n",
      "[Noun Extractor] postprocessing ignore_features : 287 -> 272\n",
      "[Noun Extractor] postprocessing ignore_NJ : 272 -> 272\n",
      "[Noun Extractor] 272 nouns (74 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.06 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1275 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1952, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 469 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 900 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1378, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3336 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5194, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1195 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 322 -> 314\n",
      "[Noun Extractor] postprocessing ignore_features : 314 -> 297\n",
      "[Noun Extractor] postprocessing ignore_NJ : 297 -> 297\n",
      "[Noun Extractor] 297 nouns (32 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 515 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=823, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1323 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1969, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 430 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 16.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 493 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=753, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 190 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 453 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=933, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 444 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=646, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 152 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1188 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2045, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 376 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 100\n",
      "[Noun Extractor] postprocessing ignore_features : 100 -> 94\n",
      "[Noun Extractor] postprocessing ignore_NJ : 94 -> 94\n",
      "[Noun Extractor] 94 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1547 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2483, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 535 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 162\n",
      "[Noun Extractor] postprocessing ignore_features : 162 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 151\n",
      "[Noun Extractor] 151 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 517 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=741, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 8.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 582 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=856, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1102 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1463, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 352 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 650 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=932, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 216 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 47\n",
      "[Noun Extractor] 47 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 684 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1069, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 507 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=718, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 190 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 521 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=710, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 322 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=536, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 125 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 743 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1078, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 721 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1133, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 443 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=697, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 131 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=940, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1212 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2110, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 398 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 45.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2045 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3252, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 670 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1100 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1636, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 467 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=713, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2779 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5321, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 826 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 217 -> 216\n",
      "[Noun Extractor] postprocessing ignore_features : 216 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 207\n",
      "[Noun Extractor] 207 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 443 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=709, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 118 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=788, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 328 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=498, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 98 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=971, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 191 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 660 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 245 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=673, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 96 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 385 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=621, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 124 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 459 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=760, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 881 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1373, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 365 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=568, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 124 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 456 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=661, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 916 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1268, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 352 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 529 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1072, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1121, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 608 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=878, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 266 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 15.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 547 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=814, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 748 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1065, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 254 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 250 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=409, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 64 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 12 -> 12\n",
      "[Noun Extractor] postprocessing ignore_features : 12 -> 12\n",
      "[Noun Extractor] postprocessing ignore_NJ : 12 -> 12\n",
      "[Noun Extractor] 12 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 690 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=970, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 239 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 590 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=946, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 210 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1196 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1939, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 509 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 107\n",
      "[Noun Extractor] postprocessing ignore_features : 107 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 686 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1240, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 283 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 18.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 539 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=820, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 7.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 461 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=639, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1309 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2152, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 504 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 144\n",
      "[Noun Extractor] postprocessing ignore_features : 144 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 133\n",
      "[Noun Extractor] 133 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 48.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 569 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=809, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 642 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1015, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 14.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 378 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=582, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 140 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 636 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=817, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 8.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 711 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=981, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 541 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=806, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 163 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1081 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1460, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 451 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 77\n",
      "[Noun Extractor] 77 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 25.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 431 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=668, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 172 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 38\n",
      "[Noun Extractor] 38 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 574 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=835, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 847 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1414, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 518 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=690, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 197 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 579 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=773, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 208 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=948, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 760 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1150, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 610 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=875, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 271 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1110 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1731, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 374 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 917 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1401, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 460 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=708, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 95 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_features : 21 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 26.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2929 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4129, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1035 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 288 -> 285\n",
      "[Noun Extractor] postprocessing ignore_features : 285 -> 271\n",
      "[Noun Extractor] postprocessing ignore_NJ : 271 -> 271\n",
      "[Noun Extractor] 271 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 17440 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=41464, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4836 words\n",
      "[Noun Extractor] checked compounds. discovered 794 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2149 -> 2047\n",
      "[Noun Extractor] postprocessing ignore_features : 2047 -> 2011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2011 -> 2007\n",
      "[Noun Extractor] 2007 nouns (794 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 55.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 519 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=674, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 796 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1090, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 668 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=936, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 303 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1289 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2031, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 415 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 128 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 504 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=704, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 412 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=734, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 141 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 639 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1040, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 178 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 397 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=660, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 153 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 667 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=983, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5937 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11362, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1844 words\n",
      "[Noun Extractor] checked compounds. discovered 98 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 561 -> 555\n",
      "[Noun Extractor] postprocessing ignore_features : 555 -> 533\n",
      "[Noun Extractor] postprocessing ignore_NJ : 533 -> 533\n",
      "[Noun Extractor] 533 nouns (98 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1076 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1830, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 985 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1504, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 306 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 68\n",
      "[Noun Extractor] 68 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 519 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=734, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 345 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=787, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 455 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=649, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1398 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2306, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 433 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 989 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1515, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1270 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1811, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1161 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1895, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 445 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1504 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2302, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 559 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 916 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1262, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 297 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 953 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1357, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 340 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=888, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 608 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1025, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 22\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22 -> 22\n",
      "[Noun Extractor] 22 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 24.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 566 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=984, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 826 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1130, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1908 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3278, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 651 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 190\n",
      "[Noun Extractor] postprocessing ignore_features : 190 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=929, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 423 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=637, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 139 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 589 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=841, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 734 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1055, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=927, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 204 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 530 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=733, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=861, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1378, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 346 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 868 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1257, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 12.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 607 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1074, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 140 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 535 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=846, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 556 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=757, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 266 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1030, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1105, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 256 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 558 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=832, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1307, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 327 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 60\n",
      "[Noun Extractor] 60 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1243, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1762 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3517, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 531 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 173 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1602, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 694 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=987, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 553 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=826, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 48\n",
      "[Noun Extractor] 48 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2605 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4864, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 810 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 242\n",
      "[Noun Extractor] postprocessing ignore_features : 242 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 49.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 365 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=568, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 26 -> 26\n",
      "[Noun Extractor] postprocessing ignore_features : 26 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 640 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=962, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 47.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 382 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=560, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 410 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=655, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1080 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1649, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 483 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=715, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 133 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 20\n",
      "[Noun Extractor] postprocessing ignore_NJ : 20 -> 20\n",
      "[Noun Extractor] 20 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 24.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 475 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=809, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 826 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1264, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 311 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2229 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3700, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 709 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 200 -> 200\n",
      "[Noun Extractor] postprocessing ignore_features : 200 -> 194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 194 -> 194\n",
      "[Noun Extractor] 194 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 720 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1267, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 578 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=836, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 14.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 861 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1285, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 327 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 17.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2622 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3867, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 810 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 191 -> 191\n",
      "[Noun Extractor] postprocessing ignore_features : 191 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 19.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1082, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 34.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 578 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=939, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1334 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1888, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 576 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1207 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1956, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 451 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 807 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1320, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 384 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 558 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=770, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 9.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 591 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=864, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 173 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 609 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=967, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 188 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 935 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1322, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 377 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 30.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 606 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=890, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 37\n",
      "[Noun Extractor] 37 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 296 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=513, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 79 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1120 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1820, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 382 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 646 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=979, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 590 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 216 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 22.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 682 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 257 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 15.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 779 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 306 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 486 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=711, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 504 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=799, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 29\n",
      "[Noun Extractor] postprocessing ignore_features : 29 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 27.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 838 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1453, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 429 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 648 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=908, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 718 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1203, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 230 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 603 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=932, mem=0.961 Gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 480 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=662, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1647 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2290, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 545 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 161 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 154\n",
      "[Noun Extractor] postprocessing ignore_NJ : 154 -> 154\n",
      "[Noun Extractor] 154 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1078 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1921, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 41.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1141 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1726, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 519 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=756, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 150 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1692 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2810, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 542 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 174 -> 174\n",
      "[Noun Extractor] postprocessing ignore_features : 174 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 169\n",
      "[Noun Extractor] 169 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1298 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2979, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 272 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=961, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 870 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1482, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 429 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=582, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 141 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1716 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3229, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 501 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 20.13 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 676 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=983, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 606 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=957, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 410 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=618, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 128 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1388 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2888, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 388 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=605, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 104 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 15\n",
      "[Noun Extractor] postprocessing ignore_NJ : 15 -> 15\n",
      "[Noun Extractor] 15 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1058, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 272 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 29.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 445 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=656, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 549 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=877, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 163 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2250 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3868, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 681 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 223 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 216\n",
      "[Noun Extractor] 216 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 43.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 573 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=854, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 216 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 36.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 330 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=491, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 35.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 487 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=651, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 13.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 559 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=778, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 865 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1184, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 347 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=499, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 101 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 15 -> 15\n",
      "[Noun Extractor] postprocessing ignore_features : 15 -> 13\n",
      "[Noun Extractor] postprocessing ignore_NJ : 13 -> 13\n",
      "[Noun Extractor] 13 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 31.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 473 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=668, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 38.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 809 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1148, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 561 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1362 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2450, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 117\n",
      "[Noun Extractor] 117 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 635 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 283 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 413 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=649, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 127 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 32.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3471 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7088, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1125 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 317 -> 313\n",
      "[Noun Extractor] postprocessing ignore_features : 313 -> 303\n",
      "[Noun Extractor] postprocessing ignore_NJ : 303 -> 303\n",
      "[Noun Extractor] 303 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 815 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1287, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 66\n",
      "[Noun Extractor] 66 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 724 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1105, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 224 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 39.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1481 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2807, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 514 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 130 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (26 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 44.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 887 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1287, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 303 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 33.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 472 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=724, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 143 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 320 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=503, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 115 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 20 -> 20\n",
      "[Noun Extractor] postprocessing ignore_features : 20 -> 19\n",
      "[Noun Extractor] postprocessing ignore_NJ : 19 -> 19\n",
      "[Noun Extractor] 19 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 37.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1390 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2146, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 509 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 42.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1266 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2134, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 383 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 40.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 890 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1272, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 10.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 967 from 1 sents. mem=0.961 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1569, mem=0.961 Gb\n",
      "[Noun Extractor] batch prediction was completed for 300 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 110 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 102\n",
      "[Noun Extractor] 102 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.961 Gb                    \n",
      "[Noun Extractor] 46.97 % eojeols are covered\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Table 'var' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b60b766303a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstatistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_rat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Humor_var.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstatistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2651\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2653\u001b[0;31m         sql.to_sql(\n\u001b[0m\u001b[1;32m   2654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m    510\u001b[0m         )\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m     pandas_sql.to_sql(\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         )\n\u001b[0;32m-> 1733\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fail\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Table '{self.name}' already exists.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Table 'var' already exists."
     ]
    }
   ],
   "source": [
    "import Extractor\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head ORDER BY wdate DESC',conn)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "ext = Extractor.Ext(df)\n",
    "df = ext.cleaning()\n",
    "\n",
    "new_words = ext.search_dict(sorted(ext.extract_nouns().items(),key=lambda _:_[1], reverse=True))\n",
    "sent = ext.extract_sent(new_words)\n",
    "\n",
    "# 변수 생성\n",
    "statistic = ext.extract_statistic_value(sent)\n",
    "r_rat = ext.extract_r_rat(sent,statistic)\n",
    "statistic = ext.combine_var(statistic, r_rat)\n",
    "conn = sqlite3.connect('Humor_var.db')\n",
    "statistic.to_sql('var', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohesion_forward</th>\n",
       "      <th>cohesion_backward</th>\n",
       "      <th>left_branching_entropy</th>\n",
       "      <th>right_branching_entropy</th>\n",
       "      <th>left_accessor_variety</th>\n",
       "      <th>right_accessor_variety</th>\n",
       "      <th>leftside_frequency</th>\n",
       "      <th>rightside_frequency</th>\n",
       "      <th>right_post_postion_ratio</th>\n",
       "      <th>right_whitespace_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>확진자</th>\n",
       "      <td>0.984480</td>\n",
       "      <td>0.260976</td>\n",
       "      <td>4.114816</td>\n",
       "      <td>3.894225</td>\n",
       "      <td>190.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>4563.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.172913</td>\n",
       "      <td>0.756520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>알아보자</th>\n",
       "      <td>0.997411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.744232</td>\n",
       "      <td>4.692180</td>\n",
       "      <td>47.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>11400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.964815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>문재앙</th>\n",
       "      <td>0.966864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.684202</td>\n",
       "      <td>4.508473</td>\n",
       "      <td>240.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>2797.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209660</td>\n",
       "      <td>0.722004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>얘들아</th>\n",
       "      <td>0.997093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.213548</td>\n",
       "      <td>4.150799</td>\n",
       "      <td>179.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.972442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>스압</th>\n",
       "      <td>0.873124</td>\n",
       "      <td>0.518752</td>\n",
       "      <td>4.241932</td>\n",
       "      <td>4.977995</td>\n",
       "      <td>334.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>7852.0</td>\n",
       "      <td>5353.0</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.636822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>유세윤</th>\n",
       "      <td>0.069765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.789972</td>\n",
       "      <td>2.308084</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.746154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>흑형</th>\n",
       "      <td>0.211278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.638187</td>\n",
       "      <td>3.122885</td>\n",
       "      <td>76.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338208</td>\n",
       "      <td>0.484461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>상남자</th>\n",
       "      <td>0.208081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.937743</td>\n",
       "      <td>3.812566</td>\n",
       "      <td>87.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190559</td>\n",
       "      <td>0.625874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>넣고</th>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.117934</td>\n",
       "      <td>2.607551</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.819444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>긴급재</th>\n",
       "      <td>0.622939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.241160</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1196 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cohesion_forward  cohesion_backward  left_branching_entropy  \\\n",
       "확진자           0.984480           0.260976                4.114816   \n",
       "알아보자          0.997411           0.000000                1.744232   \n",
       "문재앙           0.966864           0.000000                4.684202   \n",
       "얘들아           0.997093           0.000000                4.213548   \n",
       "스압            0.873124           0.518752                4.241932   \n",
       "...                ...                ...                     ...   \n",
       "유세윤           0.069765           0.000000                2.789972   \n",
       "흑형            0.211278           0.000000                3.638187   \n",
       "상남자           0.208081           0.000000                3.937743   \n",
       "넣고            0.993103           0.000000                2.117934   \n",
       "긴급재           0.622939           0.000000                3.241160   \n",
       "\n",
       "      right_branching_entropy  left_accessor_variety  right_accessor_variety  \\\n",
       "확진자                  3.894225                  190.0                   211.0   \n",
       "알아보자                 4.692180                   47.0                   445.0   \n",
       "문재앙                  4.508473                  240.0                   243.0   \n",
       "얘들아                  4.150799                  179.0                   253.0   \n",
       "스압                   4.977995                  334.0                   460.0   \n",
       "...                       ...                    ...                     ...   \n",
       "유세윤                  2.308084                   24.0                    15.0   \n",
       "흑형                   3.122885                   76.0                    42.0   \n",
       "상남자                  3.812566                   87.0                    68.0   \n",
       "넣고                   2.607551                   17.0                    25.0   \n",
       "긴급재                 -0.000000                   60.0                     1.0   \n",
       "\n",
       "      leftside_frequency  rightside_frequency  right_post_postion_ratio  \\\n",
       "확진자               4563.0                274.0                  0.172913   \n",
       "알아보자             11400.0                  0.0                  0.007019   \n",
       "문재앙               2797.0                  0.0                  0.209660   \n",
       "얘들아               2911.0                  0.0                  0.004823   \n",
       "스압                7852.0               5353.0                  0.004904   \n",
       "...                  ...                  ...                       ...   \n",
       "유세윤                130.0                  0.0                  0.184615   \n",
       "흑형                 547.0                  0.0                  0.338208   \n",
       "상남자                573.0                  0.0                  0.190559   \n",
       "넣고                 144.0                  0.0                  0.041667   \n",
       "긴급재                929.0                  0.0                  0.000000   \n",
       "\n",
       "      right_whitespace_ratio  \n",
       "확진자                 0.756520  \n",
       "알아보자                0.964815  \n",
       "문재앙                 0.722004  \n",
       "얘들아                 0.972442  \n",
       "스압                  0.636822  \n",
       "...                      ...  \n",
       "유세윤                 0.746154  \n",
       "흑형                  0.484461  \n",
       "상남자                 0.625874  \n",
       "넣고                  0.819444  \n",
       "긴급재                 0.000000  \n",
       "\n",
       "[1196 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
