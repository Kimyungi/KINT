{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Extractor\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn1 = sqlite3.connect('Entertainment.db')\n",
    "cur1 = conn1.cursor()\n",
    "df1 = pd.read_sql('SELECT * FROM head ORDER BY wdate DESC',conn1)\n",
    "conn1.close()\n",
    "\n",
    "conn2 = sqlite3.connect('Entertainment101.db')\n",
    "cur2 = conn2.cursor()\n",
    "df2 = pd.read_sql('SELECT * FROM head ORDER BY wdate DESC',conn2)\n",
    "conn2.close()\n",
    "\n",
    "df = df1.append(df2) # 엔터테인먼트는 여기서 분할해서 저장해야함.\n",
    "\n",
    "df['pk'] = range(1, len(df)+1)\n",
    "\n",
    "conn3 = sqlite3.connect('Total_Ent.db')\n",
    "cur3 = conn3.cursor()\n",
    "cur3.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df.to_sql('head', conn3, if_exists='append', index=False)\n",
    "conn3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Total_Ent.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT * FROM head ORDER BY wdate DESC',conn)\n",
    "df1 = df[:2000000]\n",
    "df2 = df[2000000:4000000]\n",
    "df3 = df[4000000:6000000]\n",
    "df4 = df[6000000:8000000]\n",
    "df5 = df[8000000:]\n",
    "\n",
    "conn = sqlite3.connect('Ent_1.db')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df1.to_sql('head', conn, if_exists='append', index=False)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_2.db')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df2.to_sql('head', conn, if_exists='append', index=False)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_3.db')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df3.to_sql('head', conn, if_exists='append', index=False)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_4.db')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df4.to_sql('head', conn, if_exists='append', index=False)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_5.db')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "            DROP TABLE IF EXISTS head;\n",
    "            CREATE TABLE head(\n",
    "                pk INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "                head TEXT NOT NULL,\n",
    "                wdate TEXT NOT NULL,\n",
    "                cdate TEXT NOT NULL,\n",
    "                ref INTEGER NOT NULL,\n",
    "                page INTEGER NOT NULL\n",
    "            );\n",
    "        ''')\n",
    "df5.to_sql('head', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "conn = sqlite3.connect('Ent_1_var.db')\n",
    "df1 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_2_var.db')\n",
    "df2 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_3_var.db')\n",
    "df3 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_4_var.db')\n",
    "df4 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('Ent_5_var.db')\n",
    "df5 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('HP_var.db')\n",
    "df6 = pd.read_sql('SELECT * FROM var', conn)\n",
    "conn.close()\n",
    "\n",
    "df = df1.append(df2).append(df3).append(df4).append(df5).append(df6)\n",
    "conn = sqlite3.connect('var.db')\n",
    "df.to_sql('var', conn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1382555 from 1658304 sents. mem=0.680 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7295360, mem=3.591 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322395 words\n",
      "[Noun Extractor] checked compounds. discovered 343047 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3513 -> 3445\n",
      "[Noun Extractor] postprocessing ignore_features : 3445 -> 3331\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3331 -> 3155\n",
      "[Noun Extractor] 3155 nouns (343047 compounds) with min frequency=165\n",
      "[Noun Extractor] flushing was done. mem=3.687 Gb                    \n",
      "[Noun Extractor] 56.21 % eojeols are covered\n",
      "training was done. used memory 4.379 Gb4.403 Gb\n",
      "all cohesion probabilities was computed. # words = 144\n",
      "all branching entropies was computed # words = 15447\n",
      "all accessor variety was computed # words = 15447\n",
      "training was done. used memory 4.382 Gb4.391 Gb\n",
      "all cohesion probabilities was computed. # words = 186\n",
      "all branching entropies was computed # words = 20760\n",
      "all accessor variety was computed # words = 20760\n",
      "training was done. used memory 4.385 Gb4.405 Gb\n",
      "all cohesion probabilities was computed. # words = 253\n",
      "all branching entropies was computed # words = 26925\n",
      "all accessor variety was computed # words = 26925\n",
      "training was done. used memory 4.387 Gb4.405 Gb\n",
      "all cohesion probabilities was computed. # words = 292\n",
      "all branching entropies was computed # words = 31362\n",
      "all accessor variety was computed # words = 31362\n",
      "training was done. used memory 4.392 Gb4.401 Gb\n",
      "all cohesion probabilities was computed. # words = 319\n",
      "all branching entropies was computed # words = 33614\n",
      "all accessor variety was computed # words = 33614\n",
      "training was done. used memory 4.392 Gb4.405 Gb\n",
      "all cohesion probabilities was computed. # words = 341\n",
      "all branching entropies was computed # words = 36758\n",
      "all accessor variety was computed # words = 36758\n",
      "training was done. used memory 4.393 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 357\n",
      "all branching entropies was computed # words = 38605\n",
      "all accessor variety was computed # words = 38605\n",
      "training was done. used memory 4.394 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 370\n",
      "all branching entropies was computed # words = 40379\n",
      "all accessor variety was computed # words = 40379\n",
      "training was done. used memory 4.394 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 386\n",
      "all branching entropies was computed # words = 42208\n",
      "all accessor variety was computed # words = 42208\n",
      "training was done. used memory 4.394 Gb4.405 Gb\n",
      "all cohesion probabilities was computed. # words = 392\n",
      "all branching entropies was computed # words = 44199\n",
      "all accessor variety was computed # words = 44199\n",
      "training was done. used memory 4.394 Gb4.400 Gb\n",
      "all cohesion probabilities was computed. # words = 409\n",
      "all branching entropies was computed # words = 45336\n",
      "all accessor variety was computed # words = 45336\n",
      "training was done. used memory 4.395 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 426\n",
      "all branching entropies was computed # words = 46352\n",
      "all accessor variety was computed # words = 46352\n",
      "training was done. used memory 4.395 Gb4.401 Gb\n",
      "all cohesion probabilities was computed. # words = 434\n",
      "all branching entropies was computed # words = 46810\n",
      "all accessor variety was computed # words = 46810\n",
      "training was done. used memory 4.395 Gb4.400 Gb\n",
      "all cohesion probabilities was computed. # words = 441\n",
      "all branching entropies was computed # words = 47185\n",
      "all accessor variety was computed # words = 47185\n",
      "training was done. used memory 4.395 Gb4.402 Gb\n",
      "all cohesion probabilities was computed. # words = 449\n",
      "all branching entropies was computed # words = 47903\n",
      "all accessor variety was computed # words = 47903\n",
      "training was done. used memory 4.395 Gb4.402 Gb\n",
      "all cohesion probabilities was computed. # words = 454\n",
      "all branching entropies was computed # words = 48394\n",
      "all accessor variety was computed # words = 48394\n",
      "training was done. used memory 4.395 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 466\n",
      "all branching entropies was computed # words = 49300\n",
      "all accessor variety was computed # words = 49300\n",
      "training was done. used memory 4.395 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 474\n",
      "all branching entropies was computed # words = 49809\n",
      "all accessor variety was computed # words = 49809\n",
      "training was done. used memory 4.396 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 494\n",
      "all branching entropies was computed # words = 50640\n",
      "all accessor variety was computed # words = 50640\n",
      "training was done. used memory 4.396 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 500\n",
      "all branching entropies was computed # words = 51418\n",
      "all accessor variety was computed # words = 51418\n",
      "training was done. used memory 4.396 Gb4.401 Gb\n",
      "all cohesion probabilities was computed. # words = 505\n",
      "all branching entropies was computed # words = 51879\n",
      "all accessor variety was computed # words = 51879\n",
      "training was done. used memory 4.396 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 515\n",
      "all branching entropies was computed # words = 52099\n",
      "all accessor variety was computed # words = 52099\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 521\n",
      "all branching entropies was computed # words = 52451\n",
      "all accessor variety was computed # words = 52451\n",
      "training was done. used memory 4.401 Gb4.401 Gb\n",
      "all cohesion probabilities was computed. # words = 523\n",
      "all branching entropies was computed # words = 52845\n",
      "all accessor variety was computed # words = 52845\n",
      "training was done. used memory 4.401 Gb4.409 Gb\n",
      "all cohesion probabilities was computed. # words = 567\n",
      "all branching entropies was computed # words = 54823\n",
      "all accessor variety was computed # words = 54823\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 574\n",
      "all branching entropies was computed # words = 55208\n",
      "all accessor variety was computed # words = 55208\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 576\n",
      "all branching entropies was computed # words = 55406\n",
      "all accessor variety was computed # words = 55406\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 578\n",
      "all branching entropies was computed # words = 55647\n",
      "all accessor variety was computed # words = 55647\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 590\n",
      "all branching entropies was computed # words = 55944\n",
      "all accessor variety was computed # words = 55944\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 591\n",
      "all branching entropies was computed # words = 56243\n",
      "all accessor variety was computed # words = 56243\n",
      "training was done. used memory 4.401 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 594\n",
      "all branching entropies was computed # words = 56547\n",
      "all accessor variety was computed # words = 56547\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 598\n",
      "all branching entropies was computed # words = 56951\n",
      "all accessor variety was computed # words = 56951\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 603\n",
      "all branching entropies was computed # words = 57532\n",
      "all accessor variety was computed # words = 57532\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 606\n",
      "all branching entropies was computed # words = 57668\n",
      "all accessor variety was computed # words = 57668\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 611\n",
      "all branching entropies was computed # words = 57781\n",
      "all accessor variety was computed # words = 57781\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 613\n",
      "all branching entropies was computed # words = 57895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 57895\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 621\n",
      "all branching entropies was computed # words = 58311\n",
      "all accessor variety was computed # words = 58311\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 621\n",
      "all branching entropies was computed # words = 58459\n",
      "all accessor variety was computed # words = 58459\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 625\n",
      "all branching entropies was computed # words = 58762\n",
      "all accessor variety was computed # words = 58762\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 627\n",
      "all branching entropies was computed # words = 58966\n",
      "all accessor variety was computed # words = 58966\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 630\n",
      "all branching entropies was computed # words = 59096\n",
      "all accessor variety was computed # words = 59096\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 631\n",
      "all branching entropies was computed # words = 59215\n",
      "all accessor variety was computed # words = 59215\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 635\n",
      "all branching entropies was computed # words = 59393\n",
      "all accessor variety was computed # words = 59393\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 636\n",
      "all branching entropies was computed # words = 59497\n",
      "all accessor variety was computed # words = 59497\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 638\n",
      "all branching entropies was computed # words = 59638\n",
      "all accessor variety was computed # words = 59638\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 641\n",
      "all branching entropies was computed # words = 59966\n",
      "all accessor variety was computed # words = 59966\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 646\n",
      "all branching entropies was computed # words = 60539\n",
      "all accessor variety was computed # words = 60539\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 647\n",
      "all branching entropies was computed # words = 60636\n",
      "all accessor variety was computed # words = 60636\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 650\n",
      "all branching entropies was computed # words = 60854\n",
      "all accessor variety was computed # words = 60854\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 653\n",
      "all branching entropies was computed # words = 61005\n",
      "all accessor variety was computed # words = 61005\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 660\n",
      "all branching entropies was computed # words = 61189\n",
      "all accessor variety was computed # words = 61189\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 661\n",
      "all branching entropies was computed # words = 61420\n",
      "all accessor variety was computed # words = 61420\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 666\n",
      "all branching entropies was computed # words = 61791\n",
      "all accessor variety was computed # words = 61791\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 668\n",
      "all branching entropies was computed # words = 61839\n",
      "all accessor variety was computed # words = 61839\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 669\n",
      "all branching entropies was computed # words = 61955\n",
      "all accessor variety was computed # words = 61955\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 670\n",
      "all branching entropies was computed # words = 62415\n",
      "all accessor variety was computed # words = 62415\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 671\n",
      "all branching entropies was computed # words = 62553\n",
      "all accessor variety was computed # words = 62553\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 672\n",
      "all branching entropies was computed # words = 62664\n",
      "all accessor variety was computed # words = 62664\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 672\n",
      "all branching entropies was computed # words = 63170\n",
      "all accessor variety was computed # words = 63170\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 680\n",
      "all branching entropies was computed # words = 63286\n",
      "all accessor variety was computed # words = 63286\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 684\n",
      "all branching entropies was computed # words = 63452\n",
      "all accessor variety was computed # words = 63452\n",
      "training was done. used memory 4.406 Gb4.406 Gb\n",
      "all cohesion probabilities was computed. # words = 687\n",
      "all branching entropies was computed # words = 63627\n",
      "all accessor variety was computed # words = 63627\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 687\n",
      "all branching entropies was computed # words = 63727\n",
      "all accessor variety was computed # words = 63727\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 689\n",
      "all branching entropies was computed # words = 63839\n",
      "all accessor variety was computed # words = 63839\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 693\n",
      "all branching entropies was computed # words = 64105\n",
      "all accessor variety was computed # words = 64105\n",
      "training was done. used memory 4.407 Gb4.410 Gb\n",
      "all cohesion probabilities was computed. # words = 702\n",
      "all branching entropies was computed # words = 65765\n",
      "all accessor variety was computed # words = 65765\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 708\n",
      "all branching entropies was computed # words = 65864\n",
      "all accessor variety was computed # words = 65864\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 710\n",
      "all branching entropies was computed # words = 65967\n",
      "all accessor variety was computed # words = 65967\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 714\n",
      "all branching entropies was computed # words = 66166\n",
      "all accessor variety was computed # words = 66166\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 719\n",
      "all branching entropies was computed # words = 66304\n",
      "all accessor variety was computed # words = 66304\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 722\n",
      "all branching entropies was computed # words = 66417\n",
      "all accessor variety was computed # words = 66417\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 724\n",
      "all branching entropies was computed # words = 66525\n",
      "all accessor variety was computed # words = 66525\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 725\n",
      "all branching entropies was computed # words = 66632\n",
      "all accessor variety was computed # words = 66632\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 727\n",
      "all branching entropies was computed # words = 66777\n",
      "all accessor variety was computed # words = 66777\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 728\n",
      "all branching entropies was computed # words = 66958\n",
      "all accessor variety was computed # words = 66958\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 729\n",
      "all branching entropies was computed # words = 67106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 67106\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 732\n",
      "all branching entropies was computed # words = 67226\n",
      "all accessor variety was computed # words = 67226\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 735\n",
      "all branching entropies was computed # words = 67406\n",
      "all accessor variety was computed # words = 67406\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 738\n",
      "all branching entropies was computed # words = 67448\n",
      "all accessor variety was computed # words = 67448\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 741\n",
      "all branching entropies was computed # words = 67572\n",
      "all accessor variety was computed # words = 67572\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 742\n",
      "all branching entropies was computed # words = 67663\n",
      "all accessor variety was computed # words = 67663\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 742\n",
      "all branching entropies was computed # words = 67700\n",
      "all accessor variety was computed # words = 67700\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 745\n",
      "all branching entropies was computed # words = 67771\n",
      "all accessor variety was computed # words = 67771\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 748\n",
      "all branching entropies was computed # words = 68030\n",
      "all accessor variety was computed # words = 68030\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 748\n",
      "all branching entropies was computed # words = 68127\n",
      "all accessor variety was computed # words = 68127\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 749\n",
      "all branching entropies was computed # words = 68345\n",
      "all accessor variety was computed # words = 68345\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 751\n",
      "all branching entropies was computed # words = 68519\n",
      "all accessor variety was computed # words = 68519\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 753\n",
      "all branching entropies was computed # words = 68642\n",
      "all accessor variety was computed # words = 68642\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 756\n",
      "all branching entropies was computed # words = 68804\n",
      "all accessor variety was computed # words = 68804\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 759\n",
      "all branching entropies was computed # words = 68887\n",
      "all accessor variety was computed # words = 68887\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 764\n",
      "all branching entropies was computed # words = 68966\n",
      "all accessor variety was computed # words = 68966\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 764\n",
      "all branching entropies was computed # words = 69008\n",
      "all accessor variety was computed # words = 69008\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 766\n",
      "all branching entropies was computed # words = 69068\n",
      "all accessor variety was computed # words = 69068\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 770\n",
      "all branching entropies was computed # words = 69139\n",
      "all accessor variety was computed # words = 69139\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 772\n",
      "all branching entropies was computed # words = 69230\n",
      "all accessor variety was computed # words = 69230\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 772\n",
      "all branching entropies was computed # words = 69281\n",
      "all accessor variety was computed # words = 69281\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 776\n",
      "all branching entropies was computed # words = 69414\n",
      "all accessor variety was computed # words = 69414\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 779\n",
      "all branching entropies was computed # words = 69549\n",
      "all accessor variety was computed # words = 69549\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 779\n",
      "all branching entropies was computed # words = 69583\n",
      "all accessor variety was computed # words = 69583\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 780\n",
      "all branching entropies was computed # words = 69634\n",
      "all accessor variety was computed # words = 69634\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 783\n",
      "all branching entropies was computed # words = 69663\n",
      "all accessor variety was computed # words = 69663\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 787\n",
      "all branching entropies was computed # words = 69863\n",
      "all accessor variety was computed # words = 69863\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 795\n",
      "all branching entropies was computed # words = 69943\n",
      "all accessor variety was computed # words = 69943\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 798\n",
      "all branching entropies was computed # words = 70020\n",
      "all accessor variety was computed # words = 70020\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 802\n",
      "all branching entropies was computed # words = 70177\n",
      "all accessor variety was computed # words = 70177\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 803\n",
      "all branching entropies was computed # words = 70337\n",
      "all accessor variety was computed # words = 70337\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 804\n",
      "all branching entropies was computed # words = 70402\n",
      "all accessor variety was computed # words = 70402\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 807\n",
      "all branching entropies was computed # words = 70533\n",
      "all accessor variety was computed # words = 70533\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 809\n",
      "all branching entropies was computed # words = 70624\n",
      "all accessor variety was computed # words = 70624\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 812\n",
      "all branching entropies was computed # words = 70978\n",
      "all accessor variety was computed # words = 70978\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 813\n",
      "all branching entropies was computed # words = 71075\n",
      "all accessor variety was computed # words = 71075\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 813\n",
      "all branching entropies was computed # words = 71099\n",
      "all accessor variety was computed # words = 71099\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 820\n",
      "all branching entropies was computed # words = 71185\n",
      "all accessor variety was computed # words = 71185\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 821\n",
      "all branching entropies was computed # words = 71418\n",
      "all accessor variety was computed # words = 71418\n",
      "'존버'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 825\n",
      "all branching entropies was computed # words = 71485\n",
      "all accessor variety was computed # words = 71485\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 826\n",
      "all branching entropies was computed # words = 71519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 71519\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 830\n",
      "all branching entropies was computed # words = 71645\n",
      "all accessor variety was computed # words = 71645\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 835\n",
      "all branching entropies was computed # words = 71723\n",
      "all accessor variety was computed # words = 71723\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 842\n",
      "all branching entropies was computed # words = 71827\n",
      "all accessor variety was computed # words = 71827\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 843\n",
      "all branching entropies was computed # words = 71929\n",
      "all accessor variety was computed # words = 71929\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 848\n",
      "all branching entropies was computed # words = 72093\n",
      "all accessor variety was computed # words = 72093\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 851\n",
      "all branching entropies was computed # words = 72180\n",
      "all accessor variety was computed # words = 72180\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 853\n",
      "all branching entropies was computed # words = 72234\n",
      "all accessor variety was computed # words = 72234\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 855\n",
      "all branching entropies was computed # words = 72285\n",
      "all accessor variety was computed # words = 72285\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 857\n",
      "all branching entropies was computed # words = 72338\n",
      "all accessor variety was computed # words = 72338\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 859\n",
      "all branching entropies was computed # words = 72471\n",
      "all accessor variety was computed # words = 72471\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 862\n",
      "all branching entropies was computed # words = 72572\n",
      "all accessor variety was computed # words = 72572\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 864\n",
      "all branching entropies was computed # words = 72654\n",
      "all accessor variety was computed # words = 72654\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 868\n",
      "all branching entropies was computed # words = 72922\n",
      "all accessor variety was computed # words = 72922\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 870\n",
      "all branching entropies was computed # words = 72959\n",
      "all accessor variety was computed # words = 72959\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 873\n",
      "all branching entropies was computed # words = 73048\n",
      "all accessor variety was computed # words = 73048\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 873\n",
      "all branching entropies was computed # words = 73110\n",
      "all accessor variety was computed # words = 73110\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 874\n",
      "all branching entropies was computed # words = 73196\n",
      "all accessor variety was computed # words = 73196\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 879\n",
      "all branching entropies was computed # words = 73238\n",
      "all accessor variety was computed # words = 73238\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 882\n",
      "all branching entropies was computed # words = 73305\n",
      "all accessor variety was computed # words = 73305\n",
      "training was done. used memory 4.407 Gb4.420 Gb\n",
      "all cohesion probabilities was computed. # words = 900\n",
      "all branching entropies was computed # words = 74966\n",
      "all accessor variety was computed # words = 74966\n",
      "'방갤'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 905\n",
      "all branching entropies was computed # words = 75060\n",
      "all accessor variety was computed # words = 75060\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 908\n",
      "all branching entropies was computed # words = 75097\n",
      "all accessor variety was computed # words = 75097\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 908\n",
      "all branching entropies was computed # words = 75142\n",
      "all accessor variety was computed # words = 75142\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 913\n",
      "all branching entropies was computed # words = 75189\n",
      "all accessor variety was computed # words = 75189\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 916\n",
      "all branching entropies was computed # words = 75232\n",
      "all accessor variety was computed # words = 75232\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 918\n",
      "all branching entropies was computed # words = 75291\n",
      "all accessor variety was computed # words = 75291\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 921\n",
      "all branching entropies was computed # words = 75358\n",
      "all accessor variety was computed # words = 75358\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 924\n",
      "all branching entropies was computed # words = 75386\n",
      "all accessor variety was computed # words = 75386\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 927\n",
      "all branching entropies was computed # words = 75446\n",
      "all accessor variety was computed # words = 75446\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 928\n",
      "all branching entropies was computed # words = 75532\n",
      "all accessor variety was computed # words = 75532\n",
      "'유튭'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 931\n",
      "all branching entropies was computed # words = 75577\n",
      "all accessor variety was computed # words = 75577\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 936\n",
      "all branching entropies was computed # words = 75649\n",
      "all accessor variety was computed # words = 75649\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 938\n",
      "all branching entropies was computed # words = 75756\n",
      "all accessor variety was computed # words = 75756\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 941\n",
      "all branching entropies was computed # words = 75817\n",
      "all accessor variety was computed # words = 75817\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 945\n",
      "all branching entropies was computed # words = 75895\n",
      "all accessor variety was computed # words = 75895\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 946\n",
      "all branching entropies was computed # words = 75956\n",
      "all accessor variety was computed # words = 75956\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 949\n",
      "all branching entropies was computed # words = 76028\n",
      "all accessor variety was computed # words = 76028\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 952\n",
      "all branching entropies was computed # words = 76084\n",
      "all accessor variety was computed # words = 76084\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 954\n",
      "all branching entropies was computed # words = 76124\n",
      "all accessor variety was computed # words = 76124\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 76214\n",
      "all accessor variety was computed # words = 76214\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 961\n",
      "all branching entropies was computed # words = 76288\n",
      "all accessor variety was computed # words = 76288\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 964\n",
      "all branching entropies was computed # words = 76342\n",
      "all accessor variety was computed # words = 76342\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 965\n",
      "all branching entropies was computed # words = 76406\n",
      "all accessor variety was computed # words = 76406\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 968\n",
      "all branching entropies was computed # words = 76460\n",
      "all accessor variety was computed # words = 76460\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 971\n",
      "all branching entropies was computed # words = 76493\n",
      "all accessor variety was computed # words = 76493\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 975\n",
      "all branching entropies was computed # words = 76539\n",
      "all accessor variety was computed # words = 76539\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 977\n",
      "all branching entropies was computed # words = 76558\n",
      "all accessor variety was computed # words = 76558\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 980\n",
      "all branching entropies was computed # words = 76617\n",
      "all accessor variety was computed # words = 76617\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 985\n",
      "all branching entropies was computed # words = 76657\n",
      "all accessor variety was computed # words = 76657\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 988\n",
      "all branching entropies was computed # words = 76677\n",
      "all accessor variety was computed # words = 76677\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 995\n",
      "all branching entropies was computed # words = 76728\n",
      "all accessor variety was computed # words = 76728\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 996\n",
      "all branching entropies was computed # words = 76770\n",
      "all accessor variety was computed # words = 76770\n",
      "'이짤'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 996\n",
      "all branching entropies was computed # words = 76786\n",
      "all accessor variety was computed # words = 76786\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 997\n",
      "all branching entropies was computed # words = 76845\n",
      "all accessor variety was computed # words = 76845\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 998\n",
      "all branching entropies was computed # words = 76909\n",
      "all accessor variety was computed # words = 76909\n",
      "'안됨'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 999\n",
      "all branching entropies was computed # words = 77013\n",
      "all accessor variety was computed # words = 77013\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1002\n",
      "all branching entropies was computed # words = 77104\n",
      "all accessor variety was computed # words = 77104\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1003\n",
      "all branching entropies was computed # words = 77202\n",
      "all accessor variety was computed # words = 77202\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1006\n",
      "all branching entropies was computed # words = 77277\n",
      "all accessor variety was computed # words = 77277\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1009\n",
      "all branching entropies was computed # words = 77476\n",
      "all accessor variety was computed # words = 77476\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1010\n",
      "all branching entropies was computed # words = 77515\n",
      "all accessor variety was computed # words = 77515\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1012\n",
      "all branching entropies was computed # words = 77675\n",
      "all accessor variety was computed # words = 77675\n",
      "'철염'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1014\n",
      "all branching entropies was computed # words = 77723\n",
      "all accessor variety was computed # words = 77723\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1017\n",
      "all branching entropies was computed # words = 77743\n",
      "all accessor variety was computed # words = 77743\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1019\n",
      "all branching entropies was computed # words = 77912\n",
      "all accessor variety was computed # words = 77912\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1022\n",
      "all branching entropies was computed # words = 78048\n",
      "all accessor variety was computed # words = 78048\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1023\n",
      "all branching entropies was computed # words = 78088\n",
      "all accessor variety was computed # words = 78088\n",
      "'푸황'\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1029\n",
      "all branching entropies was computed # words = 78170\n",
      "all accessor variety was computed # words = 78170\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1032\n",
      "all branching entropies was computed # words = 78190\n",
      "all accessor variety was computed # words = 78190\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1034\n",
      "all branching entropies was computed # words = 78218\n",
      "all accessor variety was computed # words = 78218\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1037\n",
      "all branching entropies was computed # words = 78269\n",
      "all accessor variety was computed # words = 78269\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1040\n",
      "all branching entropies was computed # words = 78339\n",
      "all accessor variety was computed # words = 78339\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1041\n",
      "all branching entropies was computed # words = 78442\n",
      "all accessor variety was computed # words = 78442\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1041\n",
      "all branching entropies was computed # words = 78459\n",
      "all accessor variety was computed # words = 78459\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1043\n",
      "all branching entropies was computed # words = 78559\n",
      "all accessor variety was computed # words = 78559\n",
      "training was done. used memory 4.407 Gb4.407 Gb\n",
      "all cohesion probabilities was computed. # words = 1045\n",
      "all branching entropies was computed # words = 78645\n",
      "all accessor variety was computed # words = 78645\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1049\n",
      "all branching entropies was computed # words = 78681\n",
      "all accessor variety was computed # words = 78681\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1050\n",
      "all branching entropies was computed # words = 78725\n",
      "all accessor variety was computed # words = 78725\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1053\n",
      "all branching entropies was computed # words = 78761\n",
      "all accessor variety was computed # words = 78761\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1055\n",
      "all branching entropies was computed # words = 78765\n",
      "all accessor variety was computed # words = 78765\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1056\n",
      "all branching entropies was computed # words = 78805\n",
      "all accessor variety was computed # words = 78805\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1059\n",
      "all branching entropies was computed # words = 78814\n",
      "all accessor variety was computed # words = 78814\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1061\n",
      "all branching entropies was computed # words = 78891\n",
      "all accessor variety was computed # words = 78891\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1063\n",
      "all branching entropies was computed # words = 78938\n",
      "all accessor variety was computed # words = 78938\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1065\n",
      "all branching entropies was computed # words = 79062\n",
      "all accessor variety was computed # words = 79062\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1066\n",
      "all branching entropies was computed # words = 79202\n",
      "all accessor variety was computed # words = 79202\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1069\n",
      "all branching entropies was computed # words = 79269\n",
      "all accessor variety was computed # words = 79269\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1070\n",
      "all branching entropies was computed # words = 79310\n",
      "all accessor variety was computed # words = 79310\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1072\n",
      "all branching entropies was computed # words = 79355\n",
      "all accessor variety was computed # words = 79355\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1074\n",
      "all branching entropies was computed # words = 79387\n",
      "all accessor variety was computed # words = 79387\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1077\n",
      "all branching entropies was computed # words = 79465\n",
      "all accessor variety was computed # words = 79465\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1080\n",
      "all branching entropies was computed # words = 79502\n",
      "all accessor variety was computed # words = 79502\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1083\n",
      "all branching entropies was computed # words = 79539\n",
      "all accessor variety was computed # words = 79539\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1084\n",
      "all branching entropies was computed # words = 79569\n",
      "all accessor variety was computed # words = 79569\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1084\n",
      "all branching entropies was computed # words = 79583\n",
      "all accessor variety was computed # words = 79583\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1086\n",
      "all branching entropies was computed # words = 79631\n",
      "all accessor variety was computed # words = 79631\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1087\n",
      "all branching entropies was computed # words = 79695\n",
      "all accessor variety was computed # words = 79695\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1088\n",
      "all branching entropies was computed # words = 79747\n",
      "all accessor variety was computed # words = 79747\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1089\n",
      "all branching entropies was computed # words = 79800\n",
      "all accessor variety was computed # words = 79800\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1094\n",
      "all branching entropies was computed # words = 79858\n",
      "all accessor variety was computed # words = 79858\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1097\n",
      "all branching entropies was computed # words = 79901\n",
      "all accessor variety was computed # words = 79901\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1098\n",
      "all branching entropies was computed # words = 79957\n",
      "all accessor variety was computed # words = 79957\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1098\n",
      "all branching entropies was computed # words = 79972\n",
      "all accessor variety was computed # words = 79972\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1101\n",
      "all branching entropies was computed # words = 79988\n",
      "all accessor variety was computed # words = 79988\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1106\n",
      "all branching entropies was computed # words = 80081\n",
      "all accessor variety was computed # words = 80081\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 80087\n",
      "all accessor variety was computed # words = 80087\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 80113\n",
      "all accessor variety was computed # words = 80113\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 80138\n",
      "all accessor variety was computed # words = 80138\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1109\n",
      "all branching entropies was computed # words = 80197\n",
      "all accessor variety was computed # words = 80197\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1112\n",
      "all branching entropies was computed # words = 80245\n",
      "all accessor variety was computed # words = 80245\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1113\n",
      "all branching entropies was computed # words = 80304\n",
      "all accessor variety was computed # words = 80304\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1120\n",
      "all branching entropies was computed # words = 80353\n",
      "all accessor variety was computed # words = 80353\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1123\n",
      "all branching entropies was computed # words = 80414\n",
      "all accessor variety was computed # words = 80414\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1124\n",
      "all branching entropies was computed # words = 80457\n",
      "all accessor variety was computed # words = 80457\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1126\n",
      "all branching entropies was computed # words = 80470\n",
      "all accessor variety was computed # words = 80470\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1127\n",
      "all branching entropies was computed # words = 80525\n",
      "all accessor variety was computed # words = 80525\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1130\n",
      "all branching entropies was computed # words = 80564\n",
      "all accessor variety was computed # words = 80564\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1132\n",
      "all branching entropies was computed # words = 80692\n",
      "all accessor variety was computed # words = 80692\n",
      "'좆목'\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1133\n",
      "all branching entropies was computed # words = 80702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 80702\n",
      "'좆나'\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1136\n",
      "all branching entropies was computed # words = 80732\n",
      "all accessor variety was computed # words = 80732\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1139\n",
      "all branching entropies was computed # words = 80771\n",
      "all accessor variety was computed # words = 80771\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1141\n",
      "all branching entropies was computed # words = 80817\n",
      "all accessor variety was computed # words = 80817\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1142\n",
      "all branching entropies was computed # words = 80830\n",
      "all accessor variety was computed # words = 80830\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1144\n",
      "all branching entropies was computed # words = 80932\n",
      "all accessor variety was computed # words = 80932\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1144\n",
      "all branching entropies was computed # words = 80951\n",
      "all accessor variety was computed # words = 80951\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1147\n",
      "all branching entropies was computed # words = 81012\n",
      "all accessor variety was computed # words = 81012\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1150\n",
      "all branching entropies was computed # words = 81053\n",
      "all accessor variety was computed # words = 81053\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1153\n",
      "all branching entropies was computed # words = 81085\n",
      "all accessor variety was computed # words = 81085\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1154\n",
      "all branching entropies was computed # words = 81134\n",
      "all accessor variety was computed # words = 81134\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1155\n",
      "all branching entropies was computed # words = 81174\n",
      "all accessor variety was computed # words = 81174\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1162\n",
      "all branching entropies was computed # words = 81227\n",
      "all accessor variety was computed # words = 81227\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1164\n",
      "all branching entropies was computed # words = 81241\n",
      "all accessor variety was computed # words = 81241\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1167\n",
      "all branching entropies was computed # words = 81314\n",
      "all accessor variety was computed # words = 81314\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1167\n",
      "all branching entropies was computed # words = 81332\n",
      "all accessor variety was computed # words = 81332\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1169\n",
      "all branching entropies was computed # words = 81380\n",
      "all accessor variety was computed # words = 81380\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1170\n",
      "all branching entropies was computed # words = 81451\n",
      "all accessor variety was computed # words = 81451\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1171\n",
      "all branching entropies was computed # words = 81509\n",
      "all accessor variety was computed # words = 81509\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1183\n",
      "all branching entropies was computed # words = 81645\n",
      "all accessor variety was computed # words = 81645\n",
      "training was done. used memory 4.398 Gb4.398 Gb\n",
      "all cohesion probabilities was computed. # words = 1185\n",
      "all branching entropies was computed # words = 81651\n",
      "all accessor variety was computed # words = 81651\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1185\n",
      "all branching entropies was computed # words = 81663\n",
      "all accessor variety was computed # words = 81663\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1186\n",
      "all branching entropies was computed # words = 81704\n",
      "all accessor variety was computed # words = 81704\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1189\n",
      "all branching entropies was computed # words = 81744\n",
      "all accessor variety was computed # words = 81744\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1191\n",
      "all branching entropies was computed # words = 81781\n",
      "all accessor variety was computed # words = 81781\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1192\n",
      "all branching entropies was computed # words = 81878\n",
      "all accessor variety was computed # words = 81878\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1194\n",
      "all branching entropies was computed # words = 81895\n",
      "all accessor variety was computed # words = 81895\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1196\n",
      "all branching entropies was computed # words = 81979\n",
      "all accessor variety was computed # words = 81979\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1198\n",
      "all branching entropies was computed # words = 81994\n",
      "all accessor variety was computed # words = 81994\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1200\n",
      "all branching entropies was computed # words = 82011\n",
      "all accessor variety was computed # words = 82011\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1203\n",
      "all branching entropies was computed # words = 82028\n",
      "all accessor variety was computed # words = 82028\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1203\n",
      "all branching entropies was computed # words = 82036\n",
      "all accessor variety was computed # words = 82036\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1204\n",
      "all branching entropies was computed # words = 82065\n",
      "all accessor variety was computed # words = 82065\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1209\n",
      "all branching entropies was computed # words = 82075\n",
      "all accessor variety was computed # words = 82075\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1211\n",
      "all branching entropies was computed # words = 82092\n",
      "all accessor variety was computed # words = 82092\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1213\n",
      "all branching entropies was computed # words = 82107\n",
      "all accessor variety was computed # words = 82107\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1215\n",
      "all branching entropies was computed # words = 82183\n",
      "all accessor variety was computed # words = 82183\n",
      "'같냐'\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1216\n",
      "all branching entropies was computed # words = 82300\n",
      "all accessor variety was computed # words = 82300\n",
      "training was done. used memory 2.862 Gb2.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1217\n",
      "all branching entropies was computed # words = 82384\n",
      "all accessor variety was computed # words = 82384\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1220\n",
      "all branching entropies was computed # words = 82424\n",
      "all accessor variety was computed # words = 82424\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 82451\n",
      "all accessor variety was computed # words = 82451\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1226\n",
      "all branching entropies was computed # words = 82496\n",
      "all accessor variety was computed # words = 82496\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1229\n",
      "all branching entropies was computed # words = 82515\n",
      "all accessor variety was computed # words = 82515\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1232\n",
      "all branching entropies was computed # words = 82528\n",
      "all accessor variety was computed # words = 82528\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1235\n",
      "all branching entropies was computed # words = 82570\n",
      "all accessor variety was computed # words = 82570\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1239\n",
      "all branching entropies was computed # words = 82650\n",
      "all accessor variety was computed # words = 82650\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1239\n",
      "all branching entropies was computed # words = 82855\n",
      "all accessor variety was computed # words = 82855\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1240\n",
      "all branching entropies was computed # words = 82860\n",
      "all accessor variety was computed # words = 82860\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1241\n",
      "all branching entropies was computed # words = 82897\n",
      "all accessor variety was computed # words = 82897\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1242\n",
      "all branching entropies was computed # words = 82936\n",
      "all accessor variety was computed # words = 82936\n",
      "'좆코'\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1245\n",
      "all branching entropies was computed # words = 82980\n",
      "all accessor variety was computed # words = 82980\n",
      "training was done. used memory 2.863 Gb2.863 Gb\n",
      "all cohesion probabilities was computed. # words = 1246\n",
      "all branching entropies was computed # words = 83023\n",
      "all accessor variety was computed # words = 83023\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1247\n",
      "all branching entropies was computed # words = 83061\n",
      "all accessor variety was computed # words = 83061\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1249\n",
      "all branching entropies was computed # words = 83102\n",
      "all accessor variety was computed # words = 83102\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1251\n",
      "all branching entropies was computed # words = 83111\n",
      "all accessor variety was computed # words = 83111\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1253\n",
      "all branching entropies was computed # words = 83154\n",
      "all accessor variety was computed # words = 83154\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1255\n",
      "all branching entropies was computed # words = 83186\n",
      "all accessor variety was computed # words = 83186\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1256\n",
      "all branching entropies was computed # words = 83232\n",
      "all accessor variety was computed # words = 83232\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1257\n",
      "all branching entropies was computed # words = 83261\n",
      "all accessor variety was computed # words = 83261\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1258\n",
      "all branching entropies was computed # words = 83327\n",
      "all accessor variety was computed # words = 83327\n",
      "'남캠'\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1261\n",
      "all branching entropies was computed # words = 83354\n",
      "all accessor variety was computed # words = 83354\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1264\n",
      "all branching entropies was computed # words = 83398\n",
      "all accessor variety was computed # words = 83398\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1267\n",
      "all branching entropies was computed # words = 83435\n",
      "all accessor variety was computed # words = 83435\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1269\n",
      "all branching entropies was computed # words = 83507\n",
      "all accessor variety was computed # words = 83507\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1272\n",
      "all branching entropies was computed # words = 83547\n",
      "all accessor variety was computed # words = 83547\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1273\n",
      "all branching entropies was computed # words = 83566\n",
      "all accessor variety was computed # words = 83566\n",
      "training was done. used memory 2.864 Gb2.875 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 83769\n",
      "all accessor variety was computed # words = 83769\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1276\n",
      "all branching entropies was computed # words = 83794\n",
      "all accessor variety was computed # words = 83794\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1280\n",
      "all branching entropies was computed # words = 83882\n",
      "all accessor variety was computed # words = 83882\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1282\n",
      "all branching entropies was computed # words = 83926\n",
      "all accessor variety was computed # words = 83926\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1283\n",
      "all branching entropies was computed # words = 83941\n",
      "all accessor variety was computed # words = 83941\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1284\n",
      "all branching entropies was computed # words = 84016\n",
      "all accessor variety was computed # words = 84016\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1286\n",
      "all branching entropies was computed # words = 84038\n",
      "all accessor variety was computed # words = 84038\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1289\n",
      "all branching entropies was computed # words = 84054\n",
      "all accessor variety was computed # words = 84054\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1292\n",
      "all branching entropies was computed # words = 84077\n",
      "all accessor variety was computed # words = 84077\n",
      "training was done. used memory 2.864 Gb2.864 Gb\n",
      "all cohesion probabilities was computed. # words = 1295\n",
      "all branching entropies was computed # words = 84139\n",
      "all accessor variety was computed # words = 84139\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1298\n",
      "all branching entropies was computed # words = 84187\n",
      "all accessor variety was computed # words = 84187\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1301\n",
      "all branching entropies was computed # words = 84200\n",
      "all accessor variety was computed # words = 84200\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1304\n",
      "all branching entropies was computed # words = 84231\n",
      "all accessor variety was computed # words = 84231\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1307\n",
      "all branching entropies was computed # words = 84277\n",
      "all accessor variety was computed # words = 84277\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1308\n",
      "all branching entropies was computed # words = 84323\n",
      "all accessor variety was computed # words = 84323\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1309\n",
      "all branching entropies was computed # words = 84386\n",
      "all accessor variety was computed # words = 84386\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1314\n",
      "all branching entropies was computed # words = 84394\n",
      "all accessor variety was computed # words = 84394\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1316\n",
      "all branching entropies was computed # words = 84419\n",
      "all accessor variety was computed # words = 84419\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1318\n",
      "all branching entropies was computed # words = 84478\n",
      "all accessor variety was computed # words = 84478\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1320\n",
      "all branching entropies was computed # words = 84507\n",
      "all accessor variety was computed # words = 84507\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1322\n",
      "all branching entropies was computed # words = 84519\n",
      "all accessor variety was computed # words = 84519\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1323\n",
      "all branching entropies was computed # words = 84563\n",
      "all accessor variety was computed # words = 84563\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1325\n",
      "all branching entropies was computed # words = 84597\n",
      "all accessor variety was computed # words = 84597\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1327\n",
      "all branching entropies was computed # words = 84623\n",
      "all accessor variety was computed # words = 84623\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1327\n",
      "all branching entropies was computed # words = 84631\n",
      "all accessor variety was computed # words = 84631\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1329\n",
      "all branching entropies was computed # words = 84696\n",
      "all accessor variety was computed # words = 84696\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1330\n",
      "all branching entropies was computed # words = 84727\n",
      "all accessor variety was computed # words = 84727\n",
      "'좆도'\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1333\n",
      "all branching entropies was computed # words = 84749\n",
      "all accessor variety was computed # words = 84749\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1335\n",
      "all branching entropies was computed # words = 84777\n",
      "all accessor variety was computed # words = 84777\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1337\n",
      "all branching entropies was computed # words = 84786\n",
      "all accessor variety was computed # words = 84786\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1338\n",
      "all branching entropies was computed # words = 84821\n",
      "all accessor variety was computed # words = 84821\n",
      "'있긴'\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1343\n",
      "all branching entropies was computed # words = 84881\n",
      "all accessor variety was computed # words = 84881\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1344\n",
      "all branching entropies was computed # words = 84925\n",
      "all accessor variety was computed # words = 84925\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1347\n",
      "all branching entropies was computed # words = 84962\n",
      "all accessor variety was computed # words = 84962\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1352\n",
      "all branching entropies was computed # words = 85000\n",
      "all accessor variety was computed # words = 85000\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1356\n",
      "all branching entropies was computed # words = 85017\n",
      "all accessor variety was computed # words = 85017\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1356\n",
      "all branching entropies was computed # words = 85021\n",
      "all accessor variety was computed # words = 85021\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1359\n",
      "all branching entropies was computed # words = 85039\n",
      "all accessor variety was computed # words = 85039\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1360\n",
      "all branching entropies was computed # words = 85063\n",
      "all accessor variety was computed # words = 85063\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1363\n",
      "all branching entropies was computed # words = 85082\n",
      "all accessor variety was computed # words = 85082\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1364\n",
      "all branching entropies was computed # words = 85137\n",
      "all accessor variety was computed # words = 85137\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1367\n",
      "all branching entropies was computed # words = 85190\n",
      "all accessor variety was computed # words = 85190\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1368\n",
      "all branching entropies was computed # words = 85202\n",
      "all accessor variety was computed # words = 85202\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1370\n",
      "all branching entropies was computed # words = 85232\n",
      "all accessor variety was computed # words = 85232\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1373\n",
      "all branching entropies was computed # words = 85241\n",
      "all accessor variety was computed # words = 85241\n",
      "training was done. used memory 1.917 Gb1.917 Gb\n",
      "all cohesion probabilities was computed. # words = 1374\n",
      "all branching entropies was computed # words = 85257\n",
      "all accessor variety was computed # words = 85257\n",
      "training was done. used memory 1.918 Gb1.940 Gb\n",
      "all cohesion probabilities was computed. # words = 1380\n",
      "all branching entropies was computed # words = 85845\n",
      "all accessor variety was computed # words = 85845\n",
      "'조새'\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1381\n",
      "all branching entropies was computed # words = 85933\n",
      "all accessor variety was computed # words = 85933\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1383\n",
      "all branching entropies was computed # words = 85950\n",
      "all accessor variety was computed # words = 85950\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1384\n",
      "all branching entropies was computed # words = 85977\n",
      "all accessor variety was computed # words = 85977\n",
      "'좋노'\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1387\n",
      "all branching entropies was computed # words = 86007\n",
      "all accessor variety was computed # words = 86007\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1389\n",
      "all branching entropies was computed # words = 86033\n",
      "all accessor variety was computed # words = 86033\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1390\n",
      "all branching entropies was computed # words = 86048\n",
      "all accessor variety was computed # words = 86048\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 86070\n",
      "all accessor variety was computed # words = 86070\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1392\n",
      "all branching entropies was computed # words = 86110\n",
      "all accessor variety was computed # words = 86110\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1393\n",
      "all branching entropies was computed # words = 86131\n",
      "all accessor variety was computed # words = 86131\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1396\n",
      "all branching entropies was computed # words = 86170\n",
      "all accessor variety was computed # words = 86170\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1399\n",
      "all branching entropies was computed # words = 86196\n",
      "all accessor variety was computed # words = 86196\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1402\n",
      "all branching entropies was computed # words = 86214\n",
      "all accessor variety was computed # words = 86214\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1408\n",
      "all branching entropies was computed # words = 86254\n",
      "all accessor variety was computed # words = 86254\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1409\n",
      "all branching entropies was computed # words = 86291\n",
      "all accessor variety was computed # words = 86291\n",
      "training was done. used memory 1.918 Gb1.928 Gb\n",
      "all cohesion probabilities was computed. # words = 1413\n",
      "all branching entropies was computed # words = 86457\n",
      "all accessor variety was computed # words = 86457\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1416\n",
      "all branching entropies was computed # words = 86485\n",
      "all accessor variety was computed # words = 86485\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1418\n",
      "all branching entropies was computed # words = 86524\n",
      "all accessor variety was computed # words = 86524\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1420\n",
      "all branching entropies was computed # words = 86538\n",
      "all accessor variety was computed # words = 86538\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1421\n",
      "all branching entropies was computed # words = 86591\n",
      "all accessor variety was computed # words = 86591\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1424\n",
      "all branching entropies was computed # words = 86633\n",
      "all accessor variety was computed # words = 86633\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1425\n",
      "all branching entropies was computed # words = 86659\n",
      "all accessor variety was computed # words = 86659\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1425\n",
      "all branching entropies was computed # words = 86744\n",
      "all accessor variety was computed # words = 86744\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1429\n",
      "all branching entropies was computed # words = 86764\n",
      "all accessor variety was computed # words = 86764\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1431\n",
      "all branching entropies was computed # words = 86793\n",
      "all accessor variety was computed # words = 86793\n",
      "training was done. used memory 1.918 Gb1.918 Gb\n",
      "all cohesion probabilities was computed. # words = 1434\n",
      "all branching entropies was computed # words = 86843\n",
      "all accessor variety was computed # words = 86843\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1434\n",
      "all branching entropies was computed # words = 86852\n",
      "all accessor variety was computed # words = 86852\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1435\n",
      "all branching entropies was computed # words = 86887\n",
      "all accessor variety was computed # words = 86887\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1437\n",
      "all branching entropies was computed # words = 86928\n",
      "all accessor variety was computed # words = 86928\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1438\n",
      "all branching entropies was computed # words = 87008\n",
      "all accessor variety was computed # words = 87008\n",
      "'좆기'\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1439\n",
      "all branching entropies was computed # words = 87089\n",
      "all accessor variety was computed # words = 87089\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1442\n",
      "all branching entropies was computed # words = 87105\n",
      "all accessor variety was computed # words = 87105\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1444\n",
      "all branching entropies was computed # words = 87120\n",
      "all accessor variety was computed # words = 87120\n",
      "training was done. used memory 1.851 Gb1.851 Gb\n",
      "all cohesion probabilities was computed. # words = 1445\n",
      "all branching entropies was computed # words = 87217\n",
      "all accessor variety was computed # words = 87217\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1447\n",
      "all branching entropies was computed # words = 87238\n",
      "all accessor variety was computed # words = 87238\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1449\n",
      "all branching entropies was computed # words = 87281\n",
      "all accessor variety was computed # words = 87281\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1452\n",
      "all branching entropies was computed # words = 87294\n",
      "all accessor variety was computed # words = 87294\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1454\n",
      "all branching entropies was computed # words = 87335\n",
      "all accessor variety was computed # words = 87335\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1457\n",
      "all branching entropies was computed # words = 87356\n",
      "all accessor variety was computed # words = 87356\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1459\n",
      "all branching entropies was computed # words = 87373\n",
      "all accessor variety was computed # words = 87373\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1462\n",
      "all branching entropies was computed # words = 87392\n",
      "all accessor variety was computed # words = 87392\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1464\n",
      "all branching entropies was computed # words = 87418\n",
      "all accessor variety was computed # words = 87418\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1465\n",
      "all branching entropies was computed # words = 87426\n",
      "all accessor variety was computed # words = 87426\n",
      "'그걸'\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1466\n",
      "all branching entropies was computed # words = 87456\n",
      "all accessor variety was computed # words = 87456\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1470\n",
      "all branching entropies was computed # words = 87465\n",
      "all accessor variety was computed # words = 87465\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1470\n",
      "all branching entropies was computed # words = 87478\n",
      "all accessor variety was computed # words = 87478\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1472\n",
      "all branching entropies was computed # words = 87484\n",
      "all accessor variety was computed # words = 87484\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1475\n",
      "all branching entropies was computed # words = 87493\n",
      "all accessor variety was computed # words = 87493\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1478\n",
      "all branching entropies was computed # words = 87506\n",
      "all accessor variety was computed # words = 87506\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1479\n",
      "all branching entropies was computed # words = 87616\n",
      "all accessor variety was computed # words = 87616\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1484\n",
      "all branching entropies was computed # words = 87631\n",
      "all accessor variety was computed # words = 87631\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1489\n",
      "all branching entropies was computed # words = 87656\n",
      "all accessor variety was computed # words = 87656\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1492\n",
      "all branching entropies was computed # words = 87668\n",
      "all accessor variety was computed # words = 87668\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1493\n",
      "all branching entropies was computed # words = 87740\n",
      "all accessor variety was computed # words = 87740\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1495\n",
      "all branching entropies was computed # words = 87755\n",
      "all accessor variety was computed # words = 87755\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1495\n",
      "all branching entropies was computed # words = 87772\n",
      "all accessor variety was computed # words = 87772\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1498\n",
      "all branching entropies was computed # words = 87787\n",
      "all accessor variety was computed # words = 87787\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1500\n",
      "all branching entropies was computed # words = 87817\n",
      "all accessor variety was computed # words = 87817\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1502\n",
      "all branching entropies was computed # words = 87821\n",
      "all accessor variety was computed # words = 87821\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1506\n",
      "all branching entropies was computed # words = 87867\n",
      "all accessor variety was computed # words = 87867\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1506\n",
      "all branching entropies was computed # words = 87869\n",
      "all accessor variety was computed # words = 87869\n",
      "training was done. used memory 1.852 Gb1.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1507\n",
      "all branching entropies was computed # words = 87913\n",
      "all accessor variety was computed # words = 87913\n",
      "'잦들'\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1508\n",
      "all branching entropies was computed # words = 87936\n",
      "all accessor variety was computed # words = 87936\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1509\n",
      "all branching entropies was computed # words = 88015\n",
      "all accessor variety was computed # words = 88015\n",
      "'진결'\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1512\n",
      "all branching entropies was computed # words = 88070\n",
      "all accessor variety was computed # words = 88070\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1512\n",
      "all branching entropies was computed # words = 88070\n",
      "all accessor variety was computed # words = 88070\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1515\n",
      "all branching entropies was computed # words = 88086\n",
      "all accessor variety was computed # words = 88086\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1518\n",
      "all branching entropies was computed # words = 88149\n",
      "all accessor variety was computed # words = 88149\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1519\n",
      "all branching entropies was computed # words = 88165\n",
      "all accessor variety was computed # words = 88165\n",
      "'염코'\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1520\n",
      "all branching entropies was computed # words = 88181\n",
      "all accessor variety was computed # words = 88181\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1525\n",
      "all branching entropies was computed # words = 88204\n",
      "all accessor variety was computed # words = 88204\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1526\n",
      "all branching entropies was computed # words = 88221\n",
      "all accessor variety was computed # words = 88221\n",
      "'맞긴'\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1529\n",
      "all branching entropies was computed # words = 88235\n",
      "all accessor variety was computed # words = 88235\n",
      "training was done. used memory 1.852 Gb1.862 Gb\n",
      "all cohesion probabilities was computed. # words = 1530\n",
      "all branching entropies was computed # words = 88295\n",
      "all accessor variety was computed # words = 88295\n",
      "training was done. used memory 1.852 Gb1.852 Gb\n",
      "all cohesion probabilities was computed. # words = 1533\n",
      "all branching entropies was computed # words = 88316\n",
      "all accessor variety was computed # words = 88316\n",
      "training was done. used memory 1.853 Gb1.853 Gb\n",
      "all cohesion probabilities was computed. # words = 1535\n",
      "all branching entropies was computed # words = 88317\n",
      "all accessor variety was computed # words = 88317\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1538\n",
      "all branching entropies was computed # words = 88330\n",
      "all accessor variety was computed # words = 88330\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1539\n",
      "all branching entropies was computed # words = 88354\n",
      "all accessor variety was computed # words = 88354\n",
      "'망왑'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1539\n",
      "all branching entropies was computed # words = 88359\n",
      "all accessor variety was computed # words = 88359\n",
      "training was done. used memory 1.517 Gb1.526 Gb\n",
      "all cohesion probabilities was computed. # words = 1540\n",
      "all branching entropies was computed # words = 88431\n",
      "all accessor variety was computed # words = 88431\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1541\n",
      "all branching entropies was computed # words = 88436\n",
      "all accessor variety was computed # words = 88436\n",
      "'망함'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1544\n",
      "all branching entropies was computed # words = 88459\n",
      "all accessor variety was computed # words = 88459\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1547\n",
      "all branching entropies was computed # words = 88468\n",
      "all accessor variety was computed # words = 88468\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1549\n",
      "all branching entropies was computed # words = 88495\n",
      "all accessor variety was computed # words = 88495\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1550\n",
      "all branching entropies was computed # words = 88537\n",
      "all accessor variety was computed # words = 88537\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1551\n",
      "all branching entropies was computed # words = 88562\n",
      "all accessor variety was computed # words = 88562\n",
      "'유툽'\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 88586\n",
      "all accessor variety was computed # words = 88586\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1553\n",
      "all branching entropies was computed # words = 88620\n",
      "all accessor variety was computed # words = 88620\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1555\n",
      "all branching entropies was computed # words = 88635\n",
      "all accessor variety was computed # words = 88635\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1557\n",
      "all branching entropies was computed # words = 88650\n",
      "all accessor variety was computed # words = 88650\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1559\n",
      "all branching entropies was computed # words = 88689\n",
      "all accessor variety was computed # words = 88689\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1562\n",
      "all branching entropies was computed # words = 88738\n",
      "all accessor variety was computed # words = 88738\n",
      "'좆구'\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1564\n",
      "all branching entropies was computed # words = 88763\n",
      "all accessor variety was computed # words = 88763\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1571\n",
      "all branching entropies was computed # words = 88791\n",
      "all accessor variety was computed # words = 88791\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1574\n",
      "all branching entropies was computed # words = 88827\n",
      "all accessor variety was computed # words = 88827\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1574\n",
      "all branching entropies was computed # words = 88829\n",
      "all accessor variety was computed # words = 88829\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1575\n",
      "all branching entropies was computed # words = 88853\n",
      "all accessor variety was computed # words = 88853\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1575\n",
      "all branching entropies was computed # words = 88857\n",
      "all accessor variety was computed # words = 88857\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1577\n",
      "all branching entropies was computed # words = 88864\n",
      "all accessor variety was computed # words = 88864\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1578\n",
      "all branching entropies was computed # words = 88871\n",
      "all accessor variety was computed # words = 88871\n",
      "'왜캐'\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1580\n",
      "all branching entropies was computed # words = 88892\n",
      "all accessor variety was computed # words = 88892\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1581\n",
      "all branching entropies was computed # words = 88917\n",
      "all accessor variety was computed # words = 88917\n",
      "training was done. used memory 1.517 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1583\n",
      "all branching entropies was computed # words = 88933\n",
      "all accessor variety was computed # words = 88933\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1586\n",
      "all branching entropies was computed # words = 88967\n",
      "all accessor variety was computed # words = 88967\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1587\n",
      "all branching entropies was computed # words = 88997\n",
      "all accessor variety was computed # words = 88997\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1588\n",
      "all branching entropies was computed # words = 89021\n",
      "all accessor variety was computed # words = 89021\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1590\n",
      "all branching entropies was computed # words = 89063\n",
      "all accessor variety was computed # words = 89063\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1592\n",
      "all branching entropies was computed # words = 89084\n",
      "all accessor variety was computed # words = 89084\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1594\n",
      "all branching entropies was computed # words = 89103\n",
      "all accessor variety was computed # words = 89103\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1597\n",
      "all branching entropies was computed # words = 89111\n",
      "all accessor variety was computed # words = 89111\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1599\n",
      "all branching entropies was computed # words = 89136\n",
      "all accessor variety was computed # words = 89136\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1600\n",
      "all branching entropies was computed # words = 89155\n",
      "all accessor variety was computed # words = 89155\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1604\n",
      "all branching entropies was computed # words = 89187\n",
      "all accessor variety was computed # words = 89187\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1604\n",
      "all branching entropies was computed # words = 89199\n",
      "all accessor variety was computed # words = 89199\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1608\n",
      "all branching entropies was computed # words = 89202\n",
      "all accessor variety was computed # words = 89202\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1610\n",
      "all branching entropies was computed # words = 89227\n",
      "all accessor variety was computed # words = 89227\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1613\n",
      "all branching entropies was computed # words = 89247\n",
      "all accessor variety was computed # words = 89247\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1615\n",
      "all branching entropies was computed # words = 89274\n",
      "all accessor variety was computed # words = 89274\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1617\n",
      "all branching entropies was computed # words = 89294\n",
      "all accessor variety was computed # words = 89294\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1619\n",
      "all branching entropies was computed # words = 89321\n",
      "all accessor variety was computed # words = 89321\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1622\n",
      "all branching entropies was computed # words = 89366\n",
      "all accessor variety was computed # words = 89366\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1625\n",
      "all branching entropies was computed # words = 89388\n",
      "all accessor variety was computed # words = 89388\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1626\n",
      "all branching entropies was computed # words = 89405\n",
      "all accessor variety was computed # words = 89405\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1629\n",
      "all branching entropies was computed # words = 89413\n",
      "all accessor variety was computed # words = 89413\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1630\n",
      "all branching entropies was computed # words = 89440\n",
      "all accessor variety was computed # words = 89440\n",
      "'머마'\n",
      "training was done. used memory 1.527 Gb1.527 Gb\n",
      "all cohesion probabilities was computed. # words = 1631\n",
      "all branching entropies was computed # words = 89474\n",
      "all accessor variety was computed # words = 89474\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1632\n",
      "all branching entropies was computed # words = 89548\n",
      "all accessor variety was computed # words = 89548\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1633\n",
      "all branching entropies was computed # words = 89558\n",
      "all accessor variety was computed # words = 89558\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1636\n",
      "all branching entropies was computed # words = 89576\n",
      "all accessor variety was computed # words = 89576\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1640\n",
      "all branching entropies was computed # words = 89593\n",
      "all accessor variety was computed # words = 89593\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1643\n",
      "all branching entropies was computed # words = 89663\n",
      "all accessor variety was computed # words = 89663\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1645\n",
      "all branching entropies was computed # words = 89677\n",
      "all accessor variety was computed # words = 89677\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1647\n",
      "all branching entropies was computed # words = 89691\n",
      "all accessor variety was computed # words = 89691\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1649\n",
      "all branching entropies was computed # words = 89728\n",
      "all accessor variety was computed # words = 89728\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1651\n",
      "all branching entropies was computed # words = 89798\n",
      "all accessor variety was computed # words = 89798\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1651\n",
      "all branching entropies was computed # words = 89799\n",
      "all accessor variety was computed # words = 89799\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1651\n",
      "all branching entropies was computed # words = 89811\n",
      "all accessor variety was computed # words = 89811\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1655\n",
      "all branching entropies was computed # words = 89815\n",
      "all accessor variety was computed # words = 89815\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1659\n",
      "all branching entropies was computed # words = 89897\n",
      "all accessor variety was computed # words = 89897\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1661\n",
      "all branching entropies was computed # words = 89922\n",
      "all accessor variety was computed # words = 89922\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1664\n",
      "all branching entropies was computed # words = 89938\n",
      "all accessor variety was computed # words = 89938\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1665\n",
      "all branching entropies was computed # words = 89962\n",
      "all accessor variety was computed # words = 89962\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1666\n",
      "all branching entropies was computed # words = 89981\n",
      "all accessor variety was computed # words = 89981\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1667\n",
      "all branching entropies was computed # words = 89984\n",
      "all accessor variety was computed # words = 89984\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1668\n",
      "all branching entropies was computed # words = 90028\n",
      "all accessor variety was computed # words = 90028\n",
      "'쏘랑'\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1669\n",
      "all branching entropies was computed # words = 90094\n",
      "all accessor variety was computed # words = 90094\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1674\n",
      "all branching entropies was computed # words = 90112\n",
      "all accessor variety was computed # words = 90112\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1675\n",
      "all branching entropies was computed # words = 90133\n",
      "all accessor variety was computed # words = 90133\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1675\n",
      "all branching entropies was computed # words = 90135\n",
      "all accessor variety was computed # words = 90135\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1678\n",
      "all branching entropies was computed # words = 90141\n",
      "all accessor variety was computed # words = 90141\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1681\n",
      "all branching entropies was computed # words = 90155\n",
      "all accessor variety was computed # words = 90155\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1683\n",
      "all branching entropies was computed # words = 90170\n",
      "all accessor variety was computed # words = 90170\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1685\n",
      "all branching entropies was computed # words = 90212\n",
      "all accessor variety was computed # words = 90212\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1685\n",
      "all branching entropies was computed # words = 90215\n",
      "all accessor variety was computed # words = 90215\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1686\n",
      "all branching entropies was computed # words = 90232\n",
      "all accessor variety was computed # words = 90232\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1686\n",
      "all branching entropies was computed # words = 90234\n",
      "all accessor variety was computed # words = 90234\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1689\n",
      "all branching entropies was computed # words = 90246\n",
      "all accessor variety was computed # words = 90246\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1692\n",
      "all branching entropies was computed # words = 90287\n",
      "all accessor variety was computed # words = 90287\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1692\n",
      "all branching entropies was computed # words = 90297\n",
      "all accessor variety was computed # words = 90297\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1694\n",
      "all branching entropies was computed # words = 90360\n",
      "all accessor variety was computed # words = 90360\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1697\n",
      "all branching entropies was computed # words = 90360\n",
      "all accessor variety was computed # words = 90360\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1700\n",
      "all branching entropies was computed # words = 90396\n",
      "all accessor variety was computed # words = 90396\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1702\n",
      "all branching entropies was computed # words = 90409\n",
      "all accessor variety was computed # words = 90409\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1705\n",
      "all branching entropies was computed # words = 90409\n",
      "all accessor variety was computed # words = 90409\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1706\n",
      "all branching entropies was computed # words = 90422\n",
      "all accessor variety was computed # words = 90422\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1708\n",
      "all branching entropies was computed # words = 90439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 90439\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1709\n",
      "all branching entropies was computed # words = 90465\n",
      "all accessor variety was computed # words = 90465\n",
      "training was done. used memory 1.511 Gb1.511 Gb\n",
      "all cohesion probabilities was computed. # words = 1711\n",
      "all branching entropies was computed # words = 90537\n",
      "all accessor variety was computed # words = 90537\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1712\n",
      "all branching entropies was computed # words = 90566\n",
      "all accessor variety was computed # words = 90566\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1712\n",
      "all branching entropies was computed # words = 90577\n",
      "all accessor variety was computed # words = 90577\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1713\n",
      "all branching entropies was computed # words = 90620\n",
      "all accessor variety was computed # words = 90620\n",
      "'퇴갤'\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1714\n",
      "all branching entropies was computed # words = 90643\n",
      "all accessor variety was computed # words = 90643\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1717\n",
      "all branching entropies was computed # words = 90653\n",
      "all accessor variety was computed # words = 90653\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1717\n",
      "all branching entropies was computed # words = 90666\n",
      "all accessor variety was computed # words = 90666\n",
      "training was done. used memory 1.512 Gb1.512 Gb\n",
      "all cohesion probabilities was computed. # words = 1718\n",
      "all branching entropies was computed # words = 90689\n",
      "all accessor variety was computed # words = 90689\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1718\n",
      "all branching entropies was computed # words = 90696\n",
      "all accessor variety was computed # words = 90696\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1720\n",
      "all branching entropies was computed # words = 90717\n",
      "all accessor variety was computed # words = 90717\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1723\n",
      "all branching entropies was computed # words = 90777\n",
      "all accessor variety was computed # words = 90777\n",
      "'철빡'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1724\n",
      "all branching entropies was computed # words = 90793\n",
      "all accessor variety was computed # words = 90793\n",
      "'좋긴'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1725\n",
      "all branching entropies was computed # words = 90824\n",
      "all accessor variety was computed # words = 90824\n",
      "'까면'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1727\n",
      "all branching entropies was computed # words = 90851\n",
      "all accessor variety was computed # words = 90851\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1730\n",
      "all branching entropies was computed # words = 90873\n",
      "all accessor variety was computed # words = 90873\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1732\n",
      "all branching entropies was computed # words = 90894\n",
      "all accessor variety was computed # words = 90894\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1734\n",
      "all branching entropies was computed # words = 90919\n",
      "all accessor variety was computed # words = 90919\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1736\n",
      "all branching entropies was computed # words = 90941\n",
      "all accessor variety was computed # words = 90941\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1739\n",
      "all branching entropies was computed # words = 90960\n",
      "all accessor variety was computed # words = 90960\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 90961\n",
      "all accessor variety was computed # words = 90961\n",
      "'저련'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1740\n",
      "all branching entropies was computed # words = 90972\n",
      "all accessor variety was computed # words = 90972\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1741\n",
      "all branching entropies was computed # words = 90997\n",
      "all accessor variety was computed # words = 90997\n",
      "'없긴'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1746\n",
      "all branching entropies was computed # words = 91004\n",
      "all accessor variety was computed # words = 91004\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1748\n",
      "all branching entropies was computed # words = 91040\n",
      "all accessor variety was computed # words = 91040\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1751\n",
      "all branching entropies was computed # words = 91055\n",
      "all accessor variety was computed # words = 91055\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1752\n",
      "all branching entropies was computed # words = 91073\n",
      "all accessor variety was computed # words = 91073\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1754\n",
      "all branching entropies was computed # words = 91082\n",
      "all accessor variety was computed # words = 91082\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1762\n",
      "all branching entropies was computed # words = 91107\n",
      "all accessor variety was computed # words = 91107\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1763\n",
      "all branching entropies was computed # words = 91135\n",
      "all accessor variety was computed # words = 91135\n",
      "'민즉'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1763\n",
      "all branching entropies was computed # words = 91145\n",
      "all accessor variety was computed # words = 91145\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1765\n",
      "all branching entropies was computed # words = 91170\n",
      "all accessor variety was computed # words = 91170\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1766\n",
      "all branching entropies was computed # words = 91175\n",
      "all accessor variety was computed # words = 91175\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1767\n",
      "all branching entropies was computed # words = 91204\n",
      "all accessor variety was computed # words = 91204\n",
      "'토황'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1769\n",
      "all branching entropies was computed # words = 91223\n",
      "all accessor variety was computed # words = 91223\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1770\n",
      "all branching entropies was computed # words = 91257\n",
      "all accessor variety was computed # words = 91257\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1772\n",
      "all branching entropies was computed # words = 91276\n",
      "all accessor variety was computed # words = 91276\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1778\n",
      "all branching entropies was computed # words = 91278\n",
      "all accessor variety was computed # words = 91278\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1779\n",
      "all branching entropies was computed # words = 91329\n",
      "all accessor variety was computed # words = 91329\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1780\n",
      "all branching entropies was computed # words = 91340\n",
      "all accessor variety was computed # words = 91340\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1782\n",
      "all branching entropies was computed # words = 91370\n",
      "all accessor variety was computed # words = 91370\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1783\n",
      "all branching entropies was computed # words = 91393\n",
      "all accessor variety was computed # words = 91393\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1783\n",
      "all branching entropies was computed # words = 91397\n",
      "all accessor variety was computed # words = 91397\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1786\n",
      "all branching entropies was computed # words = 91436\n",
      "all accessor variety was computed # words = 91436\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 91445\n",
      "all accessor variety was computed # words = 91445\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1787\n",
      "all branching entropies was computed # words = 91453\n",
      "all accessor variety was computed # words = 91453\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1788\n",
      "all branching entropies was computed # words = 91483\n",
      "all accessor variety was computed # words = 91483\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1790\n",
      "all branching entropies was computed # words = 91497\n",
      "all accessor variety was computed # words = 91497\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1791\n",
      "all branching entropies was computed # words = 91510\n",
      "all accessor variety was computed # words = 91510\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1792\n",
      "all branching entropies was computed # words = 91527\n",
      "all accessor variety was computed # words = 91527\n",
      "'느꿈'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1793\n",
      "all branching entropies was computed # words = 91560\n",
      "all accessor variety was computed # words = 91560\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1796\n",
      "all branching entropies was computed # words = 91579\n",
      "all accessor variety was computed # words = 91579\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1798\n",
      "all branching entropies was computed # words = 91604\n",
      "all accessor variety was computed # words = 91604\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1798\n",
      "all branching entropies was computed # words = 91695\n",
      "all accessor variety was computed # words = 91695\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1800\n",
      "all branching entropies was computed # words = 91729\n",
      "all accessor variety was computed # words = 91729\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1801\n",
      "all branching entropies was computed # words = 91744\n",
      "all accessor variety was computed # words = 91744\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1804\n",
      "all branching entropies was computed # words = 91767\n",
      "all accessor variety was computed # words = 91767\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1806\n",
      "all branching entropies was computed # words = 91790\n",
      "all accessor variety was computed # words = 91790\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1808\n",
      "all branching entropies was computed # words = 91811\n",
      "all accessor variety was computed # words = 91811\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1810\n",
      "all branching entropies was computed # words = 91827\n",
      "all accessor variety was computed # words = 91827\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1816\n",
      "all branching entropies was computed # words = 92017\n",
      "all accessor variety was computed # words = 92017\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1820\n",
      "all branching entropies was computed # words = 92026\n",
      "all accessor variety was computed # words = 92026\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1821\n",
      "all branching entropies was computed # words = 92036\n",
      "all accessor variety was computed # words = 92036\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1822\n",
      "all branching entropies was computed # words = 92052\n",
      "all accessor variety was computed # words = 92052\n",
      "'머효'\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1824\n",
      "all branching entropies was computed # words = 92069\n",
      "all accessor variety was computed # words = 92069\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1825\n",
      "all branching entropies was computed # words = 92101\n",
      "all accessor variety was computed # words = 92101\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1826\n",
      "all branching entropies was computed # words = 92125\n",
      "all accessor variety was computed # words = 92125\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1830\n",
      "all branching entropies was computed # words = 92146\n",
      "all accessor variety was computed # words = 92146\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1830\n",
      "all branching entropies was computed # words = 92147\n",
      "all accessor variety was computed # words = 92147\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1830\n",
      "all branching entropies was computed # words = 92153\n",
      "all accessor variety was computed # words = 92153\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1832\n",
      "all branching entropies was computed # words = 92158\n",
      "all accessor variety was computed # words = 92158\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1834\n",
      "all branching entropies was computed # words = 92191\n",
      "all accessor variety was computed # words = 92191\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1836\n",
      "all branching entropies was computed # words = 92196\n",
      "all accessor variety was computed # words = 92196\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1839\n",
      "all branching entropies was computed # words = 92210\n",
      "all accessor variety was computed # words = 92210\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1841\n",
      "all branching entropies was computed # words = 92216\n",
      "all accessor variety was computed # words = 92216\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1844\n",
      "all branching entropies was computed # words = 92238\n",
      "all accessor variety was computed # words = 92238\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1846\n",
      "all branching entropies was computed # words = 92254\n",
      "all accessor variety was computed # words = 92254\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1847\n",
      "all branching entropies was computed # words = 92265\n",
      "all accessor variety was computed # words = 92265\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1849\n",
      "all branching entropies was computed # words = 92277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 92277\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1854\n",
      "all branching entropies was computed # words = 92292\n",
      "all accessor variety was computed # words = 92292\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1856\n",
      "all branching entropies was computed # words = 92304\n",
      "all accessor variety was computed # words = 92304\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1858\n",
      "all branching entropies was computed # words = 92320\n",
      "all accessor variety was computed # words = 92320\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1859\n",
      "all branching entropies was computed # words = 92340\n",
      "all accessor variety was computed # words = 92340\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1860\n",
      "all branching entropies was computed # words = 92340\n",
      "all accessor variety was computed # words = 92340\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1861\n",
      "all branching entropies was computed # words = 92423\n",
      "all accessor variety was computed # words = 92423\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1863\n",
      "all branching entropies was computed # words = 92429\n",
      "all accessor variety was computed # words = 92429\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1865\n",
      "all branching entropies was computed # words = 92451\n",
      "all accessor variety was computed # words = 92451\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1865\n",
      "all branching entropies was computed # words = 92452\n",
      "all accessor variety was computed # words = 92452\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1867\n",
      "all branching entropies was computed # words = 92470\n",
      "all accessor variety was computed # words = 92470\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1869\n",
      "all branching entropies was computed # words = 92479\n",
      "all accessor variety was computed # words = 92479\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1870\n",
      "all branching entropies was computed # words = 92498\n",
      "all accessor variety was computed # words = 92498\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1871\n",
      "all branching entropies was computed # words = 92514\n",
      "all accessor variety was computed # words = 92514\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1874\n",
      "all branching entropies was computed # words = 92525\n",
      "all accessor variety was computed # words = 92525\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1876\n",
      "all branching entropies was computed # words = 92539\n",
      "all accessor variety was computed # words = 92539\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1877\n",
      "all branching entropies was computed # words = 92558\n",
      "all accessor variety was computed # words = 92558\n",
      "'개엑'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1877\n",
      "all branching entropies was computed # words = 92566\n",
      "all accessor variety was computed # words = 92566\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1879\n",
      "all branching entropies was computed # words = 92579\n",
      "all accessor variety was computed # words = 92579\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1880\n",
      "all branching entropies was computed # words = 92595\n",
      "all accessor variety was computed # words = 92595\n",
      "'좆별'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1882\n",
      "all branching entropies was computed # words = 92602\n",
      "all accessor variety was computed # words = 92602\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1884\n",
      "all branching entropies was computed # words = 92624\n",
      "all accessor variety was computed # words = 92624\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1885\n",
      "all branching entropies was computed # words = 92635\n",
      "all accessor variety was computed # words = 92635\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1887\n",
      "all branching entropies was computed # words = 92643\n",
      "all accessor variety was computed # words = 92643\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1887\n",
      "all branching entropies was computed # words = 92644\n",
      "all accessor variety was computed # words = 92644\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1889\n",
      "all branching entropies was computed # words = 92663\n",
      "all accessor variety was computed # words = 92663\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1891\n",
      "all branching entropies was computed # words = 92699\n",
      "all accessor variety was computed # words = 92699\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1894\n",
      "all branching entropies was computed # words = 92711\n",
      "all accessor variety was computed # words = 92711\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1895\n",
      "all branching entropies was computed # words = 92754\n",
      "all accessor variety was computed # words = 92754\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1897\n",
      "all branching entropies was computed # words = 92789\n",
      "all accessor variety was computed # words = 92789\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1901\n",
      "all branching entropies was computed # words = 92805\n",
      "all accessor variety was computed # words = 92805\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1906\n",
      "all branching entropies was computed # words = 92839\n",
      "all accessor variety was computed # words = 92839\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1909\n",
      "all branching entropies was computed # words = 92858\n",
      "all accessor variety was computed # words = 92858\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1910\n",
      "all branching entropies was computed # words = 92889\n",
      "all accessor variety was computed # words = 92889\n",
      "'같긴'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1911\n",
      "all branching entropies was computed # words = 92918\n",
      "all accessor variety was computed # words = 92918\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1913\n",
      "all branching entropies was computed # words = 92930\n",
      "all accessor variety was computed # words = 92930\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1915\n",
      "all branching entropies was computed # words = 92942\n",
      "all accessor variety was computed # words = 92942\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1917\n",
      "all branching entropies was computed # words = 92990\n",
      "all accessor variety was computed # words = 92990\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1918\n",
      "all branching entropies was computed # words = 93062\n",
      "all accessor variety was computed # words = 93062\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1919\n",
      "all branching entropies was computed # words = 93073\n",
      "all accessor variety was computed # words = 93073\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 93087\n",
      "all accessor variety was computed # words = 93087\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1921\n",
      "all branching entropies was computed # words = 93114\n",
      "all accessor variety was computed # words = 93114\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1923\n",
      "all branching entropies was computed # words = 93127\n",
      "all accessor variety was computed # words = 93127\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1926\n",
      "all branching entropies was computed # words = 93169\n",
      "all accessor variety was computed # words = 93169\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1929\n",
      "all branching entropies was computed # words = 93197\n",
      "all accessor variety was computed # words = 93197\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1931\n",
      "all branching entropies was computed # words = 93202\n",
      "all accessor variety was computed # words = 93202\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1933\n",
      "all branching entropies was computed # words = 93236\n",
      "all accessor variety was computed # words = 93236\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 1934\n",
      "all branching entropies was computed # words = 93248\n",
      "all accessor variety was computed # words = 93248\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1936\n",
      "all branching entropies was computed # words = 93252\n",
      "all accessor variety was computed # words = 93252\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1937\n",
      "all branching entropies was computed # words = 93285\n",
      "all accessor variety was computed # words = 93285\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1938\n",
      "all branching entropies was computed # words = 93319\n",
      "all accessor variety was computed # words = 93319\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1939\n",
      "all branching entropies was computed # words = 93358\n",
      "all accessor variety was computed # words = 93358\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1941\n",
      "all branching entropies was computed # words = 93381\n",
      "all accessor variety was computed # words = 93381\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1943\n",
      "all branching entropies was computed # words = 93412\n",
      "all accessor variety was computed # words = 93412\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1944\n",
      "all branching entropies was computed # words = 93434\n",
      "all accessor variety was computed # words = 93434\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1945\n",
      "all branching entropies was computed # words = 93438\n",
      "all accessor variety was computed # words = 93438\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1946\n",
      "all branching entropies was computed # words = 93457\n",
      "all accessor variety was computed # words = 93457\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1948\n",
      "all branching entropies was computed # words = 93487\n",
      "all accessor variety was computed # words = 93487\n",
      "'좆간'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1949\n",
      "all branching entropies was computed # words = 93506\n",
      "all accessor variety was computed # words = 93506\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1950\n",
      "all branching entropies was computed # words = 93522\n",
      "all accessor variety was computed # words = 93522\n",
      "'롤체'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1951\n",
      "all branching entropies was computed # words = 93551\n",
      "all accessor variety was computed # words = 93551\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1954\n",
      "all branching entropies was computed # words = 93577\n",
      "all accessor variety was computed # words = 93577\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1954\n",
      "all branching entropies was computed # words = 93582\n",
      "all accessor variety was computed # words = 93582\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 93595\n",
      "all accessor variety was computed # words = 93595\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1957\n",
      "all branching entropies was computed # words = 93599\n",
      "all accessor variety was computed # words = 93599\n",
      "'개역겹네'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 1958\n",
      "all branching entropies was computed # words = 93620\n",
      "all accessor variety was computed # words = 93620\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1961\n",
      "all branching entropies was computed # words = 93642\n",
      "all accessor variety was computed # words = 93642\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1962\n",
      "all branching entropies was computed # words = 93690\n",
      "all accessor variety was computed # words = 93690\n",
      "'까고'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1964\n",
      "all branching entropies was computed # words = 93705\n",
      "all accessor variety was computed # words = 93705\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1966\n",
      "all branching entropies was computed # words = 93710\n",
      "all accessor variety was computed # words = 93710\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1969\n",
      "all branching entropies was computed # words = 93729\n",
      "all accessor variety was computed # words = 93729\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1971\n",
      "all branching entropies was computed # words = 93742\n",
      "all accessor variety was computed # words = 93742\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1971\n",
      "all branching entropies was computed # words = 93755\n",
      "all accessor variety was computed # words = 93755\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1971\n",
      "all branching entropies was computed # words = 93806\n",
      "all accessor variety was computed # words = 93806\n",
      "'보지코'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1973\n",
      "all branching entropies was computed # words = 93830\n",
      "all accessor variety was computed # words = 93830\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1973\n",
      "all branching entropies was computed # words = 93842\n",
      "all accessor variety was computed # words = 93842\n",
      "'이쁨'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1974\n",
      "all branching entropies was computed # words = 93858\n",
      "all accessor variety was computed # words = 93858\n",
      "'빼면'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1976\n",
      "all branching entropies was computed # words = 93874\n",
      "all accessor variety was computed # words = 93874\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1979\n",
      "all branching entropies was computed # words = 93891\n",
      "all accessor variety was computed # words = 93891\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1983\n",
      "all branching entropies was computed # words = 93894\n",
      "all accessor variety was computed # words = 93894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1984\n",
      "all branching entropies was computed # words = 93947\n",
      "all accessor variety was computed # words = 93947\n",
      "'존잘'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1986\n",
      "all branching entropies was computed # words = 93960\n",
      "all accessor variety was computed # words = 93960\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1987\n",
      "all branching entropies was computed # words = 93970\n",
      "all accessor variety was computed # words = 93970\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1989\n",
      "all branching entropies was computed # words = 93988\n",
      "all accessor variety was computed # words = 93988\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1991\n",
      "all branching entropies was computed # words = 94008\n",
      "all accessor variety was computed # words = 94008\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1991\n",
      "all branching entropies was computed # words = 94021\n",
      "all accessor variety was computed # words = 94021\n",
      "'임앵웅'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1993\n",
      "all branching entropies was computed # words = 94033\n",
      "all accessor variety was computed # words = 94033\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1994\n",
      "all branching entropies was computed # words = 94044\n",
      "all accessor variety was computed # words = 94044\n",
      "'머식'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1995\n",
      "all branching entropies was computed # words = 94060\n",
      "all accessor variety was computed # words = 94060\n",
      "'쏘갈'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1995\n",
      "all branching entropies was computed # words = 94071\n",
      "all accessor variety was computed # words = 94071\n",
      "'안하는데'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1997\n",
      "all branching entropies was computed # words = 94081\n",
      "all accessor variety was computed # words = 94081\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1997\n",
      "all branching entropies was computed # words = 94095\n",
      "all accessor variety was computed # words = 94095\n",
      "'손정우'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 1998\n",
      "all branching entropies was computed # words = 94120\n",
      "all accessor variety was computed # words = 94120\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 1999\n",
      "all branching entropies was computed # words = 94187\n",
      "all accessor variety was computed # words = 94187\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2002\n",
      "all branching entropies was computed # words = 94215\n",
      "all accessor variety was computed # words = 94215\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2005\n",
      "all branching entropies was computed # words = 94233\n",
      "all accessor variety was computed # words = 94233\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2007\n",
      "all branching entropies was computed # words = 94251\n",
      "all accessor variety was computed # words = 94251\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2007\n",
      "all branching entropies was computed # words = 94251\n",
      "all accessor variety was computed # words = 94251\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2009\n",
      "all branching entropies was computed # words = 94259\n",
      "all accessor variety was computed # words = 94259\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2010\n",
      "all branching entropies was computed # words = 94271\n",
      "all accessor variety was computed # words = 94271\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2013\n",
      "all branching entropies was computed # words = 94292\n",
      "all accessor variety was computed # words = 94292\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2015\n",
      "all branching entropies was computed # words = 94313\n",
      "all accessor variety was computed # words = 94313\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2017\n",
      "all branching entropies was computed # words = 94318\n",
      "all accessor variety was computed # words = 94318\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2019\n",
      "all branching entropies was computed # words = 94349\n",
      "all accessor variety was computed # words = 94349\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2020\n",
      "all branching entropies was computed # words = 94378\n",
      "all accessor variety was computed # words = 94378\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2020\n",
      "all branching entropies was computed # words = 94401\n",
      "all accessor variety was computed # words = 94401\n",
      "'대기방'\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2022\n",
      "all branching entropies was computed # words = 94429\n",
      "all accessor variety was computed # words = 94429\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2024\n",
      "all branching entropies was computed # words = 94440\n",
      "all accessor variety was computed # words = 94440\n",
      "training was done. used memory 1.508 Gb1.508 Gb\n",
      "all cohesion probabilities was computed. # words = 2026\n",
      "all branching entropies was computed # words = 94448\n",
      "all accessor variety was computed # words = 94448\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2027\n",
      "all branching entropies was computed # words = 94452\n",
      "all accessor variety was computed # words = 94452\n",
      "'느낀게'\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2028\n",
      "all branching entropies was computed # words = 94466\n",
      "all accessor variety was computed # words = 94466\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2028\n",
      "all branching entropies was computed # words = 94467\n",
      "all accessor variety was computed # words = 94467\n",
      "'이시간에'\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2029\n",
      "all branching entropies was computed # words = 94468\n",
      "all accessor variety was computed # words = 94468\n",
      "'재미없네'\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2031\n",
      "all branching entropies was computed # words = 94478\n",
      "all accessor variety was computed # words = 94478\n",
      "training was done. used memory 1.505 Gb1.505 Gb\n",
      "all cohesion probabilities was computed. # words = 2033\n",
      "all branching entropies was computed # words = 94487\n",
      "all accessor variety was computed # words = 94487\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2037\n",
      "all branching entropies was computed # words = 94494\n",
      "all accessor variety was computed # words = 94494\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2039\n",
      "all branching entropies was computed # words = 94502\n",
      "all accessor variety was computed # words = 94502\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2040\n",
      "all branching entropies was computed # words = 94511\n",
      "all accessor variety was computed # words = 94511\n",
      "'안한다고'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2041\n",
      "all branching entropies was computed # words = 94559\n",
      "all accessor variety was computed # words = 94559\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 94566\n",
      "all accessor variety was computed # words = 94566\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2044\n",
      "all branching entropies was computed # words = 94581\n",
      "all accessor variety was computed # words = 94581\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2046\n",
      "all branching entropies was computed # words = 94599\n",
      "all accessor variety was computed # words = 94599\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2048\n",
      "all branching entropies was computed # words = 94646\n",
      "all accessor variety was computed # words = 94646\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2049\n",
      "all branching entropies was computed # words = 94661\n",
      "all accessor variety was computed # words = 94661\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2051\n",
      "all branching entropies was computed # words = 94673\n",
      "all accessor variety was computed # words = 94673\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2052\n",
      "all branching entropies was computed # words = 94682\n",
      "all accessor variety was computed # words = 94682\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2054\n",
      "all branching entropies was computed # words = 94704\n",
      "all accessor variety was computed # words = 94704\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2055\n",
      "all branching entropies was computed # words = 94721\n",
      "all accessor variety was computed # words = 94721\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2055\n",
      "all branching entropies was computed # words = 94750\n",
      "all accessor variety was computed # words = 94750\n",
      "'다르네'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2056\n",
      "all branching entropies was computed # words = 94786\n",
      "all accessor variety was computed # words = 94786\n",
      "training was done. used memory 1.506 Gb1.516 Gb\n",
      "all cohesion probabilities was computed. # words = 2056\n",
      "all branching entropies was computed # words = 94799\n",
      "all accessor variety was computed # words = 94799\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2058\n",
      "all branching entropies was computed # words = 94812\n",
      "all accessor variety was computed # words = 94812\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2059\n",
      "all branching entropies was computed # words = 94816\n",
      "all accessor variety was computed # words = 94816\n",
      "'맞는듯'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2061\n",
      "all branching entropies was computed # words = 94823\n",
      "all accessor variety was computed # words = 94823\n",
      "'시발련들아'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2063\n",
      "all branching entropies was computed # words = 94832\n",
      "all accessor variety was computed # words = 94832\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2065\n",
      "all branching entropies was computed # words = 94837\n",
      "all accessor variety was computed # words = 94837\n",
      "'알려줄게'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2066\n",
      "all branching entropies was computed # words = 94858\n",
      "all accessor variety was computed # words = 94858\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2069\n",
      "all branching entropies was computed # words = 94867\n",
      "all accessor variety was computed # words = 94867\n",
      "training was done. used memory 1.506 Gb1.516 Gb\n",
      "all cohesion probabilities was computed. # words = 2070\n",
      "all branching entropies was computed # words = 94934\n",
      "all accessor variety was computed # words = 94934\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2070\n",
      "all branching entropies was computed # words = 94949\n",
      "all accessor variety was computed # words = 94949\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2070\n",
      "all branching entropies was computed # words = 94957\n",
      "all accessor variety was computed # words = 94957\n",
      "'김성태'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2072\n",
      "all branching entropies was computed # words = 94968\n",
      "all accessor variety was computed # words = 94968\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2074\n",
      "all branching entropies was computed # words = 94981\n",
      "all accessor variety was computed # words = 94981\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2074\n",
      "all branching entropies was computed # words = 94997\n",
      "all accessor variety was computed # words = 94997\n",
      "'짹창'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2076\n",
      "all branching entropies was computed # words = 95009\n",
      "all accessor variety was computed # words = 95009\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2078\n",
      "all branching entropies was computed # words = 95019\n",
      "all accessor variety was computed # words = 95019\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2078\n",
      "all branching entropies was computed # words = 95050\n",
      "all accessor variety was computed # words = 95050\n",
      "'까는글'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2079\n",
      "all branching entropies was computed # words = 95065\n",
      "all accessor variety was computed # words = 95065\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2079\n",
      "all branching entropies was computed # words = 95094\n",
      "all accessor variety was computed # words = 95094\n",
      "'잦편'\n",
      "training was done. used memory 1.506 Gb1.506 Gb\n",
      "all cohesion probabilities was computed. # words = 2079\n",
      "all branching entropies was computed # words = 95100\n",
      "all accessor variety was computed # words = 95100\n",
      "'응디린'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2080\n",
      "all branching entropies was computed # words = 95115\n",
      "all accessor variety was computed # words = 95115\n",
      "'보키'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2083\n",
      "all branching entropies was computed # words = 95135\n",
      "all accessor variety was computed # words = 95135\n",
      "training was done. used memory 1.507 Gb1.516 Gb\n",
      "all cohesion probabilities was computed. # words = 2084\n",
      "all branching entropies was computed # words = 95169\n",
      "all accessor variety was computed # words = 95169\n",
      "'열폭'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2086\n",
      "all branching entropies was computed # words = 95182\n",
      "all accessor variety was computed # words = 95182\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2087\n",
      "all branching entropies was computed # words = 95183\n",
      "all accessor variety was computed # words = 95183\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2088\n",
      "all branching entropies was computed # words = 95206\n",
      "all accessor variety was computed # words = 95206\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2088\n",
      "all branching entropies was computed # words = 95209\n",
      "all accessor variety was computed # words = 95209\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2090\n",
      "all branching entropies was computed # words = 95209\n",
      "all accessor variety was computed # words = 95209\n",
      "'말해준다'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2091\n",
      "all branching entropies was computed # words = 95227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 95227\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2093\n",
      "all branching entropies was computed # words = 95250\n",
      "all accessor variety was computed # words = 95250\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2093\n",
      "all branching entropies was computed # words = 95257\n",
      "all accessor variety was computed # words = 95257\n",
      "'재밌다고'\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2093\n",
      "all branching entropies was computed # words = 95269\n",
      "all accessor variety was computed # words = 95269\n",
      "'개이득'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2095\n",
      "all branching entropies was computed # words = 95293\n",
      "all accessor variety was computed # words = 95293\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2095\n",
      "all branching entropies was computed # words = 95293\n",
      "all accessor variety was computed # words = 95293\n",
      "'족구도'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2097\n",
      "all branching entropies was computed # words = 95314\n",
      "all accessor variety was computed # words = 95314\n",
      "training was done. used memory 1.507 Gb1.507 Gb\n",
      "all cohesion probabilities was computed. # words = 2099\n",
      "all branching entropies was computed # words = 95322\n",
      "all accessor variety was computed # words = 95322\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2099\n",
      "all branching entropies was computed # words = 95326\n",
      "all accessor variety was computed # words = 95326\n",
      "'혜디이모'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2100\n",
      "all branching entropies was computed # words = 95345\n",
      "all accessor variety was computed # words = 95345\n",
      "'봊질'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2101\n",
      "all branching entropies was computed # words = 95358\n",
      "all accessor variety was computed # words = 95358\n",
      "'올리면'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2103\n",
      "all branching entropies was computed # words = 95373\n",
      "all accessor variety was computed # words = 95373\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2105\n",
      "all branching entropies was computed # words = 95384\n",
      "all accessor variety was computed # words = 95384\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2108\n",
      "all branching entropies was computed # words = 95390\n",
      "all accessor variety was computed # words = 95390\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2109\n",
      "all branching entropies was computed # words = 95403\n",
      "all accessor variety was computed # words = 95403\n",
      "'나무늘봉순'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2111\n",
      "all branching entropies was computed # words = 95425\n",
      "all accessor variety was computed # words = 95425\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2111\n",
      "all branching entropies was computed # words = 95431\n",
      "all accessor variety was computed # words = 95431\n",
      "'왜이래'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2112\n",
      "all branching entropies was computed # words = 95449\n",
      "all accessor variety was computed # words = 95449\n",
      "'좆본'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2113\n",
      "all branching entropies was computed # words = 95456\n",
      "all accessor variety was computed # words = 95456\n",
      "'잘생김'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2114\n",
      "all branching entropies was computed # words = 95465\n",
      "all accessor variety was computed # words = 95465\n",
      "'안했으면'\n",
      "training was done. used memory 1.507 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2117\n",
      "all branching entropies was computed # words = 95512\n",
      "all accessor variety was computed # words = 95512\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2117\n",
      "all branching entropies was computed # words = 95516\n",
      "all accessor variety was computed # words = 95516\n",
      "'좆튭'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2117\n",
      "all branching entropies was computed # words = 95526\n",
      "all accessor variety was computed # words = 95526\n",
      "'흑어공주'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2117\n",
      "all branching entropies was computed # words = 95533\n",
      "all accessor variety was computed # words = 95533\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2118\n",
      "all branching entropies was computed # words = 95544\n",
      "all accessor variety was computed # words = 95544\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2118\n",
      "all branching entropies was computed # words = 95549\n",
      "all accessor variety was computed # words = 95549\n",
      "'푸렬쇼'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2118\n",
      "all branching entropies was computed # words = 95562\n",
      "all accessor variety was computed # words = 95562\n",
      "'좆이폰'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2119\n",
      "all branching entropies was computed # words = 95583\n",
      "all accessor variety was computed # words = 95583\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2119\n",
      "all branching entropies was computed # words = 95590\n",
      "all accessor variety was computed # words = 95590\n",
      "'꽃벼리'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2120\n",
      "all branching entropies was computed # words = 95613\n",
      "all accessor variety was computed # words = 95613\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2121\n",
      "all branching entropies was computed # words = 95630\n",
      "all accessor variety was computed # words = 95630\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2122\n",
      "all branching entropies was computed # words = 95652\n",
      "all accessor variety was computed # words = 95652\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2124\n",
      "all branching entropies was computed # words = 95661\n",
      "all accessor variety was computed # words = 95661\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2125\n",
      "all branching entropies was computed # words = 95679\n",
      "all accessor variety was computed # words = 95679\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2125\n",
      "all branching entropies was computed # words = 95680\n",
      "all accessor variety was computed # words = 95680\n",
      "'이러고'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2125\n",
      "all branching entropies was computed # words = 95687\n",
      "all accessor variety was computed # words = 95687\n",
      "'봉퀴새기들'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2125\n",
      "all branching entropies was computed # words = 95695\n",
      "all accessor variety was computed # words = 95695\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2125\n",
      "all branching entropies was computed # words = 95702\n",
      "all accessor variety was computed # words = 95702\n",
      "'끠리'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2126\n",
      "all branching entropies was computed # words = 95731\n",
      "all accessor variety was computed # words = 95731\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2128\n",
      "all branching entropies was computed # words = 95745\n",
      "all accessor variety was computed # words = 95745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2128\n",
      "all branching entropies was computed # words = 95748\n",
      "all accessor variety was computed # words = 95748\n",
      "'마조시'\n",
      "training was done. used memory 1.517 Gb1.517 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95765\n",
      "all accessor variety was computed # words = 95765\n",
      "'터지면'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95777\n",
      "all accessor variety was computed # words = 95777\n",
      "'유혜디님'\n",
      "training was done. used memory 1.517 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95792\n",
      "all accessor variety was computed # words = 95792\n",
      "'찜토커'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95806\n",
      "all accessor variety was computed # words = 95806\n",
      "'직캠'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95819\n",
      "all accessor variety was computed # words = 95819\n",
      "'도배중'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2129\n",
      "all branching entropies was computed # words = 95821\n",
      "all accessor variety was computed # words = 95821\n",
      "'웃긴점'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2130\n",
      "all branching entropies was computed # words = 95853\n",
      "all accessor variety was computed # words = 95853\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2134\n",
      "all branching entropies was computed # words = 95870\n",
      "all accessor variety was computed # words = 95870\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2134\n",
      "all branching entropies was computed # words = 95873\n",
      "all accessor variety was computed # words = 95873\n",
      "'구독자수'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 95876\n",
      "all accessor variety was computed # words = 95876\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 95889\n",
      "all accessor variety was computed # words = 95889\n",
      "'보라제우스'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2136\n",
      "all branching entropies was computed # words = 95895\n",
      "all accessor variety was computed # words = 95895\n",
      "'좆육대'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2137\n",
      "all branching entropies was computed # words = 95906\n",
      "all accessor variety was computed # words = 95906\n",
      "'보는중'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2139\n",
      "all branching entropies was computed # words = 95924\n",
      "all accessor variety was computed # words = 95924\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2139\n",
      "all branching entropies was computed # words = 95928\n",
      "all accessor variety was computed # words = 95928\n",
      "'철구님'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2139\n",
      "all branching entropies was computed # words = 95944\n",
      "all accessor variety was computed # words = 95944\n",
      "'박딘딘'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95952\n",
      "all accessor variety was computed # words = 95952\n",
      "'나왔네'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95961\n",
      "all accessor variety was computed # words = 95961\n",
      "'안보고'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95970\n",
      "all accessor variety was computed # words = 95970\n",
      "'파급력'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95975\n",
      "all accessor variety was computed # words = 95975\n",
      "'문자님'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95977\n",
      "all accessor variety was computed # words = 95977\n",
      "'방송끄고'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95979\n",
      "all accessor variety was computed # words = 95979\n",
      "'만푸르'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95993\n",
      "all accessor variety was computed # words = 95993\n",
      "'따이면'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2140\n",
      "all branching entropies was computed # words = 95996\n",
      "all accessor variety was computed # words = 95996\n",
      "'이딴거'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2142\n",
      "all branching entropies was computed # words = 96012\n",
      "all accessor variety was computed # words = 96012\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2142\n",
      "all branching entropies was computed # words = 96014\n",
      "all accessor variety was computed # words = 96014\n",
      "'좆기때'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2142\n",
      "all branching entropies was computed # words = 96024\n",
      "all accessor variety was computed # words = 96024\n",
      "'좋은점'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96031\n",
      "all accessor variety was computed # words = 96031\n",
      "'합방때'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96047\n",
      "all accessor variety was computed # words = 96047\n",
      "'마라탕'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96049\n",
      "all accessor variety was computed # words = 96049\n",
      "'이사람'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96059\n",
      "all accessor variety was computed # words = 96059\n",
      "'드럼갑'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96060\n",
      "all accessor variety was computed # words = 96060\n",
      "'시발년들아'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96080\n",
      "all accessor variety was computed # words = 96080\n",
      "'로켓단'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2143\n",
      "all branching entropies was computed # words = 96093\n",
      "all accessor variety was computed # words = 96093\n",
      "'치아문'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2145\n",
      "all branching entropies was computed # words = 96111\n",
      "all accessor variety was computed # words = 96111\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2145\n",
      "all branching entropies was computed # words = 96111\n",
      "all accessor variety was computed # words = 96111\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2145\n",
      "all branching entropies was computed # words = 96117\n",
      "all accessor variety was computed # words = 96117\n",
      "'초딩때'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2146\n",
      "all branching entropies was computed # words = 96131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 96131\n",
      "'웃참'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2146\n",
      "all branching entropies was computed # words = 96147\n",
      "all accessor variety was computed # words = 96147\n",
      "'푸가놈'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2146\n",
      "all branching entropies was computed # words = 96153\n",
      "all accessor variety was computed # words = 96153\n",
      "'이쁘다고'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2146\n",
      "all branching entropies was computed # words = 96157\n",
      "all accessor variety was computed # words = 96157\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2146\n",
      "all branching entropies was computed # words = 96176\n",
      "all accessor variety was computed # words = 96176\n",
      "'티저'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96184\n",
      "all accessor variety was computed # words = 96184\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96187\n",
      "all accessor variety was computed # words = 96187\n",
      "'뭐지'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96211\n",
      "all accessor variety was computed # words = 96211\n",
      "'잦주'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96237\n",
      "all accessor variety was computed # words = 96237\n",
      "'오토콜'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96256\n",
      "all accessor variety was computed # words = 96256\n",
      "'세줌'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96266\n",
      "all accessor variety was computed # words = 96266\n",
      "'지코형님'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2148\n",
      "all branching entropies was computed # words = 96269\n",
      "all accessor variety was computed # words = 96269\n",
      "'좆스타'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2149\n",
      "all branching entropies was computed # words = 96297\n",
      "all accessor variety was computed # words = 96297\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2151\n",
      "all branching entropies was computed # words = 96324\n",
      "all accessor variety was computed # words = 96324\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2154\n",
      "all branching entropies was computed # words = 96340\n",
      "all accessor variety was computed # words = 96340\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2154\n",
      "all branching entropies was computed # words = 96346\n",
      "all accessor variety was computed # words = 96346\n",
      "'느금유'\n",
      "training was done. used memory 1.518 Gb1.518 Gb\n",
      "all cohesion probabilities was computed. # words = 2154\n",
      "all branching entropies was computed # words = 96365\n",
      "all accessor variety was computed # words = 96365\n",
      "'많긴'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2154\n",
      "all branching entropies was computed # words = 96382\n",
      "all accessor variety was computed # words = 96382\n",
      "'푸르글'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2154\n",
      "all branching entropies was computed # words = 96384\n",
      "all accessor variety was computed # words = 96384\n",
      "'좆이돌들'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2157\n",
      "all branching entropies was computed # words = 96410\n",
      "all accessor variety was computed # words = 96410\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2157\n",
      "all branching entropies was computed # words = 96421\n",
      "all accessor variety was computed # words = 96421\n",
      "'윽신이'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2158\n",
      "all branching entropies was computed # words = 96438\n",
      "all accessor variety was computed # words = 96438\n",
      "'패면'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2159\n",
      "all branching entropies was computed # words = 96461\n",
      "all accessor variety was computed # words = 96461\n",
      "'구꿀'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2159\n",
      "all branching entropies was computed # words = 96469\n",
      "all accessor variety was computed # words = 96469\n",
      "'킹민교'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2159\n",
      "all branching entropies was computed # words = 96476\n",
      "all accessor variety was computed # words = 96476\n",
      "'소은이'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2159\n",
      "all branching entropies was computed # words = 96482\n",
      "all accessor variety was computed # words = 96482\n",
      "'고닉들'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 96485\n",
      "all accessor variety was computed # words = 96485\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 96487\n",
      "all accessor variety was computed # words = 96487\n",
      "'나은데'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 96502\n",
      "all accessor variety was computed # words = 96502\n",
      "'미군누나'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 96512\n",
      "all accessor variety was computed # words = 96512\n",
      "'킹은호'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2162\n",
      "all branching entropies was computed # words = 96518\n",
      "all accessor variety was computed # words = 96518\n",
      "'이왕호'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2164\n",
      "all branching entropies was computed # words = 96534\n",
      "all accessor variety was computed # words = 96534\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2165\n",
      "all branching entropies was computed # words = 96550\n",
      "all accessor variety was computed # words = 96550\n",
      "'안망'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2165\n",
      "all branching entropies was computed # words = 96568\n",
      "all accessor variety was computed # words = 96568\n",
      "'보리보리'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2167\n",
      "all branching entropies was computed # words = 96579\n",
      "all accessor variety was computed # words = 96579\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2167\n",
      "all branching entropies was computed # words = 96580\n",
      "all accessor variety was computed # words = 96580\n",
      "'지리긴'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96619\n",
      "all accessor variety was computed # words = 96619\n",
      "'머갈'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96631\n",
      "all accessor variety was computed # words = 96631\n",
      "'뒤지게'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96646\n",
      "all accessor variety was computed # words = 96646\n",
      "'인팁'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 96646\n",
      "all accessor variety was computed # words = 96646\n",
      "'롤아카데미'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96664\n",
      "all accessor variety was computed # words = 96664\n",
      "'푸사단'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96678\n",
      "all accessor variety was computed # words = 96678\n",
      "'킹케이'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2168\n",
      "all branching entropies was computed # words = 96684\n",
      "all accessor variety was computed # words = 96684\n",
      "'코트방송'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2169\n",
      "all branching entropies was computed # words = 96700\n",
      "all accessor variety was computed # words = 96700\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2169\n",
      "all branching entropies was computed # words = 96713\n",
      "all accessor variety was computed # words = 96713\n",
      "'오뎅탕지수'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2169\n",
      "all branching entropies was computed # words = 96724\n",
      "all accessor variety was computed # words = 96724\n",
      "'스트리머'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2169\n",
      "all branching entropies was computed # words = 96727\n",
      "all accessor variety was computed # words = 96727\n",
      "'찍으면'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2170\n",
      "all branching entropies was computed # words = 96762\n",
      "all accessor variety was computed # words = 96762\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2170\n",
      "all branching entropies was computed # words = 96765\n",
      "all accessor variety was computed # words = 96765\n",
      "'여겜비들'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 96787\n",
      "all accessor variety was computed # words = 96787\n",
      "'쏘푸'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 96789\n",
      "all accessor variety was computed # words = 96789\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 96791\n",
      "all accessor variety was computed # words = 96791\n",
      "'이적생들'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 96795\n",
      "all accessor variety was computed # words = 96795\n",
      "'예쁜데'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2171\n",
      "all branching entropies was computed # words = 96803\n",
      "all accessor variety was computed # words = 96803\n",
      "'팡이요'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2173\n",
      "all branching entropies was computed # words = 96834\n",
      "all accessor variety was computed # words = 96834\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2173\n",
      "all branching entropies was computed # words = 96848\n",
      "all accessor variety was computed # words = 96848\n",
      "'나라카일'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 96868\n",
      "all accessor variety was computed # words = 96868\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 96928\n",
      "all accessor variety was computed # words = 96928\n",
      "'봉염'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2174\n",
      "all branching entropies was computed # words = 96951\n",
      "all accessor variety was computed # words = 96951\n",
      "'유읍읍'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2175\n",
      "all branching entropies was computed # words = 96973\n",
      "all accessor variety was computed # words = 96973\n",
      "'만들고'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2176\n",
      "all branching entropies was computed # words = 96998\n",
      "all accessor variety was computed # words = 96998\n",
      "'병크'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 97053\n",
      "all accessor variety was computed # words = 97053\n",
      "'부랄'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2177\n",
      "all branching entropies was computed # words = 97062\n",
      "all accessor variety was computed # words = 97062\n",
      "'급식들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2178\n",
      "all branching entropies was computed # words = 97071\n",
      "all accessor variety was computed # words = 97071\n",
      "'욕풍'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2179\n",
      "all branching entropies was computed # words = 97084\n",
      "all accessor variety was computed # words = 97084\n",
      "'말빨'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2181\n",
      "all branching entropies was computed # words = 97112\n",
      "all accessor variety was computed # words = 97112\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2183\n",
      "all branching entropies was computed # words = 97126\n",
      "all accessor variety was computed # words = 97126\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2183\n",
      "all branching entropies was computed # words = 97145\n",
      "all accessor variety was computed # words = 97145\n",
      "'옴봊'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2185\n",
      "all branching entropies was computed # words = 97163\n",
      "all accessor variety was computed # words = 97163\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97167\n",
      "all accessor variety was computed # words = 97167\n",
      "'올라옴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97189\n",
      "all accessor variety was computed # words = 97189\n",
      "'봉퀴아오'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97196\n",
      "all accessor variety was computed # words = 97196\n",
      "'족구방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97201\n",
      "all accessor variety was computed # words = 97201\n",
      "'빙첸'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97269\n",
      "all accessor variety was computed # words = 97269\n",
      "'청자수'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2186\n",
      "all branching entropies was computed # words = 97275\n",
      "all accessor variety was computed # words = 97275\n",
      "'이쁘고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2188\n",
      "all branching entropies was computed # words = 97283\n",
      "all accessor variety was computed # words = 97283\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2188\n",
      "all branching entropies was computed # words = 97287\n",
      "all accessor variety was computed # words = 97287\n",
      "'죄송합니다'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2188\n",
      "all branching entropies was computed # words = 97306\n",
      "all accessor variety was computed # words = 97306\n",
      "'합의금'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97318\n",
      "all accessor variety was computed # words = 97318\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97323\n",
      "all accessor variety was computed # words = 97323\n",
      "'만식이형'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97338\n",
      "all accessor variety was computed # words = 97338\n",
      "'큰손들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97344\n",
      "all accessor variety was computed # words = 97344\n",
      "'나올때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97347\n",
      "all accessor variety was computed # words = 97347\n",
      "'즈그주인'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97355\n",
      "all accessor variety was computed # words = 97355\n",
      "'롤방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97360\n",
      "all accessor variety was computed # words = 97360\n",
      "'까는게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97363\n",
      "all accessor variety was computed # words = 97363\n",
      "'쏘한테'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97365\n",
      "all accessor variety was computed # words = 97365\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97374\n",
      "all accessor variety was computed # words = 97374\n",
      "'토뀨'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97383\n",
      "all accessor variety was computed # words = 97383\n",
      "'푸앵토'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97393\n",
      "all accessor variety was computed # words = 97393\n",
      "'레전드긴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97397\n",
      "all accessor variety was computed # words = 97397\n",
      "'머민이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97401\n",
      "all accessor variety was computed # words = 97401\n",
      "'하알라'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97402\n",
      "all accessor variety was computed # words = 97402\n",
      "'개불쌍'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97414\n",
      "all accessor variety was computed # words = 97414\n",
      "'팬비하'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97421\n",
      "all accessor variety was computed # words = 97421\n",
      "'줌냐들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97424\n",
      "all accessor variety was computed # words = 97424\n",
      "'좆프리카'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97439\n",
      "all accessor variety was computed # words = 97439\n",
      "'당근마켓'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97441\n",
      "all accessor variety was computed # words = 97441\n",
      "'스트형'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97446\n",
      "all accessor variety was computed # words = 97446\n",
      "'잘가라'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97450\n",
      "all accessor variety was computed # words = 97450\n",
      "'창짓'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2189\n",
      "all branching entropies was computed # words = 97458\n",
      "all accessor variety was computed # words = 97458\n",
      "'유능혜'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97476\n",
      "all accessor variety was computed # words = 97476\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97480\n",
      "all accessor variety was computed # words = 97480\n",
      "'봉보미'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97496\n",
      "all accessor variety was computed # words = 97496\n",
      "'웹툰'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97499\n",
      "all accessor variety was computed # words = 97499\n",
      "'김봉준방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97503\n",
      "all accessor variety was computed # words = 97503\n",
      "'이사진'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97508\n",
      "all accessor variety was computed # words = 97508\n",
      "'가성비'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97518\n",
      "all accessor variety was computed # words = 97518\n",
      "'시간동안'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97525\n",
      "all accessor variety was computed # words = 97525\n",
      "'진또삐'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97553\n",
      "all accessor variety was computed # words = 97553\n",
      "'성범죄자'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2192\n",
      "all branching entropies was computed # words = 97557\n",
      "all accessor variety was computed # words = 97557\n",
      "'커여워'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2193\n",
      "all branching entropies was computed # words = 97567\n",
      "all accessor variety was computed # words = 97567\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2193\n",
      "all branching entropies was computed # words = 97577\n",
      "all accessor variety was computed # words = 97577\n",
      "'카피카피룸룸'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2194\n",
      "all branching entropies was computed # words = 97609\n",
      "all accessor variety was computed # words = 97609\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97624\n",
      "all accessor variety was computed # words = 97624\n",
      "'만들면'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97639\n",
      "all accessor variety was computed # words = 97639\n",
      "'감킴봉'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 97667\n",
      "'정배우'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97682\n",
      "all accessor variety was computed # words = 97682\n",
      "'애엄마'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97693\n",
      "all accessor variety was computed # words = 97693\n",
      "'설명좀'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97699\n",
      "all accessor variety was computed # words = 97699\n",
      "'양여명'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97703\n",
      "all accessor variety was computed # words = 97703\n",
      "'따이고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97708\n",
      "all accessor variety was computed # words = 97708\n",
      "'붙잡지'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97713\n",
      "all accessor variety was computed # words = 97713\n",
      "'휴메'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97720\n",
      "all accessor variety was computed # words = 97720\n",
      "'족형래'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97723\n",
      "all accessor variety was computed # words = 97723\n",
      "'역겨운게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97734\n",
      "all accessor variety was computed # words = 97734\n",
      "'렴보성'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97736\n",
      "all accessor variety was computed # words = 97736\n",
      "'여캠중'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97742\n",
      "all accessor variety was computed # words = 97742\n",
      "'똑같음'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97748\n",
      "all accessor variety was computed # words = 97748\n",
      "'상윤이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97751\n",
      "all accessor variety was computed # words = 97751\n",
      "'브이앱'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97759\n",
      "all accessor variety was computed # words = 97759\n",
      "'데헷'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97787\n",
      "all accessor variety was computed # words = 97787\n",
      "'좹까'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97795\n",
      "all accessor variety was computed # words = 97795\n",
      "'대기중'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97798\n",
      "all accessor variety was computed # words = 97798\n",
      "'복귀후'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2195\n",
      "all branching entropies was computed # words = 97806\n",
      "all accessor variety was computed # words = 97806\n",
      "'하는말'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2197\n",
      "all branching entropies was computed # words = 97820\n",
      "all accessor variety was computed # words = 97820\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2197\n",
      "all branching entropies was computed # words = 97837\n",
      "all accessor variety was computed # words = 97837\n",
      "'시간전'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2197\n",
      "all branching entropies was computed # words = 97845\n",
      "all accessor variety was computed # words = 97845\n",
      "'며칠전'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2198\n",
      "all branching entropies was computed # words = 97859\n",
      "all accessor variety was computed # words = 97859\n",
      "'될듯'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2198\n",
      "all branching entropies was computed # words = 97879\n",
      "all accessor variety was computed # words = 97879\n",
      "'팬닉'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97898\n",
      "all accessor variety was computed # words = 97898\n",
      "'시참'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97898\n",
      "all accessor variety was computed # words = 97898\n",
      "'구라안'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97905\n",
      "all accessor variety was computed # words = 97905\n",
      "'감성팔이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97908\n",
      "all accessor variety was computed # words = 97908\n",
      "'아프리카판'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97927\n",
      "all accessor variety was computed # words = 97927\n",
      "'봉순님'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97928\n",
      "all accessor variety was computed # words = 97928\n",
      "'이런건'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97943\n",
      "all accessor variety was computed # words = 97943\n",
      "'필테'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97946\n",
      "all accessor variety was computed # words = 97946\n",
      "'실북갤'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97956\n",
      "all accessor variety was computed # words = 97956\n",
      "'종만이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97964\n",
      "all accessor variety was computed # words = 97964\n",
      "'가축형들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97973\n",
      "all accessor variety was computed # words = 97973\n",
      "'좆가시데'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97982\n",
      "all accessor variety was computed # words = 97982\n",
      "'년만에'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97987\n",
      "all accessor variety was computed # words = 97987\n",
      "'개인방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2199\n",
      "all branching entropies was computed # words = 97991\n",
      "all accessor variety was computed # words = 97991\n",
      "'힐링방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98018\n",
      "all accessor variety was computed # words = 98018\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 98024\n",
      "all accessor variety was computed # words = 98024\n",
      "'살때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98041\n",
      "all accessor variety was computed # words = 98041\n",
      "'잘하면'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98046\n",
      "all accessor variety was computed # words = 98046\n",
      "'좆되네'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98059\n",
      "all accessor variety was computed # words = 98059\n",
      "'능남'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98073\n",
      "all accessor variety was computed # words = 98073\n",
      "'만킬'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98076\n",
      "all accessor variety was computed # words = 98076\n",
      "'딱갤주'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2201\n",
      "all branching entropies was computed # words = 98087\n",
      "all accessor variety was computed # words = 98087\n",
      "'소녀재판'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98107\n",
      "all accessor variety was computed # words = 98107\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98113\n",
      "all accessor variety was computed # words = 98113\n",
      "'명박티아이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98123\n",
      "all accessor variety was computed # words = 98123\n",
      "'역겹긴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98131\n",
      "all accessor variety was computed # words = 98131\n",
      "'잘하긴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98139\n",
      "all accessor variety was computed # words = 98139\n",
      "'천앵'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98146\n",
      "all accessor variety was computed # words = 98146\n",
      "'띵박티아이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98148\n",
      "all accessor variety was computed # words = 98148\n",
      "'꺼지라고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98167\n",
      "all accessor variety was computed # words = 98167\n",
      "'이제동'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98170\n",
      "all accessor variety was computed # words = 98170\n",
      "'나락즈때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98170\n",
      "all accessor variety was computed # words = 98170\n",
      "'퀸혜디'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98183\n",
      "all accessor variety was computed # words = 98183\n",
      "'고딩때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98190\n",
      "all accessor variety was computed # words = 98190\n",
      "'키빼몸'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98196\n",
      "all accessor variety was computed # words = 98196\n",
      "'롤대회'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98199\n",
      "all accessor variety was computed # words = 98199\n",
      "'푸르집들이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98207\n",
      "all accessor variety was computed # words = 98207\n",
      "'피자나라'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2202\n",
      "all branching entropies was computed # words = 98216\n",
      "all accessor variety was computed # words = 98216\n",
      "'걸그룹'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98234\n",
      "all accessor variety was computed # words = 98234\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98239\n",
      "all accessor variety was computed # words = 98239\n",
      "'외퀴들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98251\n",
      "all accessor variety was computed # words = 98251\n",
      "'족구한테'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98261\n",
      "all accessor variety was computed # words = 98261\n",
      "'빻러'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98271\n",
      "all accessor variety was computed # words = 98271\n",
      "'부일좌'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98276\n",
      "all accessor variety was computed # words = 98276\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98281\n",
      "all accessor variety was computed # words = 98281\n",
      "'웃기지'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2203\n",
      "all branching entropies was computed # words = 98281\n",
      "all accessor variety was computed # words = 98281\n",
      "'왜저렇게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2205\n",
      "all branching entropies was computed # words = 98295\n",
      "all accessor variety was computed # words = 98295\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2205\n",
      "all branching entropies was computed # words = 98305\n",
      "all accessor variety was computed # words = 98305\n",
      "'잦배슬'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2205\n",
      "all branching entropies was computed # words = 98310\n",
      "all accessor variety was computed # words = 98310\n",
      "'띵품'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2205\n",
      "all branching entropies was computed # words = 98316\n",
      "all accessor variety was computed # words = 98316\n",
      "'준밧드'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2208\n",
      "all branching entropies was computed # words = 98376\n",
      "all accessor variety was computed # words = 98376\n",
      "'하는것'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2209\n",
      "all branching entropies was computed # words = 98407\n",
      "all accessor variety was computed # words = 98407\n",
      "'빨면'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2210\n",
      "all branching entropies was computed # words = 98439\n",
      "all accessor variety was computed # words = 98439\n",
      "'좆밥'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2212\n",
      "all branching entropies was computed # words = 98456\n",
      "all accessor variety was computed # words = 98456\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2213\n",
      "all branching entropies was computed # words = 98479\n",
      "all accessor variety was computed # words = 98479\n",
      "'좆울'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2213\n",
      "all branching entropies was computed # words = 98491\n",
      "all accessor variety was computed # words = 98491\n",
      "'세자파'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2213\n",
      "all branching entropies was computed # words = 98498\n",
      "all accessor variety was computed # words = 98498\n",
      "'군대가기전'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2214\n",
      "all branching entropies was computed # words = 98525\n",
      "all accessor variety was computed # words = 98525\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2215\n",
      "all branching entropies was computed # words = 98570\n",
      "all accessor variety was computed # words = 98570\n",
      "'쳐먹고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2215\n",
      "all branching entropies was computed # words = 98571\n",
      "all accessor variety was computed # words = 98571\n",
      "'넣으면'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2215\n",
      "all branching entropies was computed # words = 98580\n",
      "all accessor variety was computed # words = 98580\n",
      "'이시각'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98643\n",
      "all accessor variety was computed # words = 98643\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98654\n",
      "all accessor variety was computed # words = 98654\n",
      "'철쪽'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98677\n",
      "all accessor variety was computed # words = 98677\n",
      "'민좆당'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98712\n",
      "all accessor variety was computed # words = 98712\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98716\n",
      "all accessor variety was computed # words = 98716\n",
      "'꽃님이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98732\n",
      "all accessor variety was computed # words = 98732\n",
      "'볼드모트'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98733\n",
      "all accessor variety was computed # words = 98733\n",
      "'어디갔노'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98742\n",
      "all accessor variety was computed # words = 98742\n",
      "'허언갑'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2216\n",
      "all branching entropies was computed # words = 98746\n",
      "all accessor variety was computed # words = 98746\n",
      "'미철이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2217\n",
      "all branching entropies was computed # words = 98802\n",
      "all accessor variety was computed # words = 98802\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2217\n",
      "all branching entropies was computed # words = 98808\n",
      "all accessor variety was computed # words = 98808\n",
      "'봊한테'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2217\n",
      "all branching entropies was computed # words = 98819\n",
      "all accessor variety was computed # words = 98819\n",
      "'게이팝'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2217\n",
      "all branching entropies was computed # words = 98824\n",
      "all accessor variety was computed # words = 98824\n",
      "'트할'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2219\n",
      "all branching entropies was computed # words = 98979\n",
      "all accessor variety was computed # words = 98979\n",
      "'분들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2219\n",
      "all branching entropies was computed # words = 98985\n",
      "all accessor variety was computed # words = 98985\n",
      "'박사장'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99005\n",
      "all accessor variety was computed # words = 99005\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99020\n",
      "all accessor variety was computed # words = 99020\n",
      "'쏘방'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99024\n",
      "all accessor variety was computed # words = 99024\n",
      "'다른데'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99029\n",
      "all accessor variety was computed # words = 99029\n",
      "'즐삭'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99030\n",
      "all accessor variety was computed # words = 99030\n",
      "'개씹노잼'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99039\n",
      "all accessor variety was computed # words = 99039\n",
      "'근깅이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99040\n",
      "all accessor variety was computed # words = 99040\n",
      "'더팩트'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99042\n",
      "all accessor variety was computed # words = 99042\n",
      "'방송함'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99049\n",
      "all accessor variety was computed # words = 99049\n",
      "'흉통'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99057\n",
      "all accessor variety was computed # words = 99057\n",
      "'잦트롯'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99061\n",
      "all accessor variety was computed # words = 99061\n",
      "'봊주'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99066\n",
      "all accessor variety was computed # words = 99066\n",
      "'현상황'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99071\n",
      "all accessor variety was computed # words = 99071\n",
      "'서수기릿'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2221\n",
      "all branching entropies was computed # words = 99076\n",
      "all accessor variety was computed # words = 99076\n",
      "'땁서연'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99094\n",
      "all accessor variety was computed # words = 99094\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99096\n",
      "all accessor variety was computed # words = 99096\n",
      "'나와도'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 99101\n",
      "'분컷'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99110\n",
      "all accessor variety was computed # words = 99110\n",
      "'디패'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99112\n",
      "all accessor variety was computed # words = 99112\n",
      "'뭘로'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99124\n",
      "all accessor variety was computed # words = 99124\n",
      "'스시잦'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99126\n",
      "all accessor variety was computed # words = 99126\n",
      "'륵륵저'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99128\n",
      "all accessor variety was computed # words = 99128\n",
      "'부천파'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99149\n",
      "all accessor variety was computed # words = 99149\n",
      "'찐따들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99154\n",
      "all accessor variety was computed # words = 99154\n",
      "'개좆망'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99159\n",
      "all accessor variety was computed # words = 99159\n",
      "'노무단'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99165\n",
      "all accessor variety was computed # words = 99165\n",
      "'녹방'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99167\n",
      "all accessor variety was computed # words = 99167\n",
      "'썩아가'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99170\n",
      "all accessor variety was computed # words = 99170\n",
      "'시원하게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99178\n",
      "all accessor variety was computed # words = 99178\n",
      "'못생김'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2224\n",
      "all branching entropies was computed # words = 99187\n",
      "all accessor variety was computed # words = 99187\n",
      "'감파이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99194\n",
      "all accessor variety was computed # words = 99194\n",
      "'감빡'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99202\n",
      "all accessor variety was computed # words = 99202\n",
      "'로드킬'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99202\n",
      "all accessor variety was computed # words = 99202\n",
      "'여연갤년들'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99202\n",
      "all accessor variety was computed # words = 99202\n",
      "'더럽게'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99203\n",
      "all accessor variety was computed # words = 99203\n",
      "'저기에'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99217\n",
      "all accessor variety was computed # words = 99217\n",
      "'조제호'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99225\n",
      "all accessor variety was computed # words = 99225\n",
      "'녀살려'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99227\n",
      "all accessor variety was computed # words = 99227\n",
      "'남순이방송'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99235\n",
      "all accessor variety was computed # words = 99235\n",
      "'마이민방'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99238\n",
      "all accessor variety was computed # words = 99238\n",
      "'저사람'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2226\n",
      "all branching entropies was computed # words = 99238\n",
      "all accessor variety was computed # words = 99238\n",
      "'마이너즈'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 99243\n",
      "all accessor variety was computed # words = 99243\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 99261\n",
      "all accessor variety was computed # words = 99261\n",
      "'애교용'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 99276\n",
      "all accessor variety was computed # words = 99276\n",
      "'사귀고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 99293\n",
      "all accessor variety was computed # words = 99293\n",
      "'전여친'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2227\n",
      "all branching entropies was computed # words = 99298\n",
      "all accessor variety was computed # words = 99298\n",
      "'흙규'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99319\n",
      "all accessor variety was computed # words = 99319\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99328\n",
      "all accessor variety was computed # words = 99328\n",
      "'심하긴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99338\n",
      "all accessor variety was computed # words = 99338\n",
      "'남연갤'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99355\n",
      "all accessor variety was computed # words = 99355\n",
      "'바이럴'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99358\n",
      "all accessor variety was computed # words = 99358\n",
      "'푸르팀'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99359\n",
      "all accessor variety was computed # words = 99359\n",
      "'아프리카갤'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99363\n",
      "all accessor variety was computed # words = 99363\n",
      "'세탁방송'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99373\n",
      "all accessor variety was computed # words = 99373\n",
      "'미니진'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n",
      "all branching entropies was computed # words = 99376\n",
      "all accessor variety was computed # words = 99376\n",
      "'좋은게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 99378\n",
      "all accessor variety was computed # words = 99378\n",
      "'이뻐서'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2231\n",
      "all branching entropies was computed # words = 99393\n",
      "all accessor variety was computed # words = 99393\n",
      "'케순'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99407\n",
      "all accessor variety was computed # words = 99407\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99417\n",
      "all accessor variety was computed # words = 99417\n",
      "'엔팁'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99423\n",
      "all accessor variety was computed # words = 99423\n",
      "'약뱅'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99431\n",
      "all accessor variety was computed # words = 99431\n",
      "'봉아가'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99435\n",
      "all accessor variety was computed # words = 99435\n",
      "'족푸르'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99455\n",
      "all accessor variety was computed # words = 99455\n",
      "'방송좀'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99469\n",
      "all accessor variety was computed # words = 99469\n",
      "'푸르언급'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99477\n",
      "all accessor variety was computed # words = 99477\n",
      "'블랙핑크'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99487\n",
      "all accessor variety was computed # words = 99487\n",
      "'다같이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99502\n",
      "all accessor variety was computed # words = 99502\n",
      "'머가리'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99520\n",
      "all accessor variety was computed # words = 99520\n",
      "'뿡갤'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99529\n",
      "all accessor variety was computed # words = 99529\n",
      "'환토미'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99548\n",
      "all accessor variety was computed # words = 99548\n",
      "'개펜씨'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99548\n",
      "all accessor variety was computed # words = 99548\n",
      "'쏘푸르'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99552\n",
      "all accessor variety was computed # words = 99552\n",
      "'좆로나'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99594\n",
      "all accessor variety was computed # words = 99594\n",
      "'키크고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99600\n",
      "all accessor variety was computed # words = 99600\n",
      "'킹만식'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99621\n",
      "all accessor variety was computed # words = 99621\n",
      "'굿즈'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2232\n",
      "all branching entropies was computed # words = 99623\n",
      "all accessor variety was computed # words = 99623\n",
      "'야무지게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99654\n",
      "all accessor variety was computed # words = 99654\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99663\n",
      "all accessor variety was computed # words = 99663\n",
      "'금강연화'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99672\n",
      "all accessor variety was computed # words = 99672\n",
      "'약젠'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99679\n",
      "all accessor variety was computed # words = 99679\n",
      "'혜디눈나'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99693\n",
      "all accessor variety was computed # words = 99693\n",
      "'망이니'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99707\n",
      "all accessor variety was computed # words = 99707\n",
      "'모기짓'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99717\n",
      "all accessor variety was computed # words = 99717\n",
      "'뮤뱅'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99722\n",
      "all accessor variety was computed # words = 99722\n",
      "'남캠들'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99730\n",
      "all accessor variety was computed # words = 99730\n",
      "'조기전역'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99731\n",
      "all accessor variety was computed # words = 99731\n",
      "'롤을'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99732\n",
      "all accessor variety was computed # words = 99732\n",
      "'좌좀들'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99733\n",
      "all accessor variety was computed # words = 99733\n",
      "'썩마토'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99758\n",
      "all accessor variety was computed # words = 99758\n",
      "'주인님'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99763\n",
      "all accessor variety was computed # words = 99763\n",
      "'안봐도'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99767\n",
      "all accessor variety was computed # words = 99767\n",
      "'선냐'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2233\n",
      "all branching entropies was computed # words = 99779\n",
      "all accessor variety was computed # words = 99779\n",
      "'감봉염'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99807\n",
      "all accessor variety was computed # words = 99807\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99815\n",
      "all accessor variety was computed # words = 99815\n",
      "'언제적'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99815\n",
      "all accessor variety was computed # words = 99815\n",
      "'인방갤보고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99831\n",
      "all accessor variety was computed # words = 99831\n",
      "'역사상'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99838\n",
      "all accessor variety was computed # words = 99838\n",
      "'많지'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99845\n",
      "all accessor variety was computed # words = 99845\n",
      "'역겨운점'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99853\n",
      "all accessor variety was computed # words = 99853\n",
      "'봉빡인데'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99854\n",
      "all accessor variety was computed # words = 99854\n",
      "'모든게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99859\n",
      "all accessor variety was computed # words = 99859\n",
      "'웃기냐'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99871\n",
      "all accessor variety was computed # words = 99871\n",
      "'뿡탄'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99884\n",
      "all accessor variety was computed # words = 99884\n",
      "'염황'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2234\n",
      "all branching entropies was computed # words = 99902\n",
      "all accessor variety was computed # words = 99902\n",
      "'트라우마'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99928\n",
      "all accessor variety was computed # words = 99928\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99932\n",
      "all accessor variety was computed # words = 99932\n",
      "'푸퀴방패'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99932\n",
      "all accessor variety was computed # words = 99932\n",
      "'철구유튜브'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99935\n",
      "all accessor variety was computed # words = 99935\n",
      "'트위치때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99948\n",
      "all accessor variety was computed # words = 99948\n",
      "'풀캠'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99967\n",
      "all accessor variety was computed # words = 99967\n",
      "'팬분들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99967\n",
      "all accessor variety was computed # words = 99967\n",
      "'봉퀴형들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99972\n",
      "all accessor variety was computed # words = 99972\n",
      "'경민이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99974\n",
      "all accessor variety was computed # words = 99974\n",
      "'일부로'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99985\n",
      "all accessor variety was computed # words = 99985\n",
      "'섹화첩'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 99991\n",
      "all accessor variety was computed # words = 99991\n",
      "'푸쏘유'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 100002\n",
      "all accessor variety was computed # words = 100002\n",
      "'으악이네'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 100005\n",
      "all accessor variety was computed # words = 100005\n",
      "'괴물쥐'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2235\n",
      "all branching entropies was computed # words = 100010\n",
      "all accessor variety was computed # words = 100010\n",
      "'이쁘게'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 100020\n",
      "all accessor variety was computed # words = 100020\n",
      "'안하지'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 100038\n",
      "all accessor variety was computed # words = 100038\n",
      "'시발새끼'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2236\n",
      "all branching entropies was computed # words = 100046\n",
      "all accessor variety was computed # words = 100046\n",
      "'망했다고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100094\n",
      "all accessor variety was computed # words = 100094\n",
      "'가는데'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100100\n",
      "all accessor variety was computed # words = 100100\n",
      "'상렬이형'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100116\n",
      "all accessor variety was computed # words = 100116\n",
      "'터지고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100116\n",
      "all accessor variety was computed # words = 100116\n",
      "'모여서'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100123\n",
      "all accessor variety was computed # words = 100123\n",
      "'다른방'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100132\n",
      "all accessor variety was computed # words = 100132\n",
      "'앵마토'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2237\n",
      "all branching entropies was computed # words = 100136\n",
      "all accessor variety was computed # words = 100136\n",
      "'뭉치면'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2238\n",
      "all branching entropies was computed # words = 100138\n",
      "all accessor variety was computed # words = 100138\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2238\n",
      "all branching entropies was computed # words = 100148\n",
      "all accessor variety was computed # words = 100148\n",
      "'아리연습중'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2238\n",
      "all branching entropies was computed # words = 100157\n",
      "all accessor variety was computed # words = 100157\n",
      "'읍희롱'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2238\n",
      "all branching entropies was computed # words = 100162\n",
      "all accessor variety was computed # words = 100162\n",
      "'빠는게'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 100164\n",
      "all accessor variety was computed # words = 100164\n",
      "'불쌍함'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100174\n",
      "all accessor variety was computed # words = 100174\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100190\n",
      "all accessor variety was computed # words = 100190\n",
      "'서폿'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100195\n",
      "all accessor variety was computed # words = 100195\n",
      "'전태규'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100199\n",
      "all accessor variety was computed # words = 100199\n",
      "'임블리'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100211\n",
      "all accessor variety was computed # words = 100211\n",
      "'시까지'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100240\n",
      "all accessor variety was computed # words = 100240\n",
      "'처먹고'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100248\n",
      "all accessor variety was computed # words = 100248\n",
      "'여끠'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100252\n",
      "all accessor variety was computed # words = 100252\n",
      "'끠자'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100254\n",
      "all accessor variety was computed # words = 100254\n",
      "'다시보기에'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2239\n",
      "all branching entropies was computed # words = 100263\n",
      "all accessor variety was computed # words = 100263\n",
      "'봉퀴매직'\n",
      "training was done. used memory 1.521 Gb1.521 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100277\n",
      "all accessor variety was computed # words = 100277\n",
      "'머뷔'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100285\n",
      "all accessor variety was computed # words = 100285\n",
      "'푸방국'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100289\n",
      "all accessor variety was computed # words = 100289\n",
      "'노라뎃'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100295\n",
      "all accessor variety was computed # words = 100295\n",
      "'썩벤져스'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100304\n",
      "all accessor variety was computed # words = 100304\n",
      "'의아라'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2240\n",
      "all branching entropies was computed # words = 100317\n",
      "all accessor variety was computed # words = 100317\n",
      "'하루마'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 100336\n",
      "all accessor variety was computed # words = 100336\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 100340\n",
      "all accessor variety was computed # words = 100340\n",
      "'투샷'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 100360\n",
      "all accessor variety was computed # words = 100360\n",
      "'대깨푸'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2241\n",
      "all branching entropies was computed # words = 100364\n",
      "all accessor variety was computed # words = 100364\n",
      "'커맨더지코'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2247\n",
      "all branching entropies was computed # words = 100410\n",
      "all accessor variety was computed # words = 100410\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2249\n",
      "all branching entropies was computed # words = 100421\n",
      "all accessor variety was computed # words = 100421\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2249\n",
      "all branching entropies was computed # words = 100423\n",
      "all accessor variety was computed # words = 100423\n",
      "'팩트체크'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2249\n",
      "all branching entropies was computed # words = 100424\n",
      "all accessor variety was computed # words = 100424\n",
      "'어딨어'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2249\n",
      "all branching entropies was computed # words = 100427\n",
      "all accessor variety was computed # words = 100427\n",
      "'똥보성'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100442\n",
      "all accessor variety was computed # words = 100442\n",
      "'안친'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100444\n",
      "all accessor variety was computed # words = 100444\n",
      "'윾갈비'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100453\n",
      "all accessor variety was computed # words = 100453\n",
      "'코마토'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100460\n",
      "all accessor variety was computed # words = 100460\n",
      "'나무위키'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100464\n",
      "all accessor variety was computed # words = 100464\n",
      "'퇴구새기'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100483\n",
      "all accessor variety was computed # words = 100483\n",
      "'쪽지테러'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100501\n",
      "all accessor variety was computed # words = 100501\n",
      "'여캠방'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100688\n",
      "all accessor variety was computed # words = 100688\n",
      "'남들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2250\n",
      "all branching entropies was computed # words = 100692\n",
      "all accessor variety was computed # words = 100692\n",
      "'이럴때'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 100705\n",
      "all accessor variety was computed # words = 100705\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2251\n",
      "all branching entropies was computed # words = 100709\n",
      "all accessor variety was computed # words = 100709\n",
      "'한달뒤'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100725\n",
      "all accessor variety was computed # words = 100725\n",
      "'없는이유'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 100727\n",
      "'빡침'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100729\n",
      "all accessor variety was computed # words = 100729\n",
      "'조회수가'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100734\n",
      "all accessor variety was computed # words = 100734\n",
      "'혜디님'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100748\n",
      "all accessor variety was computed # words = 100748\n",
      "'나오지'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100751\n",
      "all accessor variety was computed # words = 100751\n",
      "'빠지고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2252\n",
      "all branching entropies was computed # words = 100755\n",
      "all accessor variety was computed # words = 100755\n",
      "'여좹'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100766\n",
      "all accessor variety was computed # words = 100766\n",
      "'잦듀'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100766\n",
      "all accessor variety was computed # words = 100766\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100783\n",
      "all accessor variety was computed # words = 100783\n",
      "'억뷰'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100788\n",
      "all accessor variety was computed # words = 100788\n",
      "'매니저들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100802\n",
      "all accessor variety was computed # words = 100802\n",
      "'방탄소년단'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100805\n",
      "all accessor variety was computed # words = 100805\n",
      "'트페미들'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100811\n",
      "all accessor variety was computed # words = 100811\n",
      "'철염중'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100822\n",
      "all accessor variety was computed # words = 100822\n",
      "'욕하고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100832\n",
      "all accessor variety was computed # words = 100832\n",
      "'씹뱉'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100833\n",
      "all accessor variety was computed # words = 100833\n",
      "'또또'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100851\n",
      "all accessor variety was computed # words = 100851\n",
      "'자낳대'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100853\n",
      "all accessor variety was computed # words = 100853\n",
      "'왕지숙'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100854\n",
      "all accessor variety was computed # words = 100854\n",
      "'인방갤도'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100854\n",
      "all accessor variety was computed # words = 100854\n",
      "'지혼자'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100862\n",
      "all accessor variety was computed # words = 100862\n",
      "'망하고'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100869\n",
      "all accessor variety was computed # words = 100869\n",
      "'물현이'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100875\n",
      "all accessor variety was computed # words = 100875\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2253\n",
      "all branching entropies was computed # words = 100881\n",
      "all accessor variety was computed # words = 100881\n",
      "'개밥이'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2254\n",
      "all branching entropies was computed # words = 100882\n",
      "all accessor variety was computed # words = 100882\n",
      "'나오고'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100895\n",
      "all accessor variety was computed # words = 100895\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100897\n",
      "all accessor variety was computed # words = 100897\n",
      "'롤유동'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100905\n",
      "all accessor variety was computed # words = 100905\n",
      "'짭태우'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100920\n",
      "all accessor variety was computed # words = 100920\n",
      "'지연주'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100929\n",
      "all accessor variety was computed # words = 100929\n",
      "'없는건'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100933\n",
      "all accessor variety was computed # words = 100933\n",
      "'누구한테'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100938\n",
      "all accessor variety was computed # words = 100938\n",
      "'재넌이'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100949\n",
      "all accessor variety was computed # words = 100949\n",
      "'좃봉준'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100962\n",
      "all accessor variety was computed # words = 100962\n",
      "'덬창'\n",
      "training was done. used memory 1.519 Gb1.519 Gb\n",
      "all cohesion probabilities was computed. # words = 2255\n",
      "all branching entropies was computed # words = 100966\n",
      "all accessor variety was computed # words = 100966\n",
      "'뱀눈새끼'\n",
      "training was done. used memory 1.520 Gb1.520 Gb\n",
      "all cohesion probabilities was computed. # words = 2256\n",
      "all branching entropies was computed # words = 100992\n",
      "all accessor variety was computed # words = 100992\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 51164 from 1 sents. mem=1.530 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=131667, mem=2.629 Gb\n",
      "[Noun Extractor] batch prediction was completed for 12918 words\n",
      "[Noun Extractor] checked compounds. discovered 3885 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 7004 -> 6125\n",
      "[Noun Extractor] postprocessing ignore_features : 6125 -> 6060\n",
      "[Noun Extractor] postprocessing ignore_NJ : 6060 -> 6024\n",
      "[Noun Extractor] 6024 nouns (3885 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.307 Gb                    \n",
      "[Noun Extractor] 61.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 28793 from 1 sents. mem=1.296 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=76949, mem=1.269 Gb\n",
      "[Noun Extractor] batch prediction was completed for 8000 words\n",
      "[Noun Extractor] checked compounds. discovered 2768 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4633 -> 3838\n",
      "[Noun Extractor] postprocessing ignore_features : 3838 -> 3777\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3777 -> 3737\n",
      "[Noun Extractor] 3737 nouns (2768 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.267 Gb                    \n",
      "[Noun Extractor] 65.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 36445 from 1 sents. mem=1.256 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=105693, mem=1.254 Gb\n",
      "[Noun Extractor] batch prediction was completed for 9998 words\n",
      "[Noun Extractor] checked compounds. discovered 3777 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 5826 -> 4927\n",
      "[Noun Extractor] postprocessing ignore_features : 4927 -> 4846\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4846 -> 4764\n",
      "[Noun Extractor] 4764 nouns (3777 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.253 Gb                    \n",
      "[Noun Extractor] 67.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 29482 from 1 sents. mem=1.255 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=80795, mem=1.240 Gb\n",
      "[Noun Extractor] batch prediction was completed for 7754 words\n",
      "[Noun Extractor] checked compounds. discovered 2454 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4268 -> 3664\n",
      "[Noun Extractor] postprocessing ignore_features : 3664 -> 3598\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3598 -> 3559\n",
      "[Noun Extractor] 3559 nouns (2454 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.235 Gb                    \n",
      "[Noun Extractor] 64.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 23663 from 1 sents. mem=1.230 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=62478, mem=1.201 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6420 words\n",
      "[Noun Extractor] checked compounds. discovered 1686 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3306 -> 2869\n",
      "[Noun Extractor] postprocessing ignore_features : 2869 -> 2816\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2816 -> 2785\n",
      "[Noun Extractor] 2785 nouns (1686 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.204 Gb                    \n",
      "[Noun Extractor] 62.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 29742 from 1 sents. mem=1.208 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=77975, mem=1.209 Gb\n",
      "[Noun Extractor] batch prediction was completed for 8130 words\n",
      "[Noun Extractor] checked compounds. discovered 2135 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 4008 -> 3509\n",
      "[Noun Extractor] postprocessing ignore_features : 3509 -> 3443\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3443 -> 3392\n",
      "[Noun Extractor] 3392 nouns (2135 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.205 Gb                    \n",
      "[Noun Extractor] 65.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 27929 from 1 sents. mem=1.204 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=72220, mem=1.204 Gb\n",
      "[Noun Extractor] batch prediction was completed for 7743 words\n",
      "[Noun Extractor] checked compounds. discovered 2335 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3999 -> 3254\n",
      "[Noun Extractor] postprocessing ignore_features : 3254 -> 3191\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3191 -> 3161\n",
      "[Noun Extractor] 3161 nouns (2335 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.202 Gb                    \n",
      "[Noun Extractor] 64.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 20596 from 1 sents. mem=1.202 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=52327, mem=1.195 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5421 words\n",
      "[Noun Extractor] checked compounds. discovered 1217 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2690 -> 2353\n",
      "[Noun Extractor] postprocessing ignore_features : 2353 -> 2305\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2305 -> 2291\n",
      "[Noun Extractor] 2291 nouns (1217 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 60.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 20765 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=51876, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6035 words\n",
      "[Noun Extractor] checked compounds. discovered 2159 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3535 -> 2886\n",
      "[Noun Extractor] postprocessing ignore_features : 2886 -> 2832\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2832 -> 2794\n",
      "[Noun Extractor] 2794 nouns (2159 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 67.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 17018 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=36844, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4760 words\n",
      "[Noun Extractor] checked compounds. discovered 653 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2037 -> 1868\n",
      "[Noun Extractor] postprocessing ignore_features : 1868 -> 1828\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1828 -> 1827\n",
      "[Noun Extractor] 1827 nouns (653 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 51.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 13415 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=32893, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3713 words\n",
      "[Noun Extractor] checked compounds. discovered 862 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1791 -> 1615\n",
      "[Noun Extractor] postprocessing ignore_features : 1615 -> 1576\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1576 -> 1566\n",
      "[Noun Extractor] 1566 nouns (862 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 63.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14107 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=34196, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3835 words\n",
      "[Noun Extractor] checked compounds. discovered 910 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1888 -> 1708\n",
      "[Noun Extractor] postprocessing ignore_features : 1708 -> 1671\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1671 -> 1652\n",
      "[Noun Extractor] 1652 nouns (910 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 62.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10493 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=24303, mem=1.180 Gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] batch prediction was completed for 3081 words\n",
      "[Noun Extractor] checked compounds. discovered 656 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1482 -> 1314\n",
      "[Noun Extractor] postprocessing ignore_features : 1314 -> 1269\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1269 -> 1262\n",
      "[Noun Extractor] 1262 nouns (656 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 61.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8681 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=18817, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2628 words\n",
      "[Noun Extractor] checked compounds. discovered 514 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1269 -> 1138\n",
      "[Noun Extractor] postprocessing ignore_features : 1138 -> 1099\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1099 -> 1088\n",
      "[Noun Extractor] 1088 nouns (514 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 59.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 12061 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27202, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3387 words\n",
      "[Noun Extractor] checked compounds. discovered 686 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1629 -> 1388\n",
      "[Noun Extractor] postprocessing ignore_features : 1388 -> 1357\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1357 -> 1350\n",
      "[Noun Extractor] 1350 nouns (686 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 61.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9761 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20994, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2818 words\n",
      "[Noun Extractor] checked compounds. discovered 505 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1243 -> 1131\n",
      "[Noun Extractor] postprocessing ignore_features : 1131 -> 1103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1103 -> 1099\n",
      "[Noun Extractor] 1099 nouns (505 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 62.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 16923 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=36496, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4995 words\n",
      "[Noun Extractor] checked compounds. discovered 1034 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2312 -> 2108\n",
      "[Noun Extractor] postprocessing ignore_features : 2108 -> 2062\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2062 -> 2048\n",
      "[Noun Extractor] 2048 nouns (1034 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 59.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11211 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=24288, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3296 words\n",
      "[Noun Extractor] checked compounds. discovered 479 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1361 -> 1196\n",
      "[Noun Extractor] postprocessing ignore_features : 1196 -> 1158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1158 -> 1148\n",
      "[Noun Extractor] 1148 nouns (479 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 57.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14750 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=35300, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4237 words\n",
      "[Noun Extractor] checked compounds. discovered 815 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1819 -> 1646\n",
      "[Noun Extractor] postprocessing ignore_features : 1646 -> 1617\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1617 -> 1608\n",
      "[Noun Extractor] 1608 nouns (815 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 60.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 12644 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=25521, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3906 words\n",
      "[Noun Extractor] checked compounds. discovered 488 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1596 -> 1523\n",
      "[Noun Extractor] postprocessing ignore_features : 1523 -> 1490\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1490 -> 1484\n",
      "[Noun Extractor] 1484 nouns (488 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 54.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6303 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12714, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1919 words\n",
      "[Noun Extractor] checked compounds. discovered 175 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 691 -> 644\n",
      "[Noun Extractor] postprocessing ignore_features : 644 -> 622\n",
      "[Noun Extractor] postprocessing ignore_NJ : 622 -> 621\n",
      "[Noun Extractor] 621 nouns (175 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 55.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8271 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16134, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2426 words\n",
      "[Noun Extractor] checked compounds. discovered 233 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 899 -> 834\n",
      "[Noun Extractor] postprocessing ignore_features : 834 -> 805\n",
      "[Noun Extractor] postprocessing ignore_NJ : 805 -> 802\n",
      "[Noun Extractor] 802 nouns (233 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 55.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7577 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16961, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2202 words\n",
      "[Noun Extractor] checked compounds. discovered 218 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 843 -> 779\n",
      "[Noun Extractor] postprocessing ignore_features : 779 -> 751\n",
      "[Noun Extractor] postprocessing ignore_NJ : 751 -> 748\n",
      "[Noun Extractor] 748 nouns (218 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 58.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5642 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13470, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1679 words\n",
      "[Noun Extractor] checked compounds. discovered 210 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 639 -> 626\n",
      "[Noun Extractor] postprocessing ignore_features : 626 -> 611\n",
      "[Noun Extractor] postprocessing ignore_NJ : 611 -> 610\n",
      "[Noun Extractor] 610 nouns (210 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 59.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 26792 from 1 sents. mem=1.169 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=60544, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 7409 words\n",
      "[Noun Extractor] checked compounds. discovered 1606 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3439 -> 2926\n",
      "[Noun Extractor] postprocessing ignore_features : 2926 -> 2877\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2877 -> 2865\n",
      "[Noun Extractor] 2865 nouns (1606 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 47.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7452 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13128, mem=1.169 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2436 words\n",
      "[Noun Extractor] checked compounds. discovered 203 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 825 -> 798\n",
      "[Noun Extractor] postprocessing ignore_features : 798 -> 777\n",
      "[Noun Extractor] postprocessing ignore_NJ : 777 -> 777\n",
      "[Noun Extractor] 777 nouns (203 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.170 Gb                    \n",
      "[Noun Extractor] 50.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5319 from 1 sents. mem=1.171 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10717, mem=1.171 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1509 words\n",
      "[Noun Extractor] checked compounds. discovered 177 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 582 -> 563\n",
      "[Noun Extractor] postprocessing ignore_features : 563 -> 543\n",
      "[Noun Extractor] postprocessing ignore_NJ : 543 -> 542\n",
      "[Noun Extractor] 542 nouns (177 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.171 Gb                    \n",
      "[Noun Extractor] 54.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5136 from 1 sents. mem=1.172 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10603, mem=1.172 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1489 words\n",
      "[Noun Extractor] checked compounds. discovered 155 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 521 -> 490\n",
      "[Noun Extractor] postprocessing ignore_features : 490 -> 468\n",
      "[Noun Extractor] postprocessing ignore_NJ : 468 -> 467\n",
      "[Noun Extractor] 467 nouns (155 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.172 Gb                    \n",
      "[Noun Extractor] 55.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6947 from 1 sents. mem=1.172 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13969, mem=1.172 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2030 words\n",
      "[Noun Extractor] checked compounds. discovered 246 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 777 -> 711\n",
      "[Noun Extractor] postprocessing ignore_features : 711 -> 688\n",
      "[Noun Extractor] postprocessing ignore_NJ : 688 -> 686\n",
      "[Noun Extractor] 686 nouns (246 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.172 Gb                    \n",
      "[Noun Extractor] 53.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3336 from 1 sents. mem=1.172 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8326, mem=1.172 Gb\n",
      "[Noun Extractor] batch prediction was completed for 956 words\n",
      "[Noun Extractor] checked compounds. discovered 94 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 367 -> 355\n",
      "[Noun Extractor] postprocessing ignore_features : 355 -> 343\n",
      "[Noun Extractor] postprocessing ignore_NJ : 343 -> 343\n",
      "[Noun Extractor] 343 nouns (94 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.173 Gb                    \n",
      "[Noun Extractor] 61.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5334 from 1 sents. mem=1.173 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11357, mem=1.173 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1404 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 513 -> 503\n",
      "[Noun Extractor] postprocessing ignore_features : 503 -> 486\n",
      "[Noun Extractor] postprocessing ignore_NJ : 486 -> 485\n",
      "[Noun Extractor] 485 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.173 Gb                    \n",
      "[Noun Extractor] 48.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7910 from 1 sents. mem=1.173 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16019, mem=1.173 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2428 words\n",
      "[Noun Extractor] checked compounds. discovered 280 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 856 -> 806\n",
      "[Noun Extractor] postprocessing ignore_features : 806 -> 783\n",
      "[Noun Extractor] postprocessing ignore_NJ : 783 -> 783\n",
      "[Noun Extractor] 783 nouns (280 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 52.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7670 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14405, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2201 words\n",
      "[Noun Extractor] checked compounds. discovered 221 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 848 -> 794\n",
      "[Noun Extractor] postprocessing ignore_features : 794 -> 761\n",
      "[Noun Extractor] postprocessing ignore_NJ : 761 -> 757\n",
      "[Noun Extractor] 757 nouns (221 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 52.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6353 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11476, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2141 words\n",
      "[Noun Extractor] checked compounds. discovered 125 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 663 -> 614\n",
      "[Noun Extractor] postprocessing ignore_features : 614 -> 584\n",
      "[Noun Extractor] postprocessing ignore_NJ : 584 -> 582\n",
      "[Noun Extractor] 582 nouns (125 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 48.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4970 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9254, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1612 words\n",
      "[Noun Extractor] checked compounds. discovered 86 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 486 -> 477\n",
      "[Noun Extractor] postprocessing ignore_features : 477 -> 458\n",
      "[Noun Extractor] postprocessing ignore_NJ : 458 -> 458\n",
      "[Noun Extractor] 458 nouns (86 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 48.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4940 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8595, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1476 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 414 -> 404\n",
      "[Noun Extractor] postprocessing ignore_features : 404 -> 386\n",
      "[Noun Extractor] postprocessing ignore_NJ : 386 -> 386\n",
      "[Noun Extractor] 386 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 39.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7566 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=17444, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2107 words\n",
      "[Noun Extractor] checked compounds. discovered 344 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 901 -> 816\n",
      "[Noun Extractor] postprocessing ignore_features : 816 -> 784\n",
      "[Noun Extractor] postprocessing ignore_NJ : 784 -> 779\n",
      "[Noun Extractor] 779 nouns (344 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 61.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4675 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8634, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1391 words\n",
      "[Noun Extractor] checked compounds. discovered 138 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 489 -> 460\n",
      "[Noun Extractor] postprocessing ignore_features : 460 -> 438\n",
      "[Noun Extractor] postprocessing ignore_NJ : 438 -> 435\n",
      "[Noun Extractor] 435 nouns (138 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 52.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5705 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11101, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1731 words\n",
      "[Noun Extractor] checked compounds. discovered 238 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 598\n",
      "[Noun Extractor] postprocessing ignore_features : 598 -> 574\n",
      "[Noun Extractor] postprocessing ignore_NJ : 574 -> 568\n",
      "[Noun Extractor] 568 nouns (238 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 55.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6135 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12305, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1861 words\n",
      "[Noun Extractor] checked compounds. discovered 233 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 725 -> 674\n",
      "[Noun Extractor] postprocessing ignore_features : 674 -> 647\n",
      "[Noun Extractor] postprocessing ignore_NJ : 647 -> 645\n",
      "[Noun Extractor] 645 nouns (233 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 56.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3440 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7451, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 817 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 277 -> 275\n",
      "[Noun Extractor] postprocessing ignore_features : 275 -> 267\n",
      "[Noun Extractor] postprocessing ignore_NJ : 267 -> 267\n",
      "[Noun Extractor] 267 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 26.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4263 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7984, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1249 words\n",
      "[Noun Extractor] checked compounds. discovered 73 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 367 -> 364\n",
      "[Noun Extractor] postprocessing ignore_features : 364 -> 353\n",
      "[Noun Extractor] postprocessing ignore_NJ : 353 -> 353\n",
      "[Noun Extractor] 353 nouns (73 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 43.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4359 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8615, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1221 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 449 -> 442\n",
      "[Noun Extractor] postprocessing ignore_features : 442 -> 430\n",
      "[Noun Extractor] postprocessing ignore_NJ : 430 -> 430\n",
      "[Noun Extractor] 430 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 54.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4596 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9423, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1339 words\n",
      "[Noun Extractor] checked compounds. discovered 157 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 540 -> 509\n",
      "[Noun Extractor] postprocessing ignore_features : 509 -> 489\n",
      "[Noun Extractor] postprocessing ignore_NJ : 489 -> 488\n",
      "[Noun Extractor] 488 nouns (157 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 57.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3285 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7593, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 970 words\n",
      "[Noun Extractor] checked compounds. discovered 87 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 331 -> 318\n",
      "[Noun Extractor] postprocessing ignore_features : 318 -> 301\n",
      "[Noun Extractor] postprocessing ignore_NJ : 301 -> 301\n",
      "[Noun Extractor] 301 nouns (87 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 62.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6419 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10327, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1893 words\n",
      "[Noun Extractor] checked compounds. discovered 91 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 590 -> 578\n",
      "[Noun Extractor] postprocessing ignore_features : 578 -> 559\n",
      "[Noun Extractor] postprocessing ignore_NJ : 559 -> 559\n",
      "[Noun Extractor] 559 nouns (91 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 40.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6422 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14470, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1682 words\n",
      "[Noun Extractor] checked compounds. discovered 208 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 686 -> 641\n",
      "[Noun Extractor] postprocessing ignore_features : 641 -> 620\n",
      "[Noun Extractor] postprocessing ignore_NJ : 620 -> 614\n",
      "[Noun Extractor] 614 nouns (208 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 52.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4066 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7670, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1192 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 346 -> 345\n",
      "[Noun Extractor] postprocessing ignore_features : 345 -> 331\n",
      "[Noun Extractor] postprocessing ignore_NJ : 331 -> 331\n",
      "[Noun Extractor] 331 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 45.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3418 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6464, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1009 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 315 -> 301\n",
      "[Noun Extractor] postprocessing ignore_features : 301 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 282\n",
      "[Noun Extractor] 282 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 46.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4276 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8722, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1231 words\n",
      "[Noun Extractor] checked compounds. discovered 129 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 477 -> 445\n",
      "[Noun Extractor] postprocessing ignore_features : 445 -> 431\n",
      "[Noun Extractor] postprocessing ignore_NJ : 431 -> 429\n",
      "[Noun Extractor] 429 nouns (129 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 55.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5047 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9778, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1524 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 527 -> 487\n",
      "[Noun Extractor] postprocessing ignore_features : 487 -> 467\n",
      "[Noun Extractor] postprocessing ignore_NJ : 467 -> 467\n",
      "[Noun Extractor] 467 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 35.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2366 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4851, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 460 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 40.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6656 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13524, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2011 words\n",
      "[Noun Extractor] checked compounds. discovered 236 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 768 -> 744\n",
      "[Noun Extractor] postprocessing ignore_features : 744 -> 726\n",
      "[Noun Extractor] postprocessing ignore_NJ : 726 -> 722\n",
      "[Noun Extractor] 722 nouns (236 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 56.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4299 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7776, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1335 words\n",
      "[Noun Extractor] checked compounds. discovered 127 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 488 -> 446\n",
      "[Noun Extractor] postprocessing ignore_features : 446 -> 422\n",
      "[Noun Extractor] postprocessing ignore_NJ : 422 -> 421\n",
      "[Noun Extractor] 421 nouns (127 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 51.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3566 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6753, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1157 words\n",
      "[Noun Extractor] checked compounds. discovered 141 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 425 -> 409\n",
      "[Noun Extractor] postprocessing ignore_features : 409 -> 396\n",
      "[Noun Extractor] postprocessing ignore_NJ : 396 -> 395\n",
      "[Noun Extractor] 395 nouns (141 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 54.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9147 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20631, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2231 words\n",
      "[Noun Extractor] checked compounds. discovered 304 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 922 -> 900\n",
      "[Noun Extractor] postprocessing ignore_features : 900 -> 876\n",
      "[Noun Extractor] postprocessing ignore_NJ : 876 -> 870\n",
      "[Noun Extractor] 870 nouns (304 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 51.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4125 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8078, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1244 words\n",
      "[Noun Extractor] checked compounds. discovered 169 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 517 -> 481\n",
      "[Noun Extractor] postprocessing ignore_features : 481 -> 453\n",
      "[Noun Extractor] postprocessing ignore_NJ : 453 -> 450\n",
      "[Noun Extractor] 450 nouns (169 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 54.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3670 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7413, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1139 words\n",
      "[Noun Extractor] checked compounds. discovered 111 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 383 -> 367\n",
      "[Noun Extractor] postprocessing ignore_features : 367 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (111 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 54.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9497 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20949, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2600 words\n",
      "[Noun Extractor] checked compounds. discovered 528 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1176 -> 1053\n",
      "[Noun Extractor] postprocessing ignore_features : 1053 -> 1035\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1035 -> 1033\n",
      "[Noun Extractor] 1033 nouns (528 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 58.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4637 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8929, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1385 words\n",
      "[Noun Extractor] checked compounds. discovered 130 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 502 -> 476\n",
      "[Noun Extractor] postprocessing ignore_features : 476 -> 454\n",
      "[Noun Extractor] postprocessing ignore_NJ : 454 -> 452\n",
      "[Noun Extractor] 452 nouns (130 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 53.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3096 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6470, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 902 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 316 -> 307\n",
      "[Noun Extractor] postprocessing ignore_features : 307 -> 292\n",
      "[Noun Extractor] postprocessing ignore_NJ : 292 -> 292\n",
      "[Noun Extractor] 292 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 49.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4211 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8256, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1197 words\n",
      "[Noun Extractor] checked compounds. discovered 131 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 458 -> 439\n",
      "[Noun Extractor] postprocessing ignore_features : 439 -> 418\n",
      "[Noun Extractor] postprocessing ignore_NJ : 418 -> 418\n",
      "[Noun Extractor] 418 nouns (131 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 56.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2639 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5810, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 801 words\n",
      "[Noun Extractor] checked compounds. discovered 60 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 275 -> 264\n",
      "[Noun Extractor] postprocessing ignore_features : 264 -> 256\n",
      "[Noun Extractor] postprocessing ignore_NJ : 256 -> 255\n",
      "[Noun Extractor] 255 nouns (60 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 56.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2430 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4873, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 727 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 218 -> 218\n",
      "[Noun Extractor] postprocessing ignore_features : 218 -> 210\n",
      "[Noun Extractor] postprocessing ignore_NJ : 210 -> 210\n",
      "[Noun Extractor] 210 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 48.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5706 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11190, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1676 words\n",
      "[Noun Extractor] checked compounds. discovered 189 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 678 -> 648\n",
      "[Noun Extractor] postprocessing ignore_features : 648 -> 626\n",
      "[Noun Extractor] postprocessing ignore_NJ : 626 -> 623\n",
      "[Noun Extractor] 623 nouns (189 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 55.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 25772 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=56037, mem=1.191 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6923 words\n",
      "[Noun Extractor] checked compounds. discovered 1192 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3139 -> 2940\n",
      "[Noun Extractor] postprocessing ignore_features : 2940 -> 2884\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2884 -> 2879\n",
      "[Noun Extractor] 2879 nouns (1192 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.192 Gb                    \n",
      "[Noun Extractor] 52.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3562 from 1 sents. mem=1.192 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7077, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1065 words\n",
      "[Noun Extractor] checked compounds. discovered 140 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 426 -> 404\n",
      "[Noun Extractor] postprocessing ignore_features : 404 -> 387\n",
      "[Noun Extractor] postprocessing ignore_NJ : 387 -> 383\n",
      "[Noun Extractor] 383 nouns (140 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 56.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3464 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6153, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1005 words\n",
      "[Noun Extractor] checked compounds. discovered 108 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 391 -> 381\n",
      "[Noun Extractor] postprocessing ignore_features : 381 -> 358\n",
      "[Noun Extractor] postprocessing ignore_NJ : 358 -> 357\n",
      "[Noun Extractor] 357 nouns (108 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 51.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5155 from 1 sents. mem=1.171 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8723, mem=1.172 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1651 words\n",
      "[Noun Extractor] checked compounds. discovered 109 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 564 -> 546\n",
      "[Noun Extractor] postprocessing ignore_features : 546 -> 517\n",
      "[Noun Extractor] postprocessing ignore_NJ : 517 -> 516\n",
      "[Noun Extractor] 516 nouns (109 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.173 Gb                    \n",
      "[Noun Extractor] 46.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4325 from 1 sents. mem=1.173 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8235, mem=1.173 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1311 words\n",
      "[Noun Extractor] checked compounds. discovered 121 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 455 -> 441\n",
      "[Noun Extractor] postprocessing ignore_features : 441 -> 429\n",
      "[Noun Extractor] postprocessing ignore_NJ : 429 -> 427\n",
      "[Noun Extractor] 427 nouns (121 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 54.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2918 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5600, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 901 words\n",
      "[Noun Extractor] checked compounds. discovered 75 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 308 -> 301\n",
      "[Noun Extractor] postprocessing ignore_features : 301 -> 290\n",
      "[Noun Extractor] postprocessing ignore_NJ : 290 -> 289\n",
      "[Noun Extractor] 289 nouns (75 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 51.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4455 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8509, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1301 words\n",
      "[Noun Extractor] checked compounds. discovered 105 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 467 -> 445\n",
      "[Noun Extractor] postprocessing ignore_features : 445 -> 418\n",
      "[Noun Extractor] postprocessing ignore_NJ : 418 -> 417\n",
      "[Noun Extractor] 417 nouns (105 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 38.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3160 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5725, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 889 words\n",
      "[Noun Extractor] checked compounds. discovered 88 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 313 -> 295\n",
      "[Noun Extractor] postprocessing ignore_features : 295 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 278\n",
      "[Noun Extractor] 278 nouns (88 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 51.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3236 from 1 sents. mem=1.174 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6211, mem=1.174 Gb\n",
      "[Noun Extractor] batch prediction was completed for 983 words\n",
      "[Noun Extractor] checked compounds. discovered 61 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 345 -> 339\n",
      "[Noun Extractor] postprocessing ignore_features : 339 -> 327\n",
      "[Noun Extractor] postprocessing ignore_NJ : 327 -> 326\n",
      "[Noun Extractor] 326 nouns (61 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.174 Gb                    \n",
      "[Noun Extractor] 48.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4960 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8057, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1603 words\n",
      "[Noun Extractor] checked compounds. discovered 88 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 490 -> 481\n",
      "[Noun Extractor] postprocessing ignore_features : 481 -> 469\n",
      "[Noun Extractor] postprocessing ignore_NJ : 469 -> 467\n",
      "[Noun Extractor] 467 nouns (88 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 46.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3795 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7101, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1188 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 458 -> 433\n",
      "[Noun Extractor] postprocessing ignore_features : 433 -> 410\n",
      "[Noun Extractor] postprocessing ignore_NJ : 410 -> 407\n",
      "[Noun Extractor] 407 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 54.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3461 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6430, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1017 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 326 -> 320\n",
      "[Noun Extractor] postprocessing ignore_features : 320 -> 311\n",
      "[Noun Extractor] postprocessing ignore_NJ : 311 -> 309\n",
      "[Noun Extractor] 309 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 52.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3372 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5942, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 997 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 341 -> 322\n",
      "[Noun Extractor] postprocessing ignore_features : 322 -> 306\n",
      "[Noun Extractor] postprocessing ignore_NJ : 306 -> 304\n",
      "[Noun Extractor] 304 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 50.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3500 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5801, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1185 words\n",
      "[Noun Extractor] checked compounds. discovered 71 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 351 -> 320\n",
      "[Noun Extractor] postprocessing ignore_features : 320 -> 305\n",
      "[Noun Extractor] postprocessing ignore_NJ : 305 -> 302\n",
      "[Noun Extractor] 302 nouns (71 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 45.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3976 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6772, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1231 words\n",
      "[Noun Extractor] checked compounds. discovered 82 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 387 -> 373\n",
      "[Noun Extractor] postprocessing ignore_features : 373 -> 357\n",
      "[Noun Extractor] postprocessing ignore_NJ : 357 -> 357\n",
      "[Noun Extractor] 357 nouns (82 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 47.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2246 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4711, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 663 words\n",
      "[Noun Extractor] checked compounds. discovered 52 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 215 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 205\n",
      "[Noun Extractor] postprocessing ignore_NJ : 205 -> 205\n",
      "[Noun Extractor] 205 nouns (52 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 54.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3072 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5441, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 967 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 289 -> 282\n",
      "[Noun Extractor] postprocessing ignore_features : 282 -> 270\n",
      "[Noun Extractor] postprocessing ignore_NJ : 270 -> 269\n",
      "[Noun Extractor] 269 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 49.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2527 from 1 sents. mem=1.175 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5163, mem=1.175 Gb\n",
      "[Noun Extractor] batch prediction was completed for 753 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 246 -> 244\n",
      "[Noun Extractor] postprocessing ignore_features : 244 -> 231\n",
      "[Noun Extractor] postprocessing ignore_NJ : 231 -> 228\n",
      "[Noun Extractor] 228 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.175 Gb                    \n",
      "[Noun Extractor] 54.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5843 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10914, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1671 words\n",
      "[Noun Extractor] checked compounds. discovered 172 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 609\n",
      "[Noun Extractor] postprocessing ignore_features : 609 -> 589\n",
      "[Noun Extractor] postprocessing ignore_NJ : 589 -> 585\n",
      "[Noun Extractor] 585 nouns (172 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 53.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5438 from 1 sents. mem=1.176 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10209, mem=1.176 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1980 words\n",
      "[Noun Extractor] checked compounds. discovered 165 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 661 -> 618\n",
      "[Noun Extractor] postprocessing ignore_features : 618 -> 600\n",
      "[Noun Extractor] postprocessing ignore_NJ : 600 -> 596\n",
      "[Noun Extractor] 596 nouns (165 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.176 Gb                    \n",
      "[Noun Extractor] 55.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4911 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9157, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1570 words\n",
      "[Noun Extractor] checked compounds. discovered 239 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 643 -> 581\n",
      "[Noun Extractor] postprocessing ignore_features : 581 -> 555\n",
      "[Noun Extractor] postprocessing ignore_NJ : 555 -> 548\n",
      "[Noun Extractor] 548 nouns (239 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 56.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4455 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8234, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1326 words\n",
      "[Noun Extractor] checked compounds. discovered 126 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 485 -> 462\n",
      "[Noun Extractor] postprocessing ignore_features : 462 -> 443\n",
      "[Noun Extractor] postprocessing ignore_NJ : 443 -> 437\n",
      "[Noun Extractor] 437 nouns (126 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 50.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2550 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4703, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 770 words\n",
      "[Noun Extractor] checked compounds. discovered 51 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 274 -> 262\n",
      "[Noun Extractor] postprocessing ignore_features : 262 -> 252\n",
      "[Noun Extractor] postprocessing ignore_NJ : 252 -> 252\n",
      "[Noun Extractor] 252 nouns (51 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 52.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2742 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4919, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 877 words\n",
      "[Noun Extractor] checked compounds. discovered 53 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 291 -> 286\n",
      "[Noun Extractor] postprocessing ignore_features : 286 -> 273\n",
      "[Noun Extractor] postprocessing ignore_NJ : 273 -> 270\n",
      "[Noun Extractor] 270 nouns (53 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 51.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2323 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4218, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 631 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 220 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 202\n",
      "[Noun Extractor] postprocessing ignore_NJ : 202 -> 202\n",
      "[Noun Extractor] 202 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 49.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2410 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4503, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 748 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 274 -> 265\n",
      "[Noun Extractor] postprocessing ignore_features : 265 -> 251\n",
      "[Noun Extractor] postprocessing ignore_NJ : 251 -> 249\n",
      "[Noun Extractor] 249 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 54.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3611 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6087, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1231 words\n",
      "[Noun Extractor] checked compounds. discovered 108 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 405 -> 384\n",
      "[Noun Extractor] postprocessing ignore_features : 384 -> 365\n",
      "[Noun Extractor] postprocessing ignore_NJ : 365 -> 365\n",
      "[Noun Extractor] 365 nouns (108 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 48.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2474 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4358, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 829 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 257 -> 244\n",
      "[Noun Extractor] postprocessing ignore_features : 244 -> 225\n",
      "[Noun Extractor] postprocessing ignore_NJ : 225 -> 225\n",
      "[Noun Extractor] 225 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 46.35 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2272 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4102, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 681 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 240 -> 234\n",
      "[Noun Extractor] postprocessing ignore_features : 234 -> 223\n",
      "[Noun Extractor] postprocessing ignore_NJ : 223 -> 223\n",
      "[Noun Extractor] 223 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 46.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2402 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5807, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 617 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 215 -> 212\n",
      "[Noun Extractor] postprocessing ignore_features : 212 -> 201\n",
      "[Noun Extractor] postprocessing ignore_NJ : 201 -> 200\n",
      "[Noun Extractor] 200 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 38.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2987 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5137, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 950 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 279 -> 272\n",
      "[Noun Extractor] postprocessing ignore_features : 272 -> 251\n",
      "[Noun Extractor] postprocessing ignore_NJ : 251 -> 251\n",
      "[Noun Extractor] 251 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 47.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3407 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5314, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1073 words\n",
      "[Noun Extractor] checked compounds. discovered 61 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 334 -> 323\n",
      "[Noun Extractor] postprocessing ignore_features : 323 -> 304\n",
      "[Noun Extractor] postprocessing ignore_NJ : 304 -> 303\n",
      "[Noun Extractor] 303 nouns (61 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 45.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2633 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4824, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 801 words\n",
      "[Noun Extractor] checked compounds. discovered 58 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 272 -> 269\n",
      "[Noun Extractor] postprocessing ignore_features : 269 -> 258\n",
      "[Noun Extractor] postprocessing ignore_NJ : 258 -> 257\n",
      "[Noun Extractor] 257 nouns (58 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 51.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3424 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6248, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1032 words\n",
      "[Noun Extractor] checked compounds. discovered 97 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 359 -> 342\n",
      "[Noun Extractor] postprocessing ignore_features : 342 -> 329\n",
      "[Noun Extractor] postprocessing ignore_NJ : 329 -> 329\n",
      "[Noun Extractor] 329 nouns (97 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 49.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2990 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5776, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 845 words\n",
      "[Noun Extractor] checked compounds. discovered 76 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 331 -> 329\n",
      "[Noun Extractor] postprocessing ignore_features : 329 -> 317\n",
      "[Noun Extractor] postprocessing ignore_NJ : 317 -> 317\n",
      "[Noun Extractor] 317 nouns (76 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 43.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2530 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3996, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 778 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 200 -> 199\n",
      "[Noun Extractor] postprocessing ignore_features : 199 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 35.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4938 from 1 sents. mem=1.177 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8697, mem=1.177 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1489 words\n",
      "[Noun Extractor] checked compounds. discovered 164 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 594 -> 553\n",
      "[Noun Extractor] postprocessing ignore_features : 553 -> 538\n",
      "[Noun Extractor] postprocessing ignore_NJ : 538 -> 533\n",
      "[Noun Extractor] 533 nouns (164 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.177 Gb                    \n",
      "[Noun Extractor] 54.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4435 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7643, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1466 words\n",
      "[Noun Extractor] checked compounds. discovered 159 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 554 -> 522\n",
      "[Noun Extractor] postprocessing ignore_features : 522 -> 503\n",
      "[Noun Extractor] postprocessing ignore_NJ : 503 -> 501\n",
      "[Noun Extractor] 501 nouns (159 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 52.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2285 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3976, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 710 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 218 -> 217\n",
      "[Noun Extractor] postprocessing ignore_features : 217 -> 205\n",
      "[Noun Extractor] postprocessing ignore_NJ : 205 -> 205\n",
      "[Noun Extractor] 205 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 50.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6705 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12812, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1971 words\n",
      "[Noun Extractor] checked compounds. discovered 251 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 750 -> 692\n",
      "[Noun Extractor] postprocessing ignore_features : 692 -> 669\n",
      "[Noun Extractor] postprocessing ignore_NJ : 669 -> 668\n",
      "[Noun Extractor] 668 nouns (251 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 55.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4280 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7034, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1419 words\n",
      "[Noun Extractor] checked compounds. discovered 77 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 451 -> 446\n",
      "[Noun Extractor] postprocessing ignore_features : 446 -> 423\n",
      "[Noun Extractor] postprocessing ignore_NJ : 423 -> 422\n",
      "[Noun Extractor] 422 nouns (77 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 45.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2194 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3870, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 611 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 197 -> 197\n",
      "[Noun Extractor] postprocessing ignore_features : 197 -> 189\n",
      "[Noun Extractor] postprocessing ignore_NJ : 189 -> 187\n",
      "[Noun Extractor] 187 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 51.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2573 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4701, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 745 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 200 -> 200\n",
      "[Noun Extractor] postprocessing ignore_features : 200 -> 189\n",
      "[Noun Extractor] postprocessing ignore_NJ : 189 -> 189\n",
      "[Noun Extractor] 189 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 42.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2911 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4957, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 821 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 215\n",
      "[Noun Extractor] postprocessing ignore_features : 215 -> 201\n",
      "[Noun Extractor] postprocessing ignore_NJ : 201 -> 201\n",
      "[Noun Extractor] 201 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 32.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6218 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10640, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2046 words\n",
      "[Noun Extractor] checked compounds. discovered 243 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 793 -> 770\n",
      "[Noun Extractor] postprocessing ignore_features : 770 -> 744\n",
      "[Noun Extractor] postprocessing ignore_NJ : 744 -> 737\n",
      "[Noun Extractor] 737 nouns (243 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 53.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3447 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6452, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1043 words\n",
      "[Noun Extractor] checked compounds. discovered 84 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 358 -> 353\n",
      "[Noun Extractor] postprocessing ignore_features : 353 -> 344\n",
      "[Noun Extractor] postprocessing ignore_NJ : 344 -> 344\n",
      "[Noun Extractor] 344 nouns (84 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 47.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2564 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4357, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 826 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 249 -> 245\n",
      "[Noun Extractor] postprocessing ignore_features : 245 -> 232\n",
      "[Noun Extractor] postprocessing ignore_NJ : 232 -> 230\n",
      "[Noun Extractor] 230 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 47.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2206 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3992, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 692 words\n",
      "[Noun Extractor] checked compounds. discovered 55 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 216\n",
      "[Noun Extractor] postprocessing ignore_features : 216 -> 202\n",
      "[Noun Extractor] postprocessing ignore_NJ : 202 -> 200\n",
      "[Noun Extractor] 200 nouns (55 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 53.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2224 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3980, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 721 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 210\n",
      "[Noun Extractor] postprocessing ignore_features : 210 -> 200\n",
      "[Noun Extractor] postprocessing ignore_NJ : 200 -> 200\n",
      "[Noun Extractor] 200 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 45.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2435 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3965, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 823 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 254 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 220\n",
      "[Noun Extractor] postprocessing ignore_NJ : 220 -> 220\n",
      "[Noun Extractor] 220 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 46.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2465 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4680, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 712 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 250 -> 244\n",
      "[Noun Extractor] postprocessing ignore_features : 244 -> 233\n",
      "[Noun Extractor] postprocessing ignore_NJ : 233 -> 232\n",
      "[Noun Extractor] 232 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 55.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2566 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4753, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 783 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 251 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 236\n",
      "[Noun Extractor] postprocessing ignore_NJ : 236 -> 236\n",
      "[Noun Extractor] 236 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 51.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1847 from 1 sents. mem=1.178 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3185, mem=1.178 Gb\n",
      "[Noun Extractor] batch prediction was completed for 579 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 149\n",
      "[Noun Extractor] postprocessing ignore_NJ : 149 -> 149\n",
      "[Noun Extractor] 149 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.178 Gb                    \n",
      "[Noun Extractor] 48.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2481 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3955, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 797 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 213 -> 209\n",
      "[Noun Extractor] postprocessing ignore_features : 209 -> 196\n",
      "[Noun Extractor] postprocessing ignore_NJ : 196 -> 196\n",
      "[Noun Extractor] 196 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 44.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4422 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8491, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1379 words\n",
      "[Noun Extractor] checked compounds. discovered 154 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 503 -> 493\n",
      "[Noun Extractor] postprocessing ignore_features : 493 -> 474\n",
      "[Noun Extractor] postprocessing ignore_NJ : 474 -> 474\n",
      "[Noun Extractor] 474 nouns (154 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 56.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2156 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3732, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 591 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 179 -> 172\n",
      "[Noun Extractor] postprocessing ignore_features : 172 -> 162\n",
      "[Noun Extractor] postprocessing ignore_NJ : 162 -> 162\n",
      "[Noun Extractor] 162 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 42.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2979 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5187, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 906 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 295 -> 293\n",
      "[Noun Extractor] postprocessing ignore_features : 293 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 281\n",
      "[Noun Extractor] 281 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 43.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2184 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3714, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 669 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 197\n",
      "[Noun Extractor] postprocessing ignore_features : 197 -> 188\n",
      "[Noun Extractor] postprocessing ignore_NJ : 188 -> 188\n",
      "[Noun Extractor] 188 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 49.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3030 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5414, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 939 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 302 -> 291\n",
      "[Noun Extractor] postprocessing ignore_features : 291 -> 279\n",
      "[Noun Extractor] postprocessing ignore_NJ : 279 -> 278\n",
      "[Noun Extractor] 278 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 48.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4114 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6464, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1384 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 423 -> 415\n",
      "[Noun Extractor] postprocessing ignore_features : 415 -> 394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 394 -> 392\n",
      "[Noun Extractor] 392 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 43.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2014 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3249, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 673 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 186 -> 181\n",
      "[Noun Extractor] postprocessing ignore_features : 181 -> 175\n",
      "[Noun Extractor] postprocessing ignore_NJ : 175 -> 175\n",
      "[Noun Extractor] 175 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 45.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4255 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8226, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1300 words\n",
      "[Noun Extractor] checked compounds. discovered 108 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 440 -> 396\n",
      "[Noun Extractor] postprocessing ignore_features : 396 -> 381\n",
      "[Noun Extractor] postprocessing ignore_NJ : 381 -> 380\n",
      "[Noun Extractor] 380 nouns (108 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 50.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5352 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8699, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1663 words\n",
      "[Noun Extractor] checked compounds. discovered 95 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 546 -> 536\n",
      "[Noun Extractor] postprocessing ignore_features : 536 -> 517\n",
      "[Noun Extractor] postprocessing ignore_NJ : 517 -> 515\n",
      "[Noun Extractor] 515 nouns (95 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 47.97 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2586 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4543, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 791 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 236 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 219\n",
      "[Noun Extractor] postprocessing ignore_NJ : 219 -> 219\n",
      "[Noun Extractor] 219 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 26.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1876 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3305, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 589 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 167 -> 165\n",
      "[Noun Extractor] postprocessing ignore_features : 165 -> 157\n",
      "[Noun Extractor] postprocessing ignore_NJ : 157 -> 157\n",
      "[Noun Extractor] 157 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 45.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4356 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8072, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1467 words\n",
      "[Noun Extractor] checked compounds. discovered 126 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 538 -> 498\n",
      "[Noun Extractor] postprocessing ignore_features : 498 -> 478\n",
      "[Noun Extractor] postprocessing ignore_NJ : 478 -> 477\n",
      "[Noun Extractor] 477 nouns (126 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 53.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3340 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5563, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1168 words\n",
      "[Noun Extractor] checked compounds. discovered 55 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 333 -> 325\n",
      "[Noun Extractor] postprocessing ignore_features : 325 -> 314\n",
      "[Noun Extractor] postprocessing ignore_NJ : 314 -> 314\n",
      "[Noun Extractor] 314 nouns (55 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 37.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1978 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3819, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 573 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 182 -> 182\n",
      "[Noun Extractor] postprocessing ignore_features : 182 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 174\n",
      "[Noun Extractor] 174 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 48.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1495 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2756, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 400 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 120 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 112\n",
      "[Noun Extractor] 112 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 45.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2669 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4896, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 888 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 308 -> 288\n",
      "[Noun Extractor] postprocessing ignore_features : 288 -> 276\n",
      "[Noun Extractor] postprocessing ignore_NJ : 276 -> 274\n",
      "[Noun Extractor] 274 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 54.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1880 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3510, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 582 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 170\n",
      "[Noun Extractor] postprocessing ignore_NJ : 170 -> 170\n",
      "[Noun Extractor] 170 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 52.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3574 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6700, mem=1.179 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1024 words\n",
      "[Noun Extractor] checked compounds. discovered 91 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 341 -> 332\n",
      "[Noun Extractor] postprocessing ignore_features : 332 -> 314\n",
      "[Noun Extractor] postprocessing ignore_NJ : 314 -> 314\n",
      "[Noun Extractor] 314 nouns (91 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.179 Gb                    \n",
      "[Noun Extractor] 52.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1834 from 1 sents. mem=1.179 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3276, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 571 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 188 -> 180\n",
      "[Noun Extractor] postprocessing ignore_features : 180 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 45.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2697 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4354, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 766 words\n",
      "[Noun Extractor] checked compounds. discovered 52 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 225\n",
      "[Noun Extractor] postprocessing ignore_features : 225 -> 217\n",
      "[Noun Extractor] postprocessing ignore_NJ : 217 -> 216\n",
      "[Noun Extractor] 216 nouns (52 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 42.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2376 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4365, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 750 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 220 -> 217\n",
      "[Noun Extractor] postprocessing ignore_features : 217 -> 208\n",
      "[Noun Extractor] postprocessing ignore_NJ : 208 -> 206\n",
      "[Noun Extractor] 206 nouns (32 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 47.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2121 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3782, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 664 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 217 -> 211\n",
      "[Noun Extractor] postprocessing ignore_features : 211 -> 200\n",
      "[Noun Extractor] postprocessing ignore_NJ : 200 -> 197\n",
      "[Noun Extractor] 197 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 51.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1721 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2830, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 524 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 172 -> 168\n",
      "[Noun Extractor] postprocessing ignore_features : 168 -> 158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 158 -> 158\n",
      "[Noun Extractor] 158 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 45.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3293 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5258, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1230 words\n",
      "[Noun Extractor] checked compounds. discovered 72 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 307\n",
      "[Noun Extractor] postprocessing ignore_features : 307 -> 294\n",
      "[Noun Extractor] postprocessing ignore_NJ : 294 -> 294\n",
      "[Noun Extractor] 294 nouns (72 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 40.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2181 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3689, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 710 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 215\n",
      "[Noun Extractor] postprocessing ignore_features : 215 -> 206\n",
      "[Noun Extractor] postprocessing ignore_NJ : 206 -> 205\n",
      "[Noun Extractor] 205 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 50.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1594 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2984, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 435 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 131 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 49.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4103 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6527, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1285 words\n",
      "[Noun Extractor] checked compounds. discovered 90 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 403 -> 390\n",
      "[Noun Extractor] postprocessing ignore_features : 390 -> 372\n",
      "[Noun Extractor] postprocessing ignore_NJ : 372 -> 372\n",
      "[Noun Extractor] 372 nouns (90 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 37.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1565 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2766, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 431 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 25.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1957 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3691, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 557 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 180\n",
      "[Noun Extractor] postprocessing ignore_features : 180 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 168\n",
      "[Noun Extractor] 168 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 51.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2547 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4224, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 842 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 280 -> 272\n",
      "[Noun Extractor] postprocessing ignore_features : 272 -> 262\n",
      "[Noun Extractor] postprocessing ignore_NJ : 262 -> 261\n",
      "[Noun Extractor] 261 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 50.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3420 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5254, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1371 words\n",
      "[Noun Extractor] checked compounds. discovered 88 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 373 -> 371\n",
      "[Noun Extractor] postprocessing ignore_features : 371 -> 358\n",
      "[Noun Extractor] postprocessing ignore_NJ : 358 -> 358\n",
      "[Noun Extractor] 358 nouns (88 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 40.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2220 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3359, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 808 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 189\n",
      "[Noun Extractor] postprocessing ignore_features : 189 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 40.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1854 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2951, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 628 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 163\n",
      "[Noun Extractor] 163 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 46.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3017 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5254, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 992 words\n",
      "[Noun Extractor] checked compounds. discovered 63 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 322 -> 305\n",
      "[Noun Extractor] postprocessing ignore_features : 305 -> 290\n",
      "[Noun Extractor] postprocessing ignore_NJ : 290 -> 288\n",
      "[Noun Extractor] 288 nouns (63 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 48.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3057 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4590, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 960 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 227 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 210\n",
      "[Noun Extractor] postprocessing ignore_NJ : 210 -> 208\n",
      "[Noun Extractor] 208 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 36.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1700 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2836, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 512 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 42.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2135 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3614, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 679 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 196 -> 193\n",
      "[Noun Extractor] postprocessing ignore_features : 193 -> 184\n",
      "[Noun Extractor] postprocessing ignore_NJ : 184 -> 184\n",
      "[Noun Extractor] 184 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 46.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2283 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3945, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 670 words\n",
      "[Noun Extractor] checked compounds. discovered 53 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 218\n",
      "[Noun Extractor] postprocessing ignore_features : 218 -> 205\n",
      "[Noun Extractor] postprocessing ignore_NJ : 205 -> 204\n",
      "[Noun Extractor] 204 nouns (53 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 48.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2043 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3465, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 605 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 175 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 43.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1637 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2876, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 527 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 163\n",
      "[Noun Extractor] 163 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 51.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1767 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2869, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 603 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 157 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 149\n",
      "[Noun Extractor] postprocessing ignore_NJ : 149 -> 148\n",
      "[Noun Extractor] 148 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 44.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2327 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3977, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 739 words\n",
      "[Noun Extractor] checked compounds. discovered 51 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 232 -> 224\n",
      "[Noun Extractor] postprocessing ignore_features : 224 -> 214\n",
      "[Noun Extractor] postprocessing ignore_NJ : 214 -> 214\n",
      "[Noun Extractor] 214 nouns (51 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 48.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1993 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3180, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 655 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 169\n",
      "[Noun Extractor] 169 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 42.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1843 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3072, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 163 -> 160\n",
      "[Noun Extractor] postprocessing ignore_features : 160 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 41.73 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1546 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3127, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 49.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1848 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3106, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 587 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 146\n",
      "[Noun Extractor] postprocessing ignore_NJ : 146 -> 146\n",
      "[Noun Extractor] 146 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 45.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1687 from 1 sents. mem=1.180 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2850, mem=1.180 Gb\n",
      "[Noun Extractor] batch prediction was completed for 530 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 179 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 169\n",
      "[Noun Extractor] 169 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.180 Gb                    \n",
      "[Noun Extractor] 49.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2878 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4633, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 833 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 232\n",
      "[Noun Extractor] postprocessing ignore_NJ : 232 -> 232\n",
      "[Noun Extractor] 232 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 35.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1850 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3156, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 594 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 186 -> 180\n",
      "[Noun Extractor] postprocessing ignore_features : 180 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 171\n",
      "[Noun Extractor] 171 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 51.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1977 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3257, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 606 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 194 -> 194\n",
      "[Noun Extractor] postprocessing ignore_features : 194 -> 185\n",
      "[Noun Extractor] postprocessing ignore_NJ : 185 -> 184\n",
      "[Noun Extractor] 184 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 43.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1974 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3310, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 585 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 177\n",
      "[Noun Extractor] postprocessing ignore_features : 177 -> 172\n",
      "[Noun Extractor] postprocessing ignore_NJ : 172 -> 170\n",
      "[Noun Extractor] 170 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 47.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4423 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8042, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1207 words\n",
      "[Noun Extractor] checked compounds. discovered 127 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 446 -> 438\n",
      "[Noun Extractor] postprocessing ignore_features : 438 -> 429\n",
      "[Noun Extractor] postprocessing ignore_NJ : 429 -> 429\n",
      "[Noun Extractor] 429 nouns (127 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 48.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1172 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2274, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 42.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2336 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3659, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 807 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 201 -> 200\n",
      "[Noun Extractor] postprocessing ignore_features : 200 -> 190\n",
      "[Noun Extractor] postprocessing ignore_NJ : 190 -> 189\n",
      "[Noun Extractor] 189 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 42.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1607 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2688, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 497 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 42.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7450 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14680, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2384 words\n",
      "[Noun Extractor] checked compounds. discovered 451 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1125 -> 957\n",
      "[Noun Extractor] postprocessing ignore_features : 957 -> 921\n",
      "[Noun Extractor] postprocessing ignore_NJ : 921 -> 911\n",
      "[Noun Extractor] 911 nouns (451 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 59.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2250 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3765, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 670 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 154\n",
      "[Noun Extractor] postprocessing ignore_NJ : 154 -> 154\n",
      "[Noun Extractor] 154 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 35.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3116 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5051, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1041 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 319 -> 311\n",
      "[Noun Extractor] postprocessing ignore_features : 311 -> 296\n",
      "[Noun Extractor] postprocessing ignore_NJ : 296 -> 296\n",
      "[Noun Extractor] 296 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 43.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1648 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2758, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 499 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 149 -> 149\n",
      "[Noun Extractor] postprocessing ignore_features : 149 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 40.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1465 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2558, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 429 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 50.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1806 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3166, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 567 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 179 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 170\n",
      "[Noun Extractor] 170 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 47.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1567 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2741, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 493 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 52.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3115 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5045, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 931 words\n",
      "[Noun Extractor] checked compounds. discovered 74 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 336 -> 333\n",
      "[Noun Extractor] postprocessing ignore_features : 333 -> 326\n",
      "[Noun Extractor] postprocessing ignore_NJ : 326 -> 325\n",
      "[Noun Extractor] 325 nouns (74 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 50.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2327 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3570, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 820 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 236\n",
      "[Noun Extractor] postprocessing ignore_features : 236 -> 227\n",
      "[Noun Extractor] postprocessing ignore_NJ : 227 -> 227\n",
      "[Noun Extractor] 227 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 43.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1739 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3159, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 553 words\n",
      "[Noun Extractor] checked compounds. discovered 55 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 183 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 154\n",
      "[Noun Extractor] 154 nouns (55 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 48.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3673 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5612, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1164 words\n",
      "[Noun Extractor] checked compounds. discovered 126 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 443 -> 424\n",
      "[Noun Extractor] postprocessing ignore_features : 424 -> 409\n",
      "[Noun Extractor] postprocessing ignore_NJ : 409 -> 408\n",
      "[Noun Extractor] 408 nouns (126 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 41.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1317 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2850, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 60.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1659 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2727, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 154\n",
      "[Noun Extractor] postprocessing ignore_features : 154 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 49.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1941 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2937, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 714 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 39.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1901 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3117, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 722 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 206 -> 205\n",
      "[Noun Extractor] postprocessing ignore_features : 205 -> 199\n",
      "[Noun Extractor] postprocessing ignore_NJ : 199 -> 199\n",
      "[Noun Extractor] 199 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 42.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2006 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3382, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 644 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 223 -> 221\n",
      "[Noun Extractor] postprocessing ignore_features : 221 -> 213\n",
      "[Noun Extractor] postprocessing ignore_NJ : 213 -> 213\n",
      "[Noun Extractor] 213 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 53.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1455 from 1 sents. mem=1.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2387, mem=1.181 Gb\n",
      "[Noun Extractor] batch prediction was completed for 414 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.181 Gb                    \n",
      "[Noun Extractor] 39.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1643 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2708, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 583 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 168\n",
      "[Noun Extractor] postprocessing ignore_features : 168 -> 158\n",
      "[Noun Extractor] postprocessing ignore_NJ : 158 -> 157\n",
      "[Noun Extractor] 157 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 47.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1646 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2839, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 169 -> 165\n",
      "[Noun Extractor] postprocessing ignore_features : 165 -> 150\n",
      "[Noun Extractor] postprocessing ignore_NJ : 150 -> 149\n",
      "[Noun Extractor] 149 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 47.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2705 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4194, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 964 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 262 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 228\n",
      "[Noun Extractor] 228 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 46.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3206 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5281, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 940 words\n",
      "[Noun Extractor] checked compounds. discovered 71 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 308\n",
      "[Noun Extractor] postprocessing ignore_features : 308 -> 293\n",
      "[Noun Extractor] postprocessing ignore_NJ : 293 -> 291\n",
      "[Noun Extractor] 291 nouns (71 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 45.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1578 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2699, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 136\n",
      "[Noun Extractor] postprocessing ignore_NJ : 136 -> 136\n",
      "[Noun Extractor] 136 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 44.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1831 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2809, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 589 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 164 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 42.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2146 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3816, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 902 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 265 -> 259\n",
      "[Noun Extractor] postprocessing ignore_features : 259 -> 251\n",
      "[Noun Extractor] postprocessing ignore_NJ : 251 -> 251\n",
      "[Noun Extractor] 251 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 36.61 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2259 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3413, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 687 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 38.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1557 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2513, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 548 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 148 -> 147\n",
      "[Noun Extractor] postprocessing ignore_features : 147 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 45.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1527 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2496, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 464 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 158 -> 155\n",
      "[Noun Extractor] postprocessing ignore_features : 155 -> 150\n",
      "[Noun Extractor] postprocessing ignore_NJ : 150 -> 149\n",
      "[Noun Extractor] 149 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 46.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1497 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2355, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 487 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 130 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 40.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1763 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3070, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 528 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 162\n",
      "[Noun Extractor] postprocessing ignore_NJ : 162 -> 162\n",
      "[Noun Extractor] 162 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 49.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2023 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3292, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 667 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 181\n",
      "[Noun Extractor] postprocessing ignore_features : 181 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 44.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2200 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3340, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 705 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 184 -> 184\n",
      "[Noun Extractor] postprocessing ignore_features : 184 -> 178\n",
      "[Noun Extractor] postprocessing ignore_NJ : 178 -> 178\n",
      "[Noun Extractor] 178 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 38.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1841 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2787, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 606 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 189 -> 188\n",
      "[Noun Extractor] postprocessing ignore_features : 188 -> 178\n",
      "[Noun Extractor] postprocessing ignore_NJ : 178 -> 178\n",
      "[Noun Extractor] 178 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 44.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1723 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2573, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 573 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 156 -> 150\n",
      "[Noun Extractor] postprocessing ignore_features : 150 -> 144\n",
      "[Noun Extractor] postprocessing ignore_NJ : 144 -> 143\n",
      "[Noun Extractor] 143 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 43.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2181 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3272, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 696 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 180 -> 180\n",
      "[Noun Extractor] postprocessing ignore_features : 180 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 168\n",
      "[Noun Extractor] 168 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 38.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1272 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2468, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 53.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1738 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2880, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 565 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (31 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 47.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2264 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3249, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 724 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 172 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 166\n",
      "[Noun Extractor] 166 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 38.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2034 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3491, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 629 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 179 -> 173\n",
      "[Noun Extractor] postprocessing ignore_features : 173 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 50.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1117 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2149, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 45.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1826 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3138, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 532 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 162\n",
      "[Noun Extractor] postprocessing ignore_features : 162 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 144\n",
      "[Noun Extractor] 144 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 44.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1572 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2475, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 620 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 177 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 165\n",
      "[Noun Extractor] 165 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 45.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1467 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2539, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 464 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 43.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1294 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2081, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 525 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 131\n",
      "[Noun Extractor] postprocessing ignore_features : 131 -> 122\n",
      "[Noun Extractor] postprocessing ignore_NJ : 122 -> 122\n",
      "[Noun Extractor] 122 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 46.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1534 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2392, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 462 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 122\n",
      "[Noun Extractor] postprocessing ignore_NJ : 122 -> 122\n",
      "[Noun Extractor] 122 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 42.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1532 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2408, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 495 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 42.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2264 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3790, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 720 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 234 -> 230\n",
      "[Noun Extractor] postprocessing ignore_features : 230 -> 220\n",
      "[Noun Extractor] postprocessing ignore_NJ : 220 -> 220\n",
      "[Noun Extractor] 220 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 49.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1278 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2275, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 149 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 128\n",
      "[Noun Extractor] 128 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 48.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1303 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2189, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 402 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 103\n",
      "[Noun Extractor] 103 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 41.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1696 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2829, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 514 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 173\n",
      "[Noun Extractor] postprocessing ignore_features : 173 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 50.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1194 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2063, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 427 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 40.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1276 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2217, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 382 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 110\n",
      "[Noun Extractor] postprocessing ignore_NJ : 110 -> 110\n",
      "[Noun Extractor] 110 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 52.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1306 from 1 sents. mem=1.182 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2151, mem=1.182 Gb\n",
      "[Noun Extractor] batch prediction was completed for 402 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.182 Gb                    \n",
      "[Noun Extractor] 46.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1571 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2531, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 490 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 172 -> 169\n",
      "[Noun Extractor] postprocessing ignore_features : 169 -> 161\n",
      "[Noun Extractor] postprocessing ignore_NJ : 161 -> 160\n",
      "[Noun Extractor] 160 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 48.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1151 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1913, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 345 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 45.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2298 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3829, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 737 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 239 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 221\n",
      "[Noun Extractor] postprocessing ignore_NJ : 221 -> 221\n",
      "[Noun Extractor] 221 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 50.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1192 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2541, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 46.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7941 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15805, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2316 words\n",
      "[Noun Extractor] checked compounds. discovered 337 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 977 -> 879\n",
      "[Noun Extractor] postprocessing ignore_features : 879 -> 847\n",
      "[Noun Extractor] postprocessing ignore_NJ : 847 -> 843\n",
      "[Noun Extractor] 843 nouns (337 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 55.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1479 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2467, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 39.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1253 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2107, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 393 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 109\n",
      "[Noun Extractor] postprocessing ignore_features : 109 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 42.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1567 from 1 sents. mem=1.183 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2651, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 537 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 154 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 46.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1483 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2187, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 465 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 110 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 38.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2209 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3210, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 698 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 187\n",
      "[Noun Extractor] postprocessing ignore_features : 187 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 40.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1751 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2555, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 547 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 137\n",
      "[Noun Extractor] postprocessing ignore_features : 137 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1178 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2039, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 373 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 100\n",
      "[Noun Extractor] 100 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 47.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2039, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 444 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 41.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1408 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2468, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 407 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 114\n",
      "[Noun Extractor] 114 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 43.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1174 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1961, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 381 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 118 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 43.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2181 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3159, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 762 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 195 -> 192\n",
      "[Noun Extractor] postprocessing ignore_features : 192 -> 184\n",
      "[Noun Extractor] postprocessing ignore_NJ : 184 -> 184\n",
      "[Noun Extractor] 184 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 39.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1793 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3139, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 156 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 142\n",
      "[Noun Extractor] postprocessing ignore_NJ : 142 -> 142\n",
      "[Noun Extractor] 142 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 48.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2211 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3342, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 691 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 169 -> 165\n",
      "[Noun Extractor] postprocessing ignore_features : 165 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 160\n",
      "[Noun Extractor] 160 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 39.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2098 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4185, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 613 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 188\n",
      "[Noun Extractor] postprocessing ignore_features : 188 -> 175\n",
      "[Noun Extractor] postprocessing ignore_NJ : 175 -> 175\n",
      "[Noun Extractor] 175 nouns (28 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 47.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1240 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1913, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 396 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 117\n",
      "[Noun Extractor] 117 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 43.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1352 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2320, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 424 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 45.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2987 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4475, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1066 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 285 -> 280\n",
      "[Noun Extractor] postprocessing ignore_features : 280 -> 270\n",
      "[Noun Extractor] postprocessing ignore_NJ : 270 -> 269\n",
      "[Noun Extractor] 269 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 39.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1928 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3021, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 647 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 144 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 134\n",
      "[Noun Extractor] 134 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 36.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1514 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2214, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 454 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 101\n",
      "[Noun Extractor] 101 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 36.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2378 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4034, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 744 words\n",
      "[Noun Extractor] checked compounds. discovered 76 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 271 -> 257\n",
      "[Noun Extractor] postprocessing ignore_features : 257 -> 243\n",
      "[Noun Extractor] postprocessing ignore_NJ : 243 -> 238\n",
      "[Noun Extractor] 238 nouns (76 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 52.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 860 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2069, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 276 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 44.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2048 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3163, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 649 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 177\n",
      "[Noun Extractor] postprocessing ignore_features : 177 -> 168\n",
      "[Noun Extractor] postprocessing ignore_NJ : 168 -> 167\n",
      "[Noun Extractor] 167 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 42.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1627 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2640, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 563 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 131\n",
      "[Noun Extractor] 131 nouns (34 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 923 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1946, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 304 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 48.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1327 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2313, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 405 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 135\n",
      "[Noun Extractor] postprocessing ignore_features : 135 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 50.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1681 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2864, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 495 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 135 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 46.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1683 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2745, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 507 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 143\n",
      "[Noun Extractor] 143 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 47.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1019 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1637, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 258 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 35.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1314 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2013, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 509 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 141 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 39.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1618 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2313, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 485 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 38.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2377 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4039, mem=1.183 Gb\n",
      "[Noun Extractor] batch prediction was completed for 776 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 235\n",
      "[Noun Extractor] postprocessing ignore_NJ : 235 -> 235\n",
      "[Noun Extractor] 235 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.183 Gb                    \n",
      "[Noun Extractor] 48.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2623 from 1 sents. mem=1.183 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4235, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 849 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 262 -> 257\n",
      "[Noun Extractor] postprocessing ignore_features : 257 -> 246\n",
      "[Noun Extractor] postprocessing ignore_NJ : 246 -> 246\n",
      "[Noun Extractor] 246 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 46.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1164 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2188, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109 -> 109\n",
      "[Noun Extractor] postprocessing ignore_features : 109 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 46.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1253 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1914, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 42.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 982 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1666, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 47.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1618 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2779, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 449 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 34.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1147 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1920, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 85\n",
      "[Noun Extractor] 85 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 45.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1615 from 1 sents. mem=1.184 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2657, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 508 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 149\n",
      "[Noun Extractor] postprocessing ignore_features : 149 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 142\n",
      "[Noun Extractor] 142 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 50.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 765 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1769, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 45.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7887 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13841, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2220 words\n",
      "[Noun Extractor] checked compounds. discovered 150 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 756 -> 741\n",
      "[Noun Extractor] postprocessing ignore_features : 741 -> 718\n",
      "[Noun Extractor] postprocessing ignore_NJ : 718 -> 718\n",
      "[Noun Extractor] 718 nouns (150 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 42.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1090 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1732, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 411 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 40.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1977 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3117, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 578 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 149\n",
      "[Noun Extractor] postprocessing ignore_NJ : 149 -> 148\n",
      "[Noun Extractor] 148 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 43.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1516 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2357, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 460 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 45.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1379 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2205, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 449 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 40.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1762 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2894, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 586 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 158 -> 154\n",
      "[Noun Extractor] postprocessing ignore_features : 154 -> 139\n",
      "[Noun Extractor] postprocessing ignore_NJ : 139 -> 139\n",
      "[Noun Extractor] 139 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 44.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1780, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 274 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 43.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1140 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1742, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 339 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 37.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2276 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3972, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 743 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 233 -> 225\n",
      "[Noun Extractor] postprocessing ignore_features : 225 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 50.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1306 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1974, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 373 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 85\n",
      "[Noun Extractor] 85 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 33.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1748 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2593, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 566 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 34.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1192 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1779, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 396 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 39.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 864 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1496, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 246 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 44.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1239 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2060, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 365 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 109\n",
      "[Noun Extractor] 109 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 48.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 959 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1650, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 43.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1219 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1934, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 118 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 44.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1151 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1883, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 334 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 86\n",
      "[Noun Extractor] 86 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 41.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1310 from 1 sents. mem=1.184 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2011, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 427 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 135\n",
      "[Noun Extractor] postprocessing ignore_features : 135 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 123\n",
      "[Noun Extractor] 123 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.184 Gb                    \n",
      "[Noun Extractor] 39.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10052 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=19542, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3006 words\n",
      "[Noun Extractor] checked compounds. discovered 280 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1037 -> 993\n",
      "[Noun Extractor] postprocessing ignore_features : 993 -> 974\n",
      "[Noun Extractor] postprocessing ignore_NJ : 974 -> 973\n",
      "[Noun Extractor] 973 nouns (280 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 50.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1544 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2527, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 695 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 172 -> 172\n",
      "[Noun Extractor] postprocessing ignore_features : 172 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 42.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2874 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4284, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 933 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 291 -> 288\n",
      "[Noun Extractor] postprocessing ignore_features : 288 -> 279\n",
      "[Noun Extractor] postprocessing ignore_NJ : 279 -> 276\n",
      "[Noun Extractor] 276 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 43.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1350 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2055, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 446 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 120 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 108\n",
      "[Noun Extractor] 108 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1055 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1712, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 46.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3102 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4584, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1102 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 271 -> 266\n",
      "[Noun Extractor] postprocessing ignore_features : 266 -> 248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 248 -> 246\n",
      "[Noun Extractor] 246 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1525, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 329 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 24.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1318 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2001, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 443 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 38.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1149 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1687, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 37.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1316 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2322, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 385 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 98\n",
      "[Noun Extractor] 98 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 47.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1073 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1682, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 340 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 41.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1159 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1839, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 389 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 28.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 982 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1595, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 274 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 44.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1100 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1742, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 346 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 40.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2191 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3412, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 871 words\n",
      "[Noun Extractor] checked compounds. discovered 52 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 222 -> 219\n",
      "[Noun Extractor] postprocessing ignore_features : 219 -> 209\n",
      "[Noun Extractor] postprocessing ignore_NJ : 209 -> 209\n",
      "[Noun Extractor] 209 nouns (52 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 42.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2037 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3365, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 667 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 205 -> 201\n",
      "[Noun Extractor] postprocessing ignore_features : 201 -> 190\n",
      "[Noun Extractor] postprocessing ignore_NJ : 190 -> 188\n",
      "[Noun Extractor] 188 nouns (34 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 51.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1038 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1470, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 380 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 34.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 980 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1555, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 46.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1126 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1779, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 369 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 100\n",
      "[Noun Extractor] postprocessing ignore_features : 100 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 92\n",
      "[Noun Extractor] 92 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1040 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1607, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 41.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1123 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1715, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 372 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1542 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2288, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 479 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 123 -> 120\n",
      "[Noun Extractor] postprocessing ignore_features : 120 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 811 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1298, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1933 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2967, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 776 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 208 -> 208\n",
      "[Noun Extractor] postprocessing ignore_features : 208 -> 201\n",
      "[Noun Extractor] postprocessing ignore_NJ : 201 -> 201\n",
      "[Noun Extractor] 201 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 39.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1206 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1814, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 430 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 37.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1503 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2571, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 489 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 153 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 148\n",
      "[Noun Extractor] postprocessing ignore_NJ : 148 -> 146\n",
      "[Noun Extractor] 146 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 50.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1214 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2029, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 411 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 132\n",
      "[Noun Extractor] postprocessing ignore_NJ : 132 -> 132\n",
      "[Noun Extractor] 132 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 50.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1384 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2219, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 411 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 22.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1192 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1795, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 38.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 764 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1517, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 44.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1751 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2813, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 525 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 152\n",
      "[Noun Extractor] 152 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 43.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 912 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1477, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 285 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 73\n",
      "[Noun Extractor] 73 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 45.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1118 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1867, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 50.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1243 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2115, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 444 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 28.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1276 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1864, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 421 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 36.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1673 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2659, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 559 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 161 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 41.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1088 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1621, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 38.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 977 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1558, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 324 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 45.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1257 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2002, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 439 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 41.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1173 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1856, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 88 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 43.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 607 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1291, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 43.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 954 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1893, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 32.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 914 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1545, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 41.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 934 from 1 sents. mem=1.185 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1605, mem=1.185 Gb\n",
      "[Noun Extractor] batch prediction was completed for 325 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.185 Gb                    \n",
      "[Noun Extractor] 37.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1239 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2121, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 96\n",
      "[Noun Extractor] 96 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 47.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1100 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1958, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 469 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 100\n",
      "[Noun Extractor] postprocessing ignore_features : 100 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 41.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1227 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2025, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 404 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 31.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1137 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1840, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 376 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 107 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 916 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1435, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 70\n",
      "[Noun Extractor] 70 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 40.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1059 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1624, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 340 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 37.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 996 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1593, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 72\n",
      "[Noun Extractor] 72 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 993 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1450, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 288 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 38.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 907 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1533, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 254 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 75\n",
      "[Noun Extractor] 75 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1483, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 33.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1235 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1917, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 434 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 120 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 110\n",
      "[Noun Extractor] 110 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 47.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1356, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 258 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1330 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2001, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 119\n",
      "[Noun Extractor] 119 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 41.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4970 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8018, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1541 words\n",
      "[Noun Extractor] checked compounds. discovered 103 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 446 -> 412\n",
      "[Noun Extractor] postprocessing ignore_features : 412 -> 394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 394 -> 393\n",
      "[Noun Extractor] 393 nouns (103 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 42.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1079 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1632, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 352 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2066 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3123, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 633 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 174 -> 171\n",
      "[Noun Extractor] postprocessing ignore_features : 171 -> 162\n",
      "[Noun Extractor] postprocessing ignore_NJ : 162 -> 162\n",
      "[Noun Extractor] 162 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 37.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 864 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1330, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 36.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3032 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4437, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 911 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 250\n",
      "[Noun Extractor] postprocessing ignore_features : 250 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 238\n",
      "[Noun Extractor] 238 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 37.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1366 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2239, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 448 words\n",
      "[Noun Extractor] checked compounds. discovered 22 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 126\n",
      "[Noun Extractor] 126 nouns (22 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 852 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1328, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 285 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 65\n",
      "[Noun Extractor] 65 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3317 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5239, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1179 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 352 -> 339\n",
      "[Noun Extractor] postprocessing ignore_features : 339 -> 325\n",
      "[Noun Extractor] postprocessing ignore_NJ : 325 -> 324\n",
      "[Noun Extractor] 324 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 46.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1119 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1581, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 373 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 34.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1493 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2096, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 511 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 123 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 35.31 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1968 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3055, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 670 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 163 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 40.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 874 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1425, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 41.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1047 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1648, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 312 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 90\n",
      "[Noun Extractor] 90 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1021 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1636, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1943 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2764, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 647 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 846 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1357, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 266 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 929 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1481, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 41.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2958 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4855, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 947 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 289 -> 283\n",
      "[Noun Extractor] postprocessing ignore_features : 283 -> 271\n",
      "[Noun Extractor] postprocessing ignore_NJ : 271 -> 271\n",
      "[Noun Extractor] 271 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 47.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1124 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1698, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 351 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 101 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 95\n",
      "[Noun Extractor] 95 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 40.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2318 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3516, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 751 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 198\n",
      "[Noun Extractor] postprocessing ignore_features : 198 -> 194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 194 -> 194\n",
      "[Noun Extractor] 194 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 37.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 843 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1298, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1622 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2671, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 536 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 160 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 835 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1311, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 266 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 44.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 835 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1408, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 242 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1034 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1711, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 320 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 87\n",
      "[Noun Extractor] 87 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 46.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 866 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1314, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 278 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1363 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1879, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 387 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 102 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 38.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1282, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 35.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 811 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1240, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 41.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1172 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1792, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 36.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1031 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1475, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 333 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 35.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 926 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1534, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2419 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3520, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 809 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 223 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 852 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1448, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 65\n",
      "[Noun Extractor] 65 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 46.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 945 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1539, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 39.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 498 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1120, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 173 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 42.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1629 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2326, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 575 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 167 -> 164\n",
      "[Noun Extractor] postprocessing ignore_features : 164 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 152\n",
      "[Noun Extractor] 152 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1004 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1674, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 458 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 130 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 122\n",
      "[Noun Extractor] postprocessing ignore_NJ : 122 -> 122\n",
      "[Noun Extractor] 122 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 54.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1043 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1595, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 85\n",
      "[Noun Extractor] postprocessing ignore_features : 85 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 35.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1068 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1692, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 355 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 43.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 912 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1382, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 45.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 841 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1250, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 34.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1956 from 1 sents. mem=1.186 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3452, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 649 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 199 -> 198\n",
      "[Noun Extractor] postprocessing ignore_features : 198 -> 193\n",
      "[Noun Extractor] postprocessing ignore_NJ : 193 -> 192\n",
      "[Noun Extractor] 192 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 40.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 788 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1201, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 241 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 35.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1055 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1606, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 366 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 44.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1507 from 1 sents. mem=1.186 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2481, mem=1.186 Gb\n",
      "[Noun Extractor] batch prediction was completed for 486 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 148 -> 144\n",
      "[Noun Extractor] postprocessing ignore_features : 144 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.186 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 905 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1406, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 283 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 42.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 804 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1232, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 59\n",
      "[Noun Extractor] 59 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 38.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1858, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 118 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 110\n",
      "[Noun Extractor] postprocessing ignore_NJ : 110 -> 108\n",
      "[Noun Extractor] 108 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 45.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 982 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1453, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 376 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 24.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 814 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1348, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 704 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1119, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 45.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1886 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2802, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 621 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 157\n",
      "[Noun Extractor] postprocessing ignore_features : 157 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 146\n",
      "[Noun Extractor] 146 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 34.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 766 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1234, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 213 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (0 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 811 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1214, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 258 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1408 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2210, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 467 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 97\n",
      "[Noun Extractor] postprocessing ignore_NJ : 97 -> 97\n",
      "[Noun Extractor] 97 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1005 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1583, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 42.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1876 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3025, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 576 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 211 -> 201\n",
      "[Noun Extractor] postprocessing ignore_features : 201 -> 192\n",
      "[Noun Extractor] postprocessing ignore_NJ : 192 -> 189\n",
      "[Noun Extractor] 189 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 50.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 988 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1614, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1217, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 224 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 41.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 709 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1212, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 34.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1367 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2196, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 110\n",
      "[Noun Extractor] 110 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 44.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1260 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2008, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 376 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 101 -> 101\n",
      "[Noun Extractor] 101 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 38.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 932 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1342, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 275 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1041 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1746, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 90\n",
      "[Noun Extractor] 90 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 51.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1219, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 954 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1447, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 299 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 71\n",
      "[Noun Extractor] 71 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 890 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1407, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 74\n",
      "[Noun Extractor] 74 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 46.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1068, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 42.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 535 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1029, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 138 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 46.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 525 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=867, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 154 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 41.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1256 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1881, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 419 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 131 -> 122\n",
      "[Noun Extractor] postprocessing ignore_features : 122 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 44.13 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 992 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1420, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 30.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 722 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1040, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 245 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 907 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1429, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 45.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1486 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2406, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 514 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 147\n",
      "[Noun Extractor] postprocessing ignore_NJ : 147 -> 144\n",
      "[Noun Extractor] 144 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 46.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1069 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1587, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 85\n",
      "[Noun Extractor] postprocessing ignore_features : 85 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1048 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1652, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 32.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 980 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1527, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1360 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1945, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 445 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 41.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1519 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2500, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 535 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 164 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 156\n",
      "[Noun Extractor] postprocessing ignore_NJ : 156 -> 156\n",
      "[Noun Extractor] 156 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 49.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 906 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1445, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 59\n",
      "[Noun Extractor] 59 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 726 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1064, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 229 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 37.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 968 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1359, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 30.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1455 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2095, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 449 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 32.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 789 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1109, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 273 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 748 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1188, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 208 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 41.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 698 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1084, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 38.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 718 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1084, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 34.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 761 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1176, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 271 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 71 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 44.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 840 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1796, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 288 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 43.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1799 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2907, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 558 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 154 -> 153\n",
      "[Noun Extractor] postprocessing ignore_features : 153 -> 148\n",
      "[Noun Extractor] postprocessing ignore_NJ : 148 -> 148\n",
      "[Noun Extractor] 148 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 49.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 766 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1168, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 812 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1160, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 8.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1165 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1958, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 361 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 35.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1021 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1597, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 356 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 84\n",
      "[Noun Extractor] 84 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 42.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 759 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1178, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 224 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 849 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1273, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 894 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1324, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 285 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 43.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3275 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5693, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1039 words\n",
      "[Noun Extractor] checked compounds. discovered 74 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 349 -> 328\n",
      "[Noun Extractor] postprocessing ignore_features : 328 -> 312\n",
      "[Noun Extractor] postprocessing ignore_NJ : 312 -> 311\n",
      "[Noun Extractor] 311 nouns (74 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 49.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1068, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 809 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1313, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 49.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 684 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1062, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 35.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2179 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3552, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 676 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 185\n",
      "[Noun Extractor] postprocessing ignore_features : 185 -> 179\n",
      "[Noun Extractor] postprocessing ignore_NJ : 179 -> 178\n",
      "[Noun Extractor] 178 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 45.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1369 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2215, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 455 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 105\n",
      "[Noun Extractor] 105 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 836 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1255, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 269 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 41.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 754 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1220, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6543 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12623, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2030 words\n",
      "[Noun Extractor] checked compounds. discovered 177 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 696 -> 662\n",
      "[Noun Extractor] postprocessing ignore_features : 662 -> 639\n",
      "[Noun Extractor] postprocessing ignore_NJ : 639 -> 639\n",
      "[Noun Extractor] 639 nouns (177 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 52.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 877 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1212, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 27.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1458 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2576, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 492 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 118\n",
      "[Noun Extractor] postprocessing ignore_NJ : 118 -> 118\n",
      "[Noun Extractor] 118 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 51.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 752 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1240, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 35.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3291 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5939, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1038 words\n",
      "[Noun Extractor] checked compounds. discovered 139 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 385 -> 350\n",
      "[Noun Extractor] postprocessing ignore_features : 350 -> 335\n",
      "[Noun Extractor] postprocessing ignore_NJ : 335 -> 335\n",
      "[Noun Extractor] 335 nouns (139 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 45.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=903, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 37.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 796 from 1 sents. mem=1.187 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1252, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 249 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 42.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1263 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1817, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 428 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109 -> 109\n",
      "[Noun Extractor] postprocessing ignore_features : 109 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 883 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1422, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 40.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1101, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2777 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5702, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 800 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 252 -> 244\n",
      "[Noun Extractor] postprocessing ignore_features : 244 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 229\n",
      "[Noun Extractor] 229 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 752 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1226, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 34.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 701 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1269, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 197 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 39.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 758 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1148, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 230 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 40.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 798 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1166, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 270 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 36.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 655 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1100, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 217 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 47.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=1.187 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=969, mem=1.187 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.187 Gb                    \n",
      "[Noun Extractor] 40.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1236 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1830, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 23.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 872 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1263, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 32.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 882 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1306, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 624 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=954, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 51\n",
      "[Noun Extractor] 51 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 799 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1286, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1550 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2538, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 554 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 115 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 105\n",
      "[Noun Extractor] 105 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1019, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 233 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 47.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1534 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2216, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 512 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 123 -> 123\n",
      "[Noun Extractor] postprocessing ignore_features : 123 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 36.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 603 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=895, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 148 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 27\n",
      "[Noun Extractor] postprocessing ignore_NJ : 27 -> 27\n",
      "[Noun Extractor] 27 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 692 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1055, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 213 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 774 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1160, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 227 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 527 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=822, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 127 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 32.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 869 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1326, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 316 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (4 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 45.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 549 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=841, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 38.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1179, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 250 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2341 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3384, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 669 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 191 -> 191\n",
      "[Noun Extractor] postprocessing ignore_features : 191 -> 182\n",
      "[Noun Extractor] postprocessing ignore_NJ : 182 -> 182\n",
      "[Noun Extractor] 182 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 909 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1413, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 286 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 49.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 869 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1219, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 291 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1187 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1750, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 380 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 36.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 635 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1039, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 204 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 750 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1075, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 254 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 37.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 712 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1078, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1129 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2163, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 338 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 738 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1258, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 38.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 988 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1456, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 337 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 41.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 814 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1200, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 74\n",
      "[Noun Extractor] 74 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 45.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 565 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=882, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 176 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 43.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1346, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 294 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1483 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2115, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 535 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 137 -> 137\n",
      "[Noun Extractor] postprocessing ignore_features : 137 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=998, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 213 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 882 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1239, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 38.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 849 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1328, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 43.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 776 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1188, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 602 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1039, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 47\n",
      "[Noun Extractor] 47 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 47.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1299 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1887, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 445 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 103\n",
      "[Noun Extractor] postprocessing ignore_NJ : 103 -> 103\n",
      "[Noun Extractor] 103 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 33.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1015 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1548, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 324 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 529 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=937, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 47.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1337 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2173, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 403 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1355, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 77\n",
      "[Noun Extractor] 77 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 282 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1172, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 22 -> 22\n",
      "[Noun Extractor] postprocessing ignore_features : 22 -> 21\n",
      "[Noun Extractor] postprocessing ignore_NJ : 21 -> 21\n",
      "[Noun Extractor] 21 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 43.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1375, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 285 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 33.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1593 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2308, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 497 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 129 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 123\n",
      "[Noun Extractor] postprocessing ignore_NJ : 123 -> 123\n",
      "[Noun Extractor] 123 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 779 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1150, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=916, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1653, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 794 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1239, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 243 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1134 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1591, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=945, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 164 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 37.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1067 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1693, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 350 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (15 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 658 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1009, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 37.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 650 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1121, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1060 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1599, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 37.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1112 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1581, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 382 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 74\n",
      "[Noun Extractor] 74 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 625 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=910, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1375, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 36.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2123 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3120, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 682 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 205 -> 202\n",
      "[Noun Extractor] postprocessing ignore_features : 202 -> 191\n",
      "[Noun Extractor] postprocessing ignore_NJ : 191 -> 191\n",
      "[Noun Extractor] 191 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 465 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=833, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 130 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 43.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1488 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2091, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 498 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4576 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8128, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1453 words\n",
      "[Noun Extractor] checked compounds. discovered 117 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 506 -> 492\n",
      "[Noun Extractor] postprocessing ignore_features : 492 -> 472\n",
      "[Noun Extractor] postprocessing ignore_NJ : 472 -> 466\n",
      "[Noun Extractor] 466 nouns (117 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 52.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 712 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1056, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 217 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 45.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1224 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1829, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (9 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1269 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1857, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 539 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 32.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 589 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=974, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 43\n",
      "[Noun Extractor] 43 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 46.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1151 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1667, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 402 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 94\n",
      "[Noun Extractor] postprocessing ignore_NJ : 94 -> 93\n",
      "[Noun Extractor] 93 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 995 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1409, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 331 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 64\n",
      "[Noun Extractor] 64 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 33.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3920 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6616, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1086 words\n",
      "[Noun Extractor] checked compounds. discovered 91 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 379 -> 368\n",
      "[Noun Extractor] postprocessing ignore_features : 368 -> 355\n",
      "[Noun Extractor] postprocessing ignore_NJ : 355 -> 353\n",
      "[Noun Extractor] 353 nouns (91 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 51.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 650 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=942, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 33.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 577 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=944, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 36.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 673 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1119, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 43.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 934 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1352, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 363 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1023 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1445, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 80\n",
      "[Noun Extractor] 80 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 41.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 557 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=936, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 849 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1327, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 278 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1159 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1821, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 377 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 114 -> 111\n",
      "[Noun Extractor] postprocessing ignore_features : 111 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 103\n",
      "[Noun Extractor] 103 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 41.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 868 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1142, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 317 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 31.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1173 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1613, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 377 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 86\n",
      "[Noun Extractor] 86 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 788 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1171, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 66\n",
      "[Noun Extractor] 66 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 44.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 935 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1347, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 89\n",
      "[Noun Extractor] 89 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 45.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 568 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 173 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 32.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 789 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1229, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 250 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 44.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 612 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=903, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 36.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 631 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=885, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 191 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 37\n",
      "[Noun Extractor] 37 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 539 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 163 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1288 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1828, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 473 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 92\n",
      "[Noun Extractor] postprocessing ignore_features : 92 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 899 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1229, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 271 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 30.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 737 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1026, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 34.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1488 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1989, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 500 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 112\n",
      "[Noun Extractor] 112 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 37.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1052, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 41.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1844 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3200, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 551 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 153 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 142\n",
      "[Noun Extractor] 142 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 48.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 693 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1064, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 204 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 42.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1080, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 55\n",
      "[Noun Extractor] 55 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 40.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 656 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=968, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 35.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 677 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1019, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 40 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 39.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 700 from 1 sents. mem=1.188 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1026, mem=1.188 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 53\n",
      "[Noun Extractor] 53 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.188 Gb                    \n",
      "[Noun Extractor] 38.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1293, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 291 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 70\n",
      "[Noun Extractor] 70 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1126 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1640, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 397 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 85\n",
      "[Noun Extractor] postprocessing ignore_features : 85 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 623 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=970, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 63 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (12 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 757 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1049, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 850 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1300, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 46.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 672 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1005, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 809 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1194, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 745 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1172, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 223 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 40\n",
      "[Noun Extractor] 40 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 667 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=916, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 730 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1065, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 692 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=958, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 258 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 62\n",
      "[Noun Extractor] 62 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 519 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=794, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 148 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2148 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3343, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 683 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 215\n",
      "[Noun Extractor] postprocessing ignore_features : 215 -> 207\n",
      "[Noun Extractor] postprocessing ignore_NJ : 207 -> 204\n",
      "[Noun Extractor] 204 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 48.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 748 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1169, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_features : 71 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1339 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2308, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 115 -> 115\n",
      "[Noun Extractor] postprocessing ignore_features : 115 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (5 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 33.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1084 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1533, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 305 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 32.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 457 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=767, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 134 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2363 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3801, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 666 words\n",
      "[Noun Extractor] checked compounds. discovered 62 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 245 -> 237\n",
      "[Noun Extractor] postprocessing ignore_features : 237 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 229\n",
      "[Noun Extractor] 229 nouns (62 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1252 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1915, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 479 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 119\n",
      "[Noun Extractor] postprocessing ignore_features : 119 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 111\n",
      "[Noun Extractor] 111 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1248 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1718, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 445 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 94\n",
      "[Noun Extractor] postprocessing ignore_NJ : 94 -> 92\n",
      "[Noun Extractor] 92 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 588 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=851, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 182 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 587 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=969, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 790 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1190, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 251 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 41.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1071 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1599, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 71\n",
      "[Noun Extractor] 71 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 594 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=845, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 785 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1123, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 60\n",
      "[Noun Extractor] postprocessing ignore_features : 60 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 662 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1043, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 275 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 49.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 607 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=886, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 189 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 13.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1386 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2092, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 410 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 112\n",
      "[Noun Extractor] 112 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1276, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 65\n",
      "[Noun Extractor] 65 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 43.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 509 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=808, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 169 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 41.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1367, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 782 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1158, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 43.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 707 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1043, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 47.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 717 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1070, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 33.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1017 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1747, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 309 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 49.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 644 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1270, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 191 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1241, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 700 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1004, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 549 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=831, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 166 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 695 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=983, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 45.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1171 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1715, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1204 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1977, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 383 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 105\n",
      "[Noun Extractor] postprocessing ignore_NJ : 105 -> 104\n",
      "[Noun Extractor] 104 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 949 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1418, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 271 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1009 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1519, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 399 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 516 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=735, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 160 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 474 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=762, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 45\n",
      "[Noun Extractor] 45 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 45.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1034 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1521, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 699 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1212, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 786 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1134, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 669 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1000, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 60\n",
      "[Noun Extractor] 60 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 43.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1267 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1793, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 532 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 29.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 688 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=967, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 32.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 631 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=991, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 194 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 43.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 681 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=976, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 249 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 771 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 29.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 423 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=683, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 108 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 23\n",
      "[Noun Extractor] postprocessing ignore_features : 23 -> 23\n",
      "[Noun Extractor] postprocessing ignore_NJ : 23 -> 23\n",
      "[Noun Extractor] 23 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 959 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1476, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 288 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 746 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1080, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 28.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 588 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 925 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1364, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 68\n",
      "[Noun Extractor] postprocessing ignore_NJ : 68 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 640 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=887, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 224 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 657 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 234 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 31.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 491 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=705, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 146 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=993, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 41\n",
      "[Noun Extractor] 41 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5046 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10752, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1435 words\n",
      "[Noun Extractor] checked compounds. discovered 270 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 661 -> 642\n",
      "[Noun Extractor] postprocessing ignore_features : 642 -> 625\n",
      "[Noun Extractor] postprocessing ignore_NJ : 625 -> 622\n",
      "[Noun Extractor] 622 nouns (270 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 58.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 867 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1196, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 70\n",
      "[Noun Extractor] 70 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 724 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1071, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 30.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 343 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=748, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 77 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 16 -> 16\n",
      "[Noun Extractor] postprocessing ignore_features : 16 -> 14\n",
      "[Noun Extractor] postprocessing ignore_NJ : 14 -> 14\n",
      "[Noun Extractor] 14 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 33.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 693 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=972, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 246 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 574 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=850, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 44\n",
      "[Noun Extractor] 44 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 43.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1326 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1950, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 480 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=741, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 132 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 438 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1100, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 121 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 32.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 523 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=799, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 953 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1276, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 347 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 76\n",
      "[Noun Extractor] 76 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 973 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1577, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 288 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 882 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1209, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 299 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 565 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=937, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 162 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 516 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=766, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 161 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 30\n",
      "[Noun Extractor] postprocessing ignore_NJ : 30 -> 30\n",
      "[Noun Extractor] 30 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 663 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=988, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 142 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 25\n",
      "[Noun Extractor] 25 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 28.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 558 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=785, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 29\n",
      "[Noun Extractor] postprocessing ignore_NJ : 29 -> 29\n",
      "[Noun Extractor] 29 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 717 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1112, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 56\n",
      "[Noun Extractor] 56 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=908, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 572 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=800, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1563 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2124, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 524 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=907, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 221 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 34.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 531 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=848, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 40.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1025, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 63\n",
      "[Noun Extractor] postprocessing ignore_features : 63 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 556 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=827, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 39\n",
      "[Noun Extractor] 39 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 919 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1281, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 33.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 618 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=886, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 15.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 751 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1079, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 33.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 699 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1207, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 73\n",
      "[Noun Extractor] 73 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 53.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1024 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1370, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 293 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 71\n",
      "[Noun Extractor] 71 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 675 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=940, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 219 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2031 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3341, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 681 words\n",
      "[Noun Extractor] checked compounds. discovered 43 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 218 -> 213\n",
      "[Noun Extractor] postprocessing ignore_features : 213 -> 197\n",
      "[Noun Extractor] postprocessing ignore_NJ : 197 -> 195\n",
      "[Noun Extractor] 195 nouns (43 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 47.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1126 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1651, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 363 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 559 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=817, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 147 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 31.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 487 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=846, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 509 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=741, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 168 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 39 -> 39\n",
      "[Noun Extractor] postprocessing ignore_features : 39 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1772 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2875, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 573 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 174 -> 171\n",
      "[Noun Extractor] postprocessing ignore_features : 171 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 50.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1055 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1552, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 573 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=842, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 770 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1122, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 264 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 632 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=935, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 679 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=954, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 45\n",
      "[Noun Extractor] 45 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 598 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=880, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 659 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=935, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 611 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=956, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 829 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1163, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 296 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (6 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 28.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 707 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=987, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 256 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 47.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 613 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=858, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 31\n",
      "[Noun Extractor] postprocessing ignore_features : 31 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 29.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1020, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 44.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1156, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 193 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 43\n",
      "[Noun Extractor] 43 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 45.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 736 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1050, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 213 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 649 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1057, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 40\n",
      "[Noun Extractor] 40 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 36.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 611 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=976, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 52\n",
      "[Noun Extractor] 52 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 45.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 468 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=735, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 133 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 25 -> 25\n",
      "[Noun Extractor] postprocessing ignore_features : 25 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 31.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 610 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 195 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 45\n",
      "[Noun Extractor] postprocessing ignore_NJ : 45 -> 44\n",
      "[Noun Extractor] 44 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 37.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 743 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1046, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 250 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 31.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=825, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 191 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 42.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1197 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1859, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 368 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 84 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (12 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 39.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 500 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=760, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 129 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 28 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 38.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 762 from 1 sents. mem=1.189 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1064, mem=1.189 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 56\n",
      "[Noun Extractor] postprocessing ignore_NJ : 56 -> 56\n",
      "[Noun Extractor] 56 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.189 Gb                    \n",
      "[Noun Extractor] 35.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 595 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=901, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 160 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 33\n",
      "[Noun Extractor] postprocessing ignore_NJ : 33 -> 33\n",
      "[Noun Extractor] 33 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 42.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 883 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1264, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 45.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 801 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1139, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 301 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 47.06 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 895 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1184, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 301 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 33.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 691 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=948, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 59\n",
      "[Noun Extractor] postprocessing ignore_features : 59 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 53\n",
      "[Noun Extractor] 53 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 40.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1356 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1988, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 118 -> 118\n",
      "[Noun Extractor] postprocessing ignore_features : 118 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 116\n",
      "[Noun Extractor] 116 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 42.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 718 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1026, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 262 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 71 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 44.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 703 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1035, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 247 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 51\n",
      "[Noun Extractor] 51 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 42.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 508 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=794, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 40.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1090 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1429, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 356 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 102\n",
      "[Noun Extractor] postprocessing ignore_features : 102 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (19 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 521 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=765, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 177 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 32.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 556 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=812, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 172 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 571 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=832, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 41\n",
      "[Noun Extractor] 41 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 43.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 546 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=818, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 162 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_features : 36 -> 36\n",
      "[Noun Extractor] postprocessing ignore_NJ : 36 -> 36\n",
      "[Noun Extractor] 36 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 42.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 863 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1135, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 29.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 603 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=819, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 27 -> 27\n",
      "[Noun Extractor] postprocessing ignore_features : 27 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 33.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 758 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1029, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 279 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 27.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 591 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=876, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 198 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 43.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 963 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1405, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 910 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1215, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 306 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 77\n",
      "[Noun Extractor] 77 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 886 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1254, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 313 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 458 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=805, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 107 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 28\n",
      "[Noun Extractor] postprocessing ignore_NJ : 28 -> 28\n",
      "[Noun Extractor] 28 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 32.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 781 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1078, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 276 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 634 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=964, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 40\n",
      "[Noun Extractor] postprocessing ignore_NJ : 40 -> 40\n",
      "[Noun Extractor] 40 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 37.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 607 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=885, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 46\n",
      "[Noun Extractor] 46 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 40.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 622 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=882, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 167 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1032 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1437, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 328 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 84\n",
      "[Noun Extractor] postprocessing ignore_NJ : 84 -> 84\n",
      "[Noun Extractor] 84 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 40.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 645 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=928, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 231 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 47\n",
      "[Noun Extractor] 47 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 36.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1256 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1751, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 418 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 108 -> 107\n",
      "[Noun Extractor] postprocessing ignore_features : 107 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 833 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1276, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 68\n",
      "[Noun Extractor] 68 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 46.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1700, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 407 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 89\n",
      "[Noun Extractor] postprocessing ignore_NJ : 89 -> 89\n",
      "[Noun Extractor] 89 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 36.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 895 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1369, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 277 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 558 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 179 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 45 -> 45\n",
      "[Noun Extractor] postprocessing ignore_features : 45 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 48.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 599 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 35 -> 35\n",
      "[Noun Extractor] postprocessing ignore_features : 35 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (1 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 31.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 550 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=802, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 158 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 33 -> 33\n",
      "[Noun Extractor] postprocessing ignore_features : 33 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 38.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 653 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1242, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 46.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 754 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1021, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 44.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 738 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=981, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 27.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 577 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=801, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 46.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 503 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=823, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 181 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 38.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1077 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1716, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 352 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 45.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 734 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=984, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 214 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 31.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 693 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=912, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 215 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_NJ : 54 -> 54\n",
      "[Noun Extractor] 54 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 35.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 760 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 242 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 674 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=911, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 261 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 30.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 960 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1320, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 284 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 86\n",
      "[Noun Extractor] postprocessing ignore_features : 86 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 78\n",
      "[Noun Extractor] 78 nouns (15 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1010, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 62\n",
      "[Noun Extractor] 62 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 35.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 212 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1089, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 64 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 30.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1199 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1952, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 85 -> 84\n",
      "[Noun Extractor] postprocessing ignore_features : 84 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1713 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2303, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 596 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 147 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 135\n",
      "[Noun Extractor] postprocessing ignore_NJ : 135 -> 134\n",
      "[Noun Extractor] 134 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 38.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1088 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1470, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 29.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 571 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=863, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 165 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_features : 32 -> 32\n",
      "[Noun Extractor] postprocessing ignore_NJ : 32 -> 32\n",
      "[Noun Extractor] 32 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 36.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 628 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=896, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 46 -> 46\n",
      "[Noun Extractor] postprocessing ignore_features : 46 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 41.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 565 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=847, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 207 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 744 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1057, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 186 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 33\n",
      "[Noun Extractor] 33 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 32.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 701 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=966, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 62 -> 62\n",
      "[Noun Extractor] postprocessing ignore_features : 62 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 59\n",
      "[Noun Extractor] 59 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 36.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 969 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1357, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 60 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 989 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1324, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 319 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] 79 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 700 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=919, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 225 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 53\n",
      "[Noun Extractor] postprocessing ignore_NJ : 53 -> 53\n",
      "[Noun Extractor] 53 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 34.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 486 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=726, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 150 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 48.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 822 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1019, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 259 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 50 -> 50\n",
      "[Noun Extractor] postprocessing ignore_features : 50 -> 47\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47 -> 47\n",
      "[Noun Extractor] 47 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 29.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 522 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1168, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 40\n",
      "[Noun Extractor] postprocessing ignore_features : 40 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 41.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 588 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=821, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 183 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 43 -> 43\n",
      "[Noun Extractor] postprocessing ignore_features : 43 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 38.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1052 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1370, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 83\n",
      "[Noun Extractor] postprocessing ignore_NJ : 83 -> 83\n",
      "[Noun Extractor] 83 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 33.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 776 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1106, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 59\n",
      "[Noun Extractor] postprocessing ignore_NJ : 59 -> 58\n",
      "[Noun Extractor] 58 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 39.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 754 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1011, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 209 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 38.48 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 659 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=846, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 203 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_NJ : 42 -> 42\n",
      "[Noun Extractor] 42 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 33.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1963 from 1 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2632, mem=1.190 Gb\n",
      "[Noun Extractor] batch prediction was completed for 603 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 142\n",
      "[Noun Extractor] postprocessing ignore_NJ : 142 -> 142\n",
      "[Noun Extractor] 142 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.190 Gb                    \n",
      "[Noun Extractor] 29.48 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "import Extractor\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Ent_1.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head ORDER BY wdate DESC',conn)\n",
    "conn.close()\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "ext = Extractor.Ext(df)\n",
    "df = ext.cleaning()\n",
    "\n",
    "new_words = ext.search_dict(sorted(ext.extract_nouns().items(),key=lambda _:_[1], reverse=True))\n",
    "sent = ext.extract_sent(new_words)\n",
    "\n",
    "# 변수 생성\n",
    "statistic = ext.extract_statistic_value(sent)\n",
    "r_rat = ext.extract_r_rat(sent,statistic)\n",
    "statistic = ext.combine_var(statistic, r_rat)\n",
    "conn = sqlite3.connect('Ent_1_var.db')\n",
    "statistic.to_sql('var', conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
