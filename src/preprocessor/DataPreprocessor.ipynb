{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 153629 from 127907 sents. mem=0.495 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=556166, mem=0.735 Gb\n",
      "[Noun Extractor] batch prediction was completed for 40185 words\n",
      "[Noun Extractor] checked compounds. discovered 27173 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 426 -> 423\n",
      "[Noun Extractor] postprocessing ignore_features : 423 -> 397\n",
      "[Noun Extractor] postprocessing ignore_NJ : 397 -> 395\n",
      "[Noun Extractor] 395 nouns (27173 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=0.805 Gb                    \n",
      "[Noun Extractor] 31.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 156261 from 127907 sents. mem=0.865 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=547864, mem=0.945 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41507 words\n",
      "[Noun Extractor] checked compounds. discovered 28598 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 429 -> 426\n",
      "[Noun Extractor] postprocessing ignore_features : 426 -> 405\n",
      "[Noun Extractor] postprocessing ignore_NJ : 405 -> 404\n",
      "[Noun Extractor] 404 nouns (28598 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=0.944 Gb                    \n",
      "[Noun Extractor] 32.52 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 152909 from 127906 sents. mem=0.969 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=522178, mem=0.980 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41573 words\n",
      "[Noun Extractor] checked compounds. discovered 26220 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 406 -> 401\n",
      "[Noun Extractor] postprocessing ignore_features : 401 -> 376\n",
      "[Noun Extractor] postprocessing ignore_NJ : 376 -> 375\n",
      "[Noun Extractor] 375 nouns (26220 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=0.978 Gb                    \n",
      "[Noun Extractor] 31.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157855 from 127907 sents. mem=0.978 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=521544, mem=1.018 Gb\n",
      "[Noun Extractor] batch prediction was completed for 43681 words\n",
      "[Noun Extractor] checked compounds. discovered 28034 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 381 -> 378\n",
      "[Noun Extractor] postprocessing ignore_features : 378 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (28034 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.014 Gb                    \n",
      "[Noun Extractor] 30.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166295 from 127906 sents. mem=1.014 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=529708, mem=1.064 Gb\n",
      "[Noun Extractor] batch prediction was completed for 47892 words\n",
      "[Noun Extractor] checked compounds. discovered 31704 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 394 -> 394\n",
      "[Noun Extractor] postprocessing ignore_features : 394 -> 366\n",
      "[Noun Extractor] postprocessing ignore_NJ : 366 -> 364\n",
      "[Noun Extractor] 364 nouns (31704 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.042 Gb                    \n",
      "[Noun Extractor] 31.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 162683 from 127907 sents. mem=1.046 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=513031, mem=1.059 Gb\n",
      "[Noun Extractor] batch prediction was completed for 45035 words\n",
      "[Noun Extractor] checked compounds. discovered 29556 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 359 -> 358\n",
      "[Noun Extractor] postprocessing ignore_features : 358 -> 333\n",
      "[Noun Extractor] postprocessing ignore_NJ : 333 -> 331\n",
      "[Noun Extractor] 331 nouns (29556 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.041 Gb                    \n",
      "[Noun Extractor] 28.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 157585 from 127907 sents. mem=1.041 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=538505, mem=1.045 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39854 words\n",
      "[Noun Extractor] checked compounds. discovered 23050 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 405 -> 405\n",
      "[Noun Extractor] postprocessing ignore_features : 405 -> 381\n",
      "[Noun Extractor] postprocessing ignore_NJ : 381 -> 379\n",
      "[Noun Extractor] 379 nouns (23050 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.036 Gb                    \n",
      "[Noun Extractor] 29.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 148501 from 127906 sents. mem=1.036 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=514069, mem=1.037 Gb\n",
      "[Noun Extractor] batch prediction was completed for 39879 words\n",
      "[Noun Extractor] checked compounds. discovered 24916 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 408 -> 407\n",
      "[Noun Extractor] postprocessing ignore_features : 407 -> 385\n",
      "[Noun Extractor] postprocessing ignore_NJ : 385 -> 383\n",
      "[Noun Extractor] 383 nouns (24916 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.027 Gb                    \n",
      "[Noun Extractor] 35.77 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 159979 from 127907 sents. mem=1.027 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=510274, mem=1.069 Gb\n",
      "[Noun Extractor] batch prediction was completed for 44551 words\n",
      "[Noun Extractor] checked compounds. discovered 28238 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 381 -> 380\n",
      "[Noun Extractor] postprocessing ignore_features : 380 -> 354\n",
      "[Noun Extractor] postprocessing ignore_NJ : 354 -> 351\n",
      "[Noun Extractor] 351 nouns (28238 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.065 Gb                    \n",
      "[Noun Extractor] 28.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 166948 from 127906 sents. mem=1.065 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=590217, mem=1.080 Gb\n",
      "[Noun Extractor] batch prediction was completed for 42936 words\n",
      "[Noun Extractor] checked compounds. discovered 25619 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 473 -> 471\n",
      "[Noun Extractor] postprocessing ignore_features : 471 -> 449\n",
      "[Noun Extractor] postprocessing ignore_NJ : 449 -> 446\n",
      "[Noun Extractor] 446 nouns (25619 compounds) with min frequency=127\n",
      "[Noun Extractor] flushing was done. mem=1.072 Gb                    \n",
      "[Noun Extractor] 34.37 % eojeols are covered\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 31\n",
      "all branching entropies was computed # words = 3301\n",
      "all accessor variety was computed # words = 3301\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 89\n",
      "all branching entropies was computed # words = 14382\n",
      "all accessor variety was computed # words = 14382\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 95\n",
      "all branching entropies was computed # words = 15330\n",
      "all accessor variety was computed # words = 15330\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 102\n",
      "all branching entropies was computed # words = 16336\n",
      "all accessor variety was computed # words = 16336\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 24359\n",
      "all accessor variety was computed # words = 24359\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 157\n",
      "all branching entropies was computed # words = 25297\n",
      "all accessor variety was computed # words = 25297\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 162\n",
      "all branching entropies was computed # words = 25586\n",
      "all accessor variety was computed # words = 25586\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 167\n",
      "all branching entropies was computed # words = 25911\n",
      "all accessor variety was computed # words = 25911\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 170\n",
      "all branching entropies was computed # words = 26080\n",
      "all accessor variety was computed # words = 26080\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 176\n",
      "all branching entropies was computed # words = 27439\n",
      "all accessor variety was computed # words = 27439\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 257\n",
      "all branching entropies was computed # words = 30453\n",
      "all accessor variety was computed # words = 30453\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 258\n",
      "all branching entropies was computed # words = 30755\n",
      "all accessor variety was computed # words = 30755\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 262\n",
      "all branching entropies was computed # words = 31027\n",
      "all accessor variety was computed # words = 31027\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 278\n",
      "all branching entropies was computed # words = 33506\n",
      "all accessor variety was computed # words = 33506\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 320\n",
      "all branching entropies was computed # words = 35698\n",
      "all accessor variety was computed # words = 35698\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 339\n",
      "all branching entropies was computed # words = 36598\n",
      "all accessor variety was computed # words = 36598\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 346\n",
      "all branching entropies was computed # words = 36749\n",
      "all accessor variety was computed # words = 36749\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 351\n",
      "all branching entropies was computed # words = 36844\n",
      "all accessor variety was computed # words = 36844\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 354\n",
      "all branching entropies was computed # words = 37056\n",
      "all accessor variety was computed # words = 37056\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 355\n",
      "all branching entropies was computed # words = 37401\n",
      "all accessor variety was computed # words = 37401\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 364\n",
      "all branching entropies was computed # words = 37835\n",
      "all accessor variety was computed # words = 37835\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 367\n",
      "all branching entropies was computed # words = 37975\n",
      "all accessor variety was computed # words = 37975\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 378\n",
      "all branching entropies was computed # words = 38321\n",
      "all accessor variety was computed # words = 38321\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 381\n",
      "all branching entropies was computed # words = 38412\n",
      "all accessor variety was computed # words = 38412\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 384\n",
      "all branching entropies was computed # words = 38733\n",
      "all accessor variety was computed # words = 38733\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 386\n",
      "all branching entropies was computed # words = 39736\n",
      "all accessor variety was computed # words = 39736\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 398\n",
      "all branching entropies was computed # words = 40363\n",
      "all accessor variety was computed # words = 40363\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 403\n",
      "all branching entropies was computed # words = 40424\n",
      "all accessor variety was computed # words = 40424\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 406\n",
      "all branching entropies was computed # words = 40468\n",
      "all accessor variety was computed # words = 40468\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 410\n",
      "all branching entropies was computed # words = 40762\n",
      "all accessor variety was computed # words = 40762\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 413\n",
      "all branching entropies was computed # words = 40939\n",
      "all accessor variety was computed # words = 40939\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 423\n",
      "all branching entropies was computed # words = 41469\n",
      "all accessor variety was computed # words = 41469\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 423\n",
      "all branching entropies was computed # words = 41522\n",
      "all accessor variety was computed # words = 41522\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 427\n",
      "all branching entropies was computed # words = 41700\n",
      "all accessor variety was computed # words = 41700\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 434\n",
      "all branching entropies was computed # words = 42020\n",
      "all accessor variety was computed # words = 42020\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 437\n",
      "all branching entropies was computed # words = 42083\n",
      "all accessor variety was computed # words = 42083\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 444\n",
      "all branching entropies was computed # words = 42663\n",
      "all accessor variety was computed # words = 42663\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 449\n",
      "all branching entropies was computed # words = 42801\n",
      "all accessor variety was computed # words = 42801\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 452\n",
      "all branching entropies was computed # words = 43025\n",
      "all accessor variety was computed # words = 43025\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 457\n",
      "all branching entropies was computed # words = 43113\n",
      "all accessor variety was computed # words = 43113\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 464\n",
      "all branching entropies was computed # words = 44474\n",
      "all accessor variety was computed # words = 44474\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 467\n",
      "all branching entropies was computed # words = 44542\n",
      "all accessor variety was computed # words = 44542\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 470\n",
      "all branching entropies was computed # words = 44555\n",
      "all accessor variety was computed # words = 44555\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 473\n",
      "all branching entropies was computed # words = 44730\n",
      "all accessor variety was computed # words = 44730\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 44818\n",
      "all accessor variety was computed # words = 44818\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 476\n",
      "all branching entropies was computed # words = 45103\n",
      "all accessor variety was computed # words = 45103\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 477\n",
      "all branching entropies was computed # words = 45294\n",
      "all accessor variety was computed # words = 45294\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 481\n",
      "all branching entropies was computed # words = 45463\n",
      "all accessor variety was computed # words = 45463\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 484\n",
      "all branching entropies was computed # words = 45521\n",
      "all accessor variety was computed # words = 45521\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 488\n",
      "all branching entropies was computed # words = 45695\n",
      "all accessor variety was computed # words = 45695\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 491\n",
      "all branching entropies was computed # words = 45739\n",
      "all accessor variety was computed # words = 45739\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 494\n",
      "all branching entropies was computed # words = 45786\n",
      "all accessor variety was computed # words = 45786\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 498\n",
      "all branching entropies was computed # words = 45875\n",
      "all accessor variety was computed # words = 45875\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 502\n",
      "all branching entropies was computed # words = 45924\n",
      "all accessor variety was computed # words = 45924\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 508\n",
      "all branching entropies was computed # words = 46305\n",
      "all accessor variety was computed # words = 46305\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 513\n",
      "all branching entropies was computed # words = 46467\n",
      "all accessor variety was computed # words = 46467\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 520\n",
      "all branching entropies was computed # words = 46900\n",
      "all accessor variety was computed # words = 46900\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 523\n",
      "all branching entropies was computed # words = 47004\n",
      "all accessor variety was computed # words = 47004\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 526\n",
      "all branching entropies was computed # words = 47107\n",
      "all accessor variety was computed # words = 47107\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 531\n",
      "all branching entropies was computed # words = 47137\n",
      "all accessor variety was computed # words = 47137\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 534\n",
      "all branching entropies was computed # words = 47306\n",
      "all accessor variety was computed # words = 47306\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 537\n",
      "all branching entropies was computed # words = 47442\n",
      "all accessor variety was computed # words = 47442\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 541\n",
      "all branching entropies was computed # words = 47592\n",
      "all accessor variety was computed # words = 47592\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 556\n",
      "all branching entropies was computed # words = 47984\n",
      "all accessor variety was computed # words = 47984\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 559\n",
      "all branching entropies was computed # words = 48022\n",
      "all accessor variety was computed # words = 48022\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 573\n",
      "all branching entropies was computed # words = 48602\n",
      "all accessor variety was computed # words = 48602\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 576\n",
      "all branching entropies was computed # words = 48798\n",
      "all accessor variety was computed # words = 48798\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 580\n",
      "all branching entropies was computed # words = 48939\n",
      "all accessor variety was computed # words = 48939\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 584\n",
      "all branching entropies was computed # words = 49306\n",
      "all accessor variety was computed # words = 49306\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 589\n",
      "all branching entropies was computed # words = 49660\n",
      "all accessor variety was computed # words = 49660\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 596\n",
      "all branching entropies was computed # words = 49837\n",
      "all accessor variety was computed # words = 49837\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 599\n",
      "all branching entropies was computed # words = 49991\n",
      "all accessor variety was computed # words = 49991\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 604\n",
      "all branching entropies was computed # words = 50045\n",
      "all accessor variety was computed # words = 50045\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 606\n",
      "all branching entropies was computed # words = 50276\n",
      "all accessor variety was computed # words = 50276\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 611\n",
      "all branching entropies was computed # words = 50304\n",
      "all accessor variety was computed # words = 50304\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 614\n",
      "all branching entropies was computed # words = 50331\n",
      "all accessor variety was computed # words = 50331\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 619\n",
      "all branching entropies was computed # words = 50401\n",
      "all accessor variety was computed # words = 50401\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 622\n",
      "all branching entropies was computed # words = 50816\n",
      "all accessor variety was computed # words = 50816\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 631\n",
      "all branching entropies was computed # words = 50877\n",
      "all accessor variety was computed # words = 50877\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 636\n",
      "all branching entropies was computed # words = 50909\n",
      "all accessor variety was computed # words = 50909\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 639\n",
      "all branching entropies was computed # words = 50942\n",
      "all accessor variety was computed # words = 50942\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 642\n",
      "all branching entropies was computed # words = 50969\n",
      "all accessor variety was computed # words = 50969\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 643\n",
      "all branching entropies was computed # words = 51049\n",
      "all accessor variety was computed # words = 51049\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 648\n",
      "all branching entropies was computed # words = 51973\n",
      "all accessor variety was computed # words = 51973\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 52063\n",
      "all accessor variety was computed # words = 52063\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 654\n",
      "all branching entropies was computed # words = 52113\n",
      "all accessor variety was computed # words = 52113\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 659\n",
      "all branching entropies was computed # words = 52491\n",
      "all accessor variety was computed # words = 52491\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 667\n",
      "all branching entropies was computed # words = 52982\n",
      "all accessor variety was computed # words = 52982\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 670\n",
      "all branching entropies was computed # words = 53066\n",
      "all accessor variety was computed # words = 53066\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 672\n",
      "all branching entropies was computed # words = 53148\n",
      "all accessor variety was computed # words = 53148\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 675\n",
      "all branching entropies was computed # words = 53188\n",
      "all accessor variety was computed # words = 53188\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 678\n",
      "all branching entropies was computed # words = 53296\n",
      "all accessor variety was computed # words = 53296\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 681\n",
      "all branching entropies was computed # words = 53410\n",
      "all accessor variety was computed # words = 53410\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 681\n",
      "all branching entropies was computed # words = 53525\n",
      "all accessor variety was computed # words = 53525\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 684\n",
      "all branching entropies was computed # words = 53555\n",
      "all accessor variety was computed # words = 53555\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 687\n",
      "all branching entropies was computed # words = 53631\n",
      "all accessor variety was computed # words = 53631\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 692\n",
      "all branching entropies was computed # words = 53966\n",
      "all accessor variety was computed # words = 53966\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 695\n",
      "all branching entropies was computed # words = 54165\n",
      "all accessor variety was computed # words = 54165\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 700\n",
      "all branching entropies was computed # words = 54192\n",
      "all accessor variety was computed # words = 54192\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 703\n",
      "all branching entropies was computed # words = 54237\n",
      "all accessor variety was computed # words = 54237\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 707\n",
      "all branching entropies was computed # words = 54283\n",
      "all accessor variety was computed # words = 54283\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 708\n",
      "all branching entropies was computed # words = 54335\n",
      "all accessor variety was computed # words = 54335\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 713\n",
      "all branching entropies was computed # words = 54398\n",
      "all accessor variety was computed # words = 54398\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 717\n",
      "all branching entropies was computed # words = 54511\n",
      "all accessor variety was computed # words = 54511\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 718\n",
      "all branching entropies was computed # words = 54657\n",
      "all accessor variety was computed # words = 54657\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 724\n",
      "all branching entropies was computed # words = 54781\n",
      "all accessor variety was computed # words = 54781\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 731\n",
      "all branching entropies was computed # words = 55050\n",
      "all accessor variety was computed # words = 55050\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 734\n",
      "all branching entropies was computed # words = 55090\n",
      "all accessor variety was computed # words = 55090\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 739\n",
      "all branching entropies was computed # words = 55215\n",
      "all accessor variety was computed # words = 55215\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 745\n",
      "all branching entropies was computed # words = 55466\n",
      "all accessor variety was computed # words = 55466\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 748\n",
      "all branching entropies was computed # words = 55520\n",
      "all accessor variety was computed # words = 55520\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 752\n",
      "all branching entropies was computed # words = 55581\n",
      "all accessor variety was computed # words = 55581\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 755\n",
      "all branching entropies was computed # words = 55608\n",
      "all accessor variety was computed # words = 55608\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 771\n",
      "all branching entropies was computed # words = 56603\n",
      "all accessor variety was computed # words = 56603\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 779\n",
      "all branching entropies was computed # words = 56625\n",
      "all accessor variety was computed # words = 56625\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 786\n",
      "all branching entropies was computed # words = 56742\n",
      "all accessor variety was computed # words = 56742\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 789\n",
      "all branching entropies was computed # words = 56780\n",
      "all accessor variety was computed # words = 56780\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 792\n",
      "all branching entropies was computed # words = 56798\n",
      "all accessor variety was computed # words = 56798\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 793\n",
      "all branching entropies was computed # words = 56872\n",
      "all accessor variety was computed # words = 56872\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 796\n",
      "all branching entropies was computed # words = 56929\n",
      "all accessor variety was computed # words = 56929\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 799\n",
      "all branching entropies was computed # words = 56958\n",
      "all accessor variety was computed # words = 56958\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 804\n",
      "all branching entropies was computed # words = 56995\n",
      "all accessor variety was computed # words = 56995\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 809\n",
      "all branching entropies was computed # words = 57033\n",
      "all accessor variety was computed # words = 57033\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 816\n",
      "all branching entropies was computed # words = 57178\n",
      "all accessor variety was computed # words = 57178\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 57198\n",
      "all accessor variety was computed # words = 57198\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 824\n",
      "all branching entropies was computed # words = 57220\n",
      "all accessor variety was computed # words = 57220\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 828\n",
      "all branching entropies was computed # words = 57281\n",
      "all accessor variety was computed # words = 57281\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 833\n",
      "all branching entropies was computed # words = 57404\n",
      "all accessor variety was computed # words = 57404\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 833\n",
      "all branching entropies was computed # words = 57419\n",
      "all accessor variety was computed # words = 57419\n",
      "training was done. used memory 1.072 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 836\n",
      "all branching entropies was computed # words = 57451\n",
      "all accessor variety was computed # words = 57451\n",
      "training was done. used memory 1.077 Gb1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 841\n",
      "all branching entropies was computed # words = 57506\n",
      "all accessor variety was computed # words = 57506\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 842\n",
      "all branching entropies was computed # words = 57583\n",
      "all accessor variety was computed # words = 57583\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 844\n",
      "all branching entropies was computed # words = 57607\n",
      "all accessor variety was computed # words = 57607\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 848\n",
      "all branching entropies was computed # words = 57621\n",
      "all accessor variety was computed # words = 57621\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 853\n",
      "all branching entropies was computed # words = 57657\n",
      "all accessor variety was computed # words = 57657\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 854\n",
      "all branching entropies was computed # words = 57694\n",
      "all accessor variety was computed # words = 57694\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 859\n",
      "all branching entropies was computed # words = 57771\n",
      "all accessor variety was computed # words = 57771\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 862\n",
      "all branching entropies was computed # words = 57829\n",
      "all accessor variety was computed # words = 57829\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 865\n",
      "all branching entropies was computed # words = 57864\n",
      "all accessor variety was computed # words = 57864\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 869\n",
      "all branching entropies was computed # words = 57890\n",
      "all accessor variety was computed # words = 57890\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 872\n",
      "all branching entropies was computed # words = 58029\n",
      "all accessor variety was computed # words = 58029\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 876\n",
      "all branching entropies was computed # words = 58046\n",
      "all accessor variety was computed # words = 58046\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 881\n",
      "all branching entropies was computed # words = 58056\n",
      "all accessor variety was computed # words = 58056\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 887\n",
      "all branching entropies was computed # words = 58389\n",
      "all accessor variety was computed # words = 58389\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 890\n",
      "all branching entropies was computed # words = 58395\n",
      "all accessor variety was computed # words = 58395\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 893\n",
      "all branching entropies was computed # words = 58409\n",
      "all accessor variety was computed # words = 58409\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 896\n",
      "all branching entropies was computed # words = 58475\n",
      "all accessor variety was computed # words = 58475\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 899\n",
      "all branching entropies was computed # words = 58511\n",
      "all accessor variety was computed # words = 58511\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 902\n",
      "all branching entropies was computed # words = 58587\n",
      "all accessor variety was computed # words = 58587\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 905\n",
      "all branching entropies was computed # words = 58615\n",
      "all accessor variety was computed # words = 58615\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 908\n",
      "all branching entropies was computed # words = 58675\n",
      "all accessor variety was computed # words = 58675\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 913\n",
      "all branching entropies was computed # words = 58695\n",
      "all accessor variety was computed # words = 58695\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 914\n",
      "all branching entropies was computed # words = 58737\n",
      "all accessor variety was computed # words = 58737\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 914\n",
      "all branching entropies was computed # words = 59034\n",
      "all accessor variety was computed # words = 59034\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 915\n",
      "all branching entropies was computed # words = 59100\n",
      "all accessor variety was computed # words = 59100\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 918\n",
      "all branching entropies was computed # words = 59169\n",
      "all accessor variety was computed # words = 59169\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 922\n",
      "all branching entropies was computed # words = 59249\n",
      "all accessor variety was computed # words = 59249\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 927\n",
      "all branching entropies was computed # words = 59377\n",
      "all accessor variety was computed # words = 59377\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 929\n",
      "all branching entropies was computed # words = 59555\n",
      "all accessor variety was computed # words = 59555\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 932\n",
      "all branching entropies was computed # words = 59612\n",
      "all accessor variety was computed # words = 59612\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 936\n",
      "all branching entropies was computed # words = 59701\n",
      "all accessor variety was computed # words = 59701\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 939\n",
      "all branching entropies was computed # words = 59742\n",
      "all accessor variety was computed # words = 59742\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 940\n",
      "all branching entropies was computed # words = 59766\n",
      "all accessor variety was computed # words = 59766\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 943\n",
      "all branching entropies was computed # words = 59812\n",
      "all accessor variety was computed # words = 59812\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all branching entropies was computed # words = 59849\n",
      "all accessor variety was computed # words = 59849\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 950\n",
      "all branching entropies was computed # words = 59871\n",
      "all accessor variety was computed # words = 59871\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 958\n",
      "all branching entropies was computed # words = 59890\n",
      "all accessor variety was computed # words = 59890\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 963\n",
      "all branching entropies was computed # words = 59970\n",
      "all accessor variety was computed # words = 59970\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 963\n",
      "all branching entropies was computed # words = 59983\n",
      "all accessor variety was computed # words = 59983\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 966\n",
      "all branching entropies was computed # words = 60026\n",
      "all accessor variety was computed # words = 60026\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 971\n",
      "all branching entropies was computed # words = 60110\n",
      "all accessor variety was computed # words = 60110\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 974\n",
      "all branching entropies was computed # words = 60149\n",
      "all accessor variety was computed # words = 60149\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 977\n",
      "all branching entropies was computed # words = 60162\n",
      "all accessor variety was computed # words = 60162\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 978\n",
      "all branching entropies was computed # words = 60236\n",
      "all accessor variety was computed # words = 60236\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 985\n",
      "all branching entropies was computed # words = 60332\n",
      "all accessor variety was computed # words = 60332\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 986\n",
      "all branching entropies was computed # words = 60347\n",
      "all accessor variety was computed # words = 60347\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 989\n",
      "all branching entropies was computed # words = 60392\n",
      "all accessor variety was computed # words = 60392\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 992\n",
      "all branching entropies was computed # words = 60414\n",
      "all accessor variety was computed # words = 60414\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 993\n",
      "all branching entropies was computed # words = 60631\n",
      "all accessor variety was computed # words = 60631\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 997\n",
      "all branching entropies was computed # words = 60750\n",
      "all accessor variety was computed # words = 60750\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1002\n",
      "all branching entropies was computed # words = 60772\n",
      "all accessor variety was computed # words = 60772\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1004\n",
      "all branching entropies was computed # words = 60904\n",
      "all accessor variety was computed # words = 60904\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1008\n",
      "all branching entropies was computed # words = 60932\n",
      "all accessor variety was computed # words = 60932\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1009\n",
      "all branching entropies was computed # words = 60976\n",
      "all accessor variety was computed # words = 60976\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1013\n",
      "all branching entropies was computed # words = 61193\n",
      "all accessor variety was computed # words = 61193\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1016\n",
      "all branching entropies was computed # words = 61264\n",
      "all accessor variety was computed # words = 61264\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1022\n",
      "all branching entropies was computed # words = 61332\n",
      "all accessor variety was computed # words = 61332\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1025\n",
      "all branching entropies was computed # words = 61348\n",
      "all accessor variety was computed # words = 61348\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1028\n",
      "all branching entropies was computed # words = 61368\n",
      "all accessor variety was computed # words = 61368\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1035\n",
      "all branching entropies was computed # words = 61424\n",
      "all accessor variety was computed # words = 61424\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1038\n",
      "all branching entropies was computed # words = 61488\n",
      "all accessor variety was computed # words = 61488\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1042\n",
      "all branching entropies was computed # words = 61537\n",
      "all accessor variety was computed # words = 61537\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1045\n",
      "all branching entropies was computed # words = 61560\n",
      "all accessor variety was computed # words = 61560\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1049\n",
      "all branching entropies was computed # words = 61575\n",
      "all accessor variety was computed # words = 61575\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1052\n",
      "all branching entropies was computed # words = 61640\n",
      "all accessor variety was computed # words = 61640\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1056\n",
      "all branching entropies was computed # words = 61813\n",
      "all accessor variety was computed # words = 61813\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1062\n",
      "all branching entropies was computed # words = 61874\n",
      "all accessor variety was computed # words = 61874\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1065\n",
      "all branching entropies was computed # words = 61907\n",
      "all accessor variety was computed # words = 61907\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1068\n",
      "all branching entropies was computed # words = 61927\n",
      "all accessor variety was computed # words = 61927\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1068\n",
      "all branching entropies was computed # words = 61964\n",
      "all accessor variety was computed # words = 61964\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1075\n",
      "all branching entropies was computed # words = 62036\n",
      "all accessor variety was computed # words = 62036\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1078\n",
      "all branching entropies was computed # words = 62129\n",
      "all accessor variety was computed # words = 62129\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1079\n",
      "all branching entropies was computed # words = 62243\n",
      "all accessor variety was computed # words = 62243\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1084\n",
      "all branching entropies was computed # words = 62286\n",
      "all accessor variety was computed # words = 62286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1093\n",
      "all branching entropies was computed # words = 62544\n",
      "all accessor variety was computed # words = 62544\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1095\n",
      "all branching entropies was computed # words = 62578\n",
      "all accessor variety was computed # words = 62578\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1097\n",
      "all branching entropies was computed # words = 62597\n",
      "all accessor variety was computed # words = 62597\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1105\n",
      "all branching entropies was computed # words = 62865\n",
      "all accessor variety was computed # words = 62865\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1108\n",
      "all branching entropies was computed # words = 62933\n",
      "all accessor variety was computed # words = 62933\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1111\n",
      "all branching entropies was computed # words = 62965\n",
      "all accessor variety was computed # words = 62965\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1114\n",
      "all branching entropies was computed # words = 62985\n",
      "all accessor variety was computed # words = 62985\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1117\n",
      "all branching entropies was computed # words = 63016\n",
      "all accessor variety was computed # words = 63016\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1120\n",
      "all branching entropies was computed # words = 63034\n",
      "all accessor variety was computed # words = 63034\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1123\n",
      "all branching entropies was computed # words = 63063\n",
      "all accessor variety was computed # words = 63063\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1125\n",
      "all branching entropies was computed # words = 63728\n",
      "all accessor variety was computed # words = 63728\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1125\n",
      "all branching entropies was computed # words = 63737\n",
      "all accessor variety was computed # words = 63737\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1130\n",
      "all branching entropies was computed # words = 63877\n",
      "all accessor variety was computed # words = 63877\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1133\n",
      "all branching entropies was computed # words = 63903\n",
      "all accessor variety was computed # words = 63903\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1136\n",
      "all branching entropies was computed # words = 63946\n",
      "all accessor variety was computed # words = 63946\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1155\n",
      "all branching entropies was computed # words = 64672\n",
      "all accessor variety was computed # words = 64672\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1158\n",
      "all branching entropies was computed # words = 64686\n",
      "all accessor variety was computed # words = 64686\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1161\n",
      "all branching entropies was computed # words = 64754\n",
      "all accessor variety was computed # words = 64754\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1172\n",
      "all branching entropies was computed # words = 65102\n",
      "all accessor variety was computed # words = 65102\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1177\n",
      "all branching entropies was computed # words = 65167\n",
      "all accessor variety was computed # words = 65167\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1178\n",
      "all branching entropies was computed # words = 65197\n",
      "all accessor variety was computed # words = 65197\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1183\n",
      "all branching entropies was computed # words = 65323\n",
      "all accessor variety was computed # words = 65323\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1185\n",
      "all branching entropies was computed # words = 65341\n",
      "all accessor variety was computed # words = 65341\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1187\n",
      "all branching entropies was computed # words = 65348\n",
      "all accessor variety was computed # words = 65348\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1195\n",
      "all branching entropies was computed # words = 65436\n",
      "all accessor variety was computed # words = 65436\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1197\n",
      "all branching entropies was computed # words = 65448\n",
      "all accessor variety was computed # words = 65448\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1200\n",
      "all branching entropies was computed # words = 65474\n",
      "all accessor variety was computed # words = 65474\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1200\n",
      "all branching entropies was computed # words = 65474\n",
      "all accessor variety was computed # words = 65474\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1204\n",
      "all branching entropies was computed # words = 65537\n",
      "all accessor variety was computed # words = 65537\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1204\n",
      "all branching entropies was computed # words = 65540\n",
      "all accessor variety was computed # words = 65540\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1207\n",
      "all branching entropies was computed # words = 65559\n",
      "all accessor variety was computed # words = 65559\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1212\n",
      "all branching entropies was computed # words = 65602\n",
      "all accessor variety was computed # words = 65602\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1217\n",
      "all branching entropies was computed # words = 65757\n",
      "all accessor variety was computed # words = 65757\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1222\n",
      "all branching entropies was computed # words = 65806\n",
      "all accessor variety was computed # words = 65806\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1227\n",
      "all branching entropies was computed # words = 65865\n",
      "all accessor variety was computed # words = 65865\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1230\n",
      "all branching entropies was computed # words = 65900\n",
      "all accessor variety was computed # words = 65900\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1236\n",
      "all branching entropies was computed # words = 66198\n",
      "all accessor variety was computed # words = 66198\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1239\n",
      "all branching entropies was computed # words = 66317\n",
      "all accessor variety was computed # words = 66317\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1246\n",
      "all branching entropies was computed # words = 66370\n",
      "all accessor variety was computed # words = 66370\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1251\n",
      "all branching entropies was computed # words = 66422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all accessor variety was computed # words = 66422\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1253\n",
      "all branching entropies was computed # words = 66523\n",
      "all accessor variety was computed # words = 66523\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1254\n",
      "all branching entropies was computed # words = 66609\n",
      "all accessor variety was computed # words = 66609\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1255\n",
      "all branching entropies was computed # words = 66702\n",
      "all accessor variety was computed # words = 66702\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1258\n",
      "all branching entropies was computed # words = 66773\n",
      "all accessor variety was computed # words = 66773\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1261\n",
      "all branching entropies was computed # words = 66816\n",
      "all accessor variety was computed # words = 66816\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1264\n",
      "all branching entropies was computed # words = 66877\n",
      "all accessor variety was computed # words = 66877\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1265\n",
      "all branching entropies was computed # words = 66938\n",
      "all accessor variety was computed # words = 66938\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1268\n",
      "all branching entropies was computed # words = 66968\n",
      "all accessor variety was computed # words = 66968\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1271\n",
      "all branching entropies was computed # words = 66986\n",
      "all accessor variety was computed # words = 66986\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1274\n",
      "all branching entropies was computed # words = 67032\n",
      "all accessor variety was computed # words = 67032\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1275\n",
      "all branching entropies was computed # words = 67219\n",
      "all accessor variety was computed # words = 67219\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1278\n",
      "all branching entropies was computed # words = 67247\n",
      "all accessor variety was computed # words = 67247\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1280\n",
      "all branching entropies was computed # words = 67305\n",
      "all accessor variety was computed # words = 67305\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1283\n",
      "all branching entropies was computed # words = 67350\n",
      "all accessor variety was computed # words = 67350\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1287\n",
      "all branching entropies was computed # words = 67537\n",
      "all accessor variety was computed # words = 67537\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1289\n",
      "all branching entropies was computed # words = 67564\n",
      "all accessor variety was computed # words = 67564\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1293\n",
      "all branching entropies was computed # words = 67650\n",
      "all accessor variety was computed # words = 67650\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1296\n",
      "all branching entropies was computed # words = 67679\n",
      "all accessor variety was computed # words = 67679\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1299\n",
      "all branching entropies was computed # words = 67702\n",
      "all accessor variety was computed # words = 67702\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1302\n",
      "all branching entropies was computed # words = 67722\n",
      "all accessor variety was computed # words = 67722\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1305\n",
      "all branching entropies was computed # words = 67734\n",
      "all accessor variety was computed # words = 67734\n",
      "training was done. used memory 1.077 Gb1.077 Gb\n",
      "all cohesion probabilities was computed. # words = 1307\n",
      "all branching entropies was computed # words = 67758\n",
      "all accessor variety was computed # words = 67758\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 8526 from 1 sents. mem=1.077 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27760, mem=0.990 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2446 words\n",
      "[Noun Extractor] checked compounds. discovered 468 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1213 -> 1193\n",
      "[Noun Extractor] postprocessing ignore_features : 1193 -> 1170\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1170 -> 1165\n",
      "[Noun Extractor] 1165 nouns (468 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.953 Gb                    \n",
      "[Noun Extractor] 66.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 25683 from 1 sents. mem=0.953 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=65483, mem=0.949 Gb\n",
      "[Noun Extractor] batch prediction was completed for 12803 words\n",
      "[Noun Extractor] checked compounds. discovered 2935 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 6229 -> 5996\n",
      "[Noun Extractor] postprocessing ignore_features : 5996 -> 5931\n",
      "[Noun Extractor] postprocessing ignore_NJ : 5931 -> 5929\n",
      "[Noun Extractor] 5929 nouns (2935 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.947 Gb                    \n",
      "[Noun Extractor] 72.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7122 from 1 sents. mem=0.947 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14256, mem=0.936 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2049 words\n",
      "[Noun Extractor] checked compounds. discovered 245 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 864 -> 809\n",
      "[Noun Extractor] postprocessing ignore_features : 809 -> 781\n",
      "[Noun Extractor] postprocessing ignore_NJ : 781 -> 781\n",
      "[Noun Extractor] 781 nouns (245 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.934 Gb                    \n",
      "[Noun Extractor] 51.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5786 from 1 sents. mem=0.933 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13089, mem=0.924 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1671 words\n",
      "[Noun Extractor] checked compounds. discovered 99 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 563 -> 554\n",
      "[Noun Extractor] postprocessing ignore_features : 554 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (99 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.924 Gb                    \n",
      "[Noun Extractor] 47.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 30256 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=73836, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 11032 words\n",
      "[Noun Extractor] checked compounds. discovered 2555 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 5447 -> 5060\n",
      "[Noun Extractor] postprocessing ignore_features : 5060 -> 4993\n",
      "[Noun Extractor] postprocessing ignore_NJ : 4993 -> 4989\n",
      "[Noun Extractor] 4989 nouns (2555 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 55.04 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 11194 from 1 sents. mem=0.923 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=39393, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3066 words\n",
      "[Noun Extractor] checked compounds. discovered 710 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1557 -> 1528\n",
      "[Noun Extractor] postprocessing ignore_features : 1528 -> 1501\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1501 -> 1494\n",
      "[Noun Extractor] 1494 nouns (710 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 67.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2772 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5284, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 865 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 310 -> 305\n",
      "[Noun Extractor] postprocessing ignore_features : 305 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3183 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6956, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 361 -> 360\n",
      "[Noun Extractor] postprocessing ignore_features : 360 -> 346\n",
      "[Noun Extractor] postprocessing ignore_NJ : 346 -> 346\n",
      "[Noun Extractor] 346 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2348 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4341, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 674 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 241 -> 233\n",
      "[Noun Extractor] postprocessing ignore_features : 233 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 221\n",
      "[Noun Extractor] 221 nouns (50 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9713 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21610, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2853 words\n",
      "[Noun Extractor] checked compounds. discovered 644 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1583 -> 1403\n",
      "[Noun Extractor] postprocessing ignore_features : 1403 -> 1367\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1367 -> 1360\n",
      "[Noun Extractor] 1360 nouns (644 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 61.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 21192 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=51210, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 6491 words\n",
      "[Noun Extractor] checked compounds. discovered 1655 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 3399 -> 3189\n",
      "[Noun Extractor] postprocessing ignore_features : 3189 -> 3130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 3130 -> 3115\n",
      "[Noun Extractor] 3115 nouns (1655 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 57.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3237 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6634, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1056 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 379 -> 379\n",
      "[Noun Extractor] postprocessing ignore_features : 379 -> 363\n",
      "[Noun Extractor] postprocessing ignore_NJ : 363 -> 363\n",
      "[Noun Extractor] 363 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3480 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6911, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1059 words\n",
      "[Noun Extractor] checked compounds. discovered 66 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 368 -> 363\n",
      "[Noun Extractor] postprocessing ignore_features : 363 -> 354\n",
      "[Noun Extractor] postprocessing ignore_NJ : 354 -> 353\n",
      "[Noun Extractor] 353 nouns (66 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 17440 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=41464, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4836 words\n",
      "[Noun Extractor] checked compounds. discovered 794 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2149 -> 2047\n",
      "[Noun Extractor] postprocessing ignore_features : 2047 -> 2011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2011 -> 2007\n",
      "[Noun Extractor] 2007 nouns (794 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 55.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14048 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27950, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3977 words\n",
      "[Noun Extractor] checked compounds. discovered 633 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1847 -> 1787\n",
      "[Noun Extractor] postprocessing ignore_features : 1787 -> 1759\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1759 -> 1759\n",
      "[Noun Extractor] 1759 nouns (633 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10006 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=21637, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3389 words\n",
      "[Noun Extractor] checked compounds. discovered 594 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1626 -> 1442\n",
      "[Noun Extractor] postprocessing ignore_features : 1442 -> 1408\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1408 -> 1402\n",
      "[Noun Extractor] 1402 nouns (594 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 59.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2290 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 728 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 267\n",
      "[Noun Extractor] postprocessing ignore_features : 267 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 256\n",
      "[Noun Extractor] 256 nouns (23 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 52.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1905 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3962, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 550 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 208\n",
      "[Noun Extractor] postprocessing ignore_features : 208 -> 197\n",
      "[Noun Extractor] postprocessing ignore_NJ : 197 -> 197\n",
      "[Noun Extractor] 197 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3139 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5794, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 968 words\n",
      "[Noun Extractor] checked compounds. discovered 40 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 348 -> 345\n",
      "[Noun Extractor] postprocessing ignore_features : 345 -> 331\n",
      "[Noun Extractor] postprocessing ignore_NJ : 331 -> 331\n",
      "[Noun Extractor] 331 nouns (40 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3318 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6304, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1059 words\n",
      "[Noun Extractor] checked compounds. discovered 61 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 387 -> 379\n",
      "[Noun Extractor] postprocessing ignore_features : 379 -> 361\n",
      "[Noun Extractor] postprocessing ignore_NJ : 361 -> 361\n",
      "[Noun Extractor] 361 nouns (61 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5484 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11171, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1490 words\n",
      "[Noun Extractor] checked compounds. discovered 93 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 473 -> 469\n",
      "[Noun Extractor] postprocessing ignore_features : 469 -> 456\n",
      "[Noun Extractor] postprocessing ignore_NJ : 456 -> 456\n",
      "[Noun Extractor] 456 nouns (93 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.00 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4265, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 749 words\n",
      "[Noun Extractor] checked compounds. discovered 45 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 229 -> 226\n",
      "[Noun Extractor] postprocessing ignore_features : 226 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 215\n",
      "[Noun Extractor] 215 nouns (45 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6074 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11840, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2163 words\n",
      "[Noun Extractor] checked compounds. discovered 308 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 857 -> 781\n",
      "[Noun Extractor] postprocessing ignore_features : 781 -> 751\n",
      "[Noun Extractor] postprocessing ignore_NJ : 751 -> 746\n",
      "[Noun Extractor] 746 nouns (308 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2019 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3325, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 644 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 232 -> 232\n",
      "[Noun Extractor] postprocessing ignore_features : 232 -> 223\n",
      "[Noun Extractor] postprocessing ignore_NJ : 223 -> 222\n",
      "[Noun Extractor] 222 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2430 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4525, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 600 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 187 -> 185\n",
      "[Noun Extractor] postprocessing ignore_features : 185 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7266 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14094, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2802 words\n",
      "[Noun Extractor] checked compounds. discovered 107 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 678 -> 666\n",
      "[Noun Extractor] postprocessing ignore_features : 666 -> 648\n",
      "[Noun Extractor] postprocessing ignore_NJ : 648 -> 647\n",
      "[Noun Extractor] 647 nouns (107 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7351 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=15092, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2444 words\n",
      "[Noun Extractor] checked compounds. discovered 146 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 863 -> 854\n",
      "[Noun Extractor] postprocessing ignore_features : 854 -> 835\n",
      "[Noun Extractor] postprocessing ignore_NJ : 835 -> 834\n",
      "[Noun Extractor] 834 nouns (146 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1452 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2594, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 150 -> 148\n",
      "[Noun Extractor] postprocessing ignore_features : 148 -> 137\n",
      "[Noun Extractor] postprocessing ignore_NJ : 137 -> 137\n",
      "[Noun Extractor] 137 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1297 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2255, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 422 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 106\n",
      "[Noun Extractor] postprocessing ignore_NJ : 106 -> 106\n",
      "[Noun Extractor] 106 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4693 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8084, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1476 words\n",
      "[Noun Extractor] checked compounds. discovered 82 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 512 -> 506\n",
      "[Noun Extractor] postprocessing ignore_features : 506 -> 491\n",
      "[Noun Extractor] postprocessing ignore_NJ : 491 -> 490\n",
      "[Noun Extractor] 490 nouns (82 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2000 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3570, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 700 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 251 -> 249\n",
      "[Noun Extractor] postprocessing ignore_features : 249 -> 242\n",
      "[Noun Extractor] postprocessing ignore_NJ : 242 -> 242\n",
      "[Noun Extractor] 242 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7620 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16636, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2070 words\n",
      "[Noun Extractor] checked compounds. discovered 211 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 808 -> 803\n",
      "[Noun Extractor] postprocessing ignore_features : 803 -> 784\n",
      "[Noun Extractor] postprocessing ignore_NJ : 784 -> 784\n",
      "[Noun Extractor] 784 nouns (211 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7367 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12735, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2849 words\n",
      "[Noun Extractor] checked compounds. discovered 205 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 859 -> 855\n",
      "[Noun Extractor] postprocessing ignore_features : 855 -> 838\n",
      "[Noun Extractor] postprocessing ignore_NJ : 838 -> 836\n",
      "[Noun Extractor] 836 nouns (205 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2298 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4304, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 658 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 242 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 229\n",
      "[Noun Extractor] postprocessing ignore_NJ : 229 -> 229\n",
      "[Noun Extractor] 229 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5278 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10607, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1498 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 565 -> 550\n",
      "[Noun Extractor] postprocessing ignore_features : 550 -> 531\n",
      "[Noun Extractor] postprocessing ignore_NJ : 531 -> 530\n",
      "[Noun Extractor] 530 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1107 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1904, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 380 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 101 -> 101\n",
      "[Noun Extractor] postprocessing ignore_features : 101 -> 93\n",
      "[Noun Extractor] postprocessing ignore_NJ : 93 -> 93\n",
      "[Noun Extractor] 93 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5937 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11362, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1844 words\n",
      "[Noun Extractor] checked compounds. discovered 98 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 561 -> 555\n",
      "[Noun Extractor] postprocessing ignore_features : 555 -> 533\n",
      "[Noun Extractor] postprocessing ignore_NJ : 533 -> 533\n",
      "[Noun Extractor] 533 nouns (98 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 29.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1080 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1940, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 115\n",
      "[Noun Extractor] postprocessing ignore_features : 115 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3709 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7036, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1215 words\n",
      "[Noun Extractor] checked compounds. discovered 65 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 430 -> 427\n",
      "[Noun Extractor] postprocessing ignore_features : 427 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (65 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1434 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2415, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 430 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 124 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.28 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 10143 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=27309, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2588 words\n",
      "[Noun Extractor] checked compounds. discovered 248 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1044 -> 1035\n",
      "[Noun Extractor] postprocessing ignore_features : 1035 -> 1011\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1011 -> 1011\n",
      "[Noun Extractor] 1011 nouns (248 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1692 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2810, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 542 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 174 -> 174\n",
      "[Noun Extractor] postprocessing ignore_features : 174 -> 169\n",
      "[Noun Extractor] postprocessing ignore_NJ : 169 -> 169\n",
      "[Noun Extractor] 169 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 961 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1665, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 296 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3591 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6698, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1170 words\n",
      "[Noun Extractor] checked compounds. discovered 110 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 443 -> 436\n",
      "[Noun Extractor] postprocessing ignore_features : 436 -> 414\n",
      "[Noun Extractor] postprocessing ignore_NJ : 414 -> 414\n",
      "[Noun Extractor] 414 nouns (110 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2119, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 143\n",
      "[Noun Extractor] postprocessing ignore_features : 143 -> 141\n",
      "[Noun Extractor] postprocessing ignore_NJ : 141 -> 141\n",
      "[Noun Extractor] 141 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.58 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6533 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10913, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2429 words\n",
      "[Noun Extractor] checked compounds. discovered 199 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 784 -> 775\n",
      "[Noun Extractor] postprocessing ignore_features : 775 -> 749\n",
      "[Noun Extractor] postprocessing ignore_NJ : 749 -> 747\n",
      "[Noun Extractor] 747 nouns (199 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1618 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2836, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 469 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.54 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2628 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4504, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1465 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 291 -> 288\n",
      "[Noun Extractor] postprocessing ignore_features : 288 -> 269\n",
      "[Noun Extractor] postprocessing ignore_NJ : 269 -> 269\n",
      "[Noun Extractor] 269 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1471 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2492, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 464 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2101 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3806, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 652 words\n",
      "[Noun Extractor] checked compounds. discovered 47 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 235 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 208\n",
      "[Noun Extractor] postprocessing ignore_NJ : 208 -> 208\n",
      "[Noun Extractor] 208 nouns (47 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.16 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2018, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 385 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 127\n",
      "[Noun Extractor] postprocessing ignore_NJ : 127 -> 127\n",
      "[Noun Extractor] 127 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1603, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 88 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.27 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2238 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3922, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 783 words\n",
      "[Noun Extractor] checked compounds. discovered 28 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 240 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 211\n",
      "[Noun Extractor] postprocessing ignore_NJ : 211 -> 211\n",
      "[Noun Extractor] 211 nouns (28 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 711 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1235, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 52.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4912 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11157, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1521 words\n",
      "[Noun Extractor] checked compounds. discovered 117 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 587 -> 569\n",
      "[Noun Extractor] postprocessing ignore_features : 569 -> 543\n",
      "[Noun Extractor] postprocessing ignore_NJ : 543 -> 542\n",
      "[Noun Extractor] 542 nouns (117 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 52.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1737 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3279, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 504 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 165 -> 161\n",
      "[Noun Extractor] postprocessing ignore_features : 161 -> 152\n",
      "[Noun Extractor] postprocessing ignore_NJ : 152 -> 151\n",
      "[Noun Extractor] 151 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6152 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11626, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2132 words\n",
      "[Noun Extractor] checked compounds. discovered 151 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 734 -> 700\n",
      "[Noun Extractor] postprocessing ignore_features : 700 -> 674\n",
      "[Noun Extractor] postprocessing ignore_NJ : 674 -> 672\n",
      "[Noun Extractor] 672 nouns (151 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1524 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2434, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 540 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 148 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 126\n",
      "[Noun Extractor] postprocessing ignore_NJ : 126 -> 126\n",
      "[Noun Extractor] 126 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2419 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4236, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 800 words\n",
      "[Noun Extractor] checked compounds. discovered 69 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 281 -> 278\n",
      "[Noun Extractor] postprocessing ignore_features : 278 -> 274\n",
      "[Noun Extractor] postprocessing ignore_NJ : 274 -> 274\n",
      "[Noun Extractor] 274 nouns (69 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.43 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2065, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4747 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8668, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1541 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 480 -> 468\n",
      "[Noun Extractor] postprocessing ignore_features : 468 -> 452\n",
      "[Noun Extractor] postprocessing ignore_NJ : 452 -> 452\n",
      "[Noun Extractor] 452 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 30.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2296 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4018, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 705 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 210 -> 209\n",
      "[Noun Extractor] postprocessing ignore_features : 209 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3635 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7104, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1061 words\n",
      "[Noun Extractor] checked compounds. discovered 59 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 340 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 309\n",
      "[Noun Extractor] postprocessing ignore_NJ : 309 -> 309\n",
      "[Noun Extractor] 309 nouns (59 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.12 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3401 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6749, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1048 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 96 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 435 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 417\n",
      "[Noun Extractor] postprocessing ignore_NJ : 417 -> 417\n",
      "[Noun Extractor] 417 nouns (96 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 58.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1437 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2403, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 537 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 139 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5727 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12231, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1820 words\n",
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 647 -> 631\n",
      "[Noun Extractor] postprocessing ignore_features : 631 -> 618\n",
      "[Noun Extractor] postprocessing ignore_NJ : 618 -> 618\n",
      "[Noun Extractor] 618 nouns (158 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 53.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2920 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5984, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 910 words\n",
      "[Noun Extractor] checked compounds. discovered 33 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 327 -> 324\n",
      "[Noun Extractor] postprocessing ignore_features : 324 -> 314\n",
      "[Noun Extractor] postprocessing ignore_NJ : 314 -> 312\n",
      "[Noun Extractor] 312 nouns (33 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2964, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 597 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 176 -> 175\n",
      "[Noun Extractor] postprocessing ignore_features : 175 -> 166\n",
      "[Noun Extractor] postprocessing ignore_NJ : 166 -> 166\n",
      "[Noun Extractor] 166 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7336 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13456, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2333 words\n",
      "[Noun Extractor] checked compounds. discovered 283 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 947 -> 857\n",
      "[Noun Extractor] postprocessing ignore_features : 857 -> 833\n",
      "[Noun Extractor] postprocessing ignore_NJ : 833 -> 830\n",
      "[Noun Extractor] 830 nouns (283 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.65 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3506 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=6208, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1202 words\n",
      "[Noun Extractor] checked compounds. discovered 70 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 397 -> 369\n",
      "[Noun Extractor] postprocessing ignore_features : 369 -> 351\n",
      "[Noun Extractor] postprocessing ignore_NJ : 351 -> 350\n",
      "[Noun Extractor] 350 nouns (70 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1298 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2979, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 272 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 75 -> 75\n",
      "[Noun Extractor] postprocessing ignore_features : 75 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2344 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4332, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 719 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 216 -> 214\n",
      "[Noun Extractor] postprocessing ignore_features : 214 -> 204\n",
      "[Noun Extractor] postprocessing ignore_NJ : 204 -> 204\n",
      "[Noun Extractor] 204 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1074 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1921, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 343 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 99\n",
      "[Noun Extractor] postprocessing ignore_NJ : 99 -> 99\n",
      "[Noun Extractor] 99 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3182 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5937, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 964 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 328 -> 322\n",
      "[Noun Extractor] postprocessing ignore_features : 322 -> 306\n",
      "[Noun Extractor] postprocessing ignore_NJ : 306 -> 305\n",
      "[Noun Extractor] 305 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1435 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2444, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.67 % eojeols are covered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1529, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 292 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.96 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1093 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1898, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 390 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 106\n",
      "[Noun Extractor] postprocessing ignore_features : 106 -> 100\n",
      "[Noun Extractor] postprocessing ignore_NJ : 100 -> 100\n",
      "[Noun Extractor] 100 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4042 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8425, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1248 words\n",
      "[Noun Extractor] checked compounds. discovered 116 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 441 -> 429\n",
      "[Noun Extractor] postprocessing ignore_features : 429 -> 410\n",
      "[Noun Extractor] postprocessing ignore_NJ : 410 -> 409\n",
      "[Noun Extractor] 409 nouns (116 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 55.47 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 830 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 232 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 68 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 960 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1592, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 314 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 100 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 915 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1521, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 627 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1035, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.55 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1333 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2357, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 453 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 124\n",
      "[Noun Extractor] postprocessing ignore_features : 124 -> 114\n",
      "[Noun Extractor] postprocessing ignore_NJ : 114 -> 114\n",
      "[Noun Extractor] 114 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9338 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=17634, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3911 words\n",
      "[Noun Extractor] checked compounds. discovered 467 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1146 -> 1120\n",
      "[Noun Extractor] postprocessing ignore_features : 1120 -> 1093\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1093 -> 1093\n",
      "[Noun Extractor] 1093 nouns (467 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1878 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3319, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 136\n",
      "[Noun Extractor] postprocessing ignore_features : 136 -> 128\n",
      "[Noun Extractor] postprocessing ignore_NJ : 128 -> 128\n",
      "[Noun Extractor] 128 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.30 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1025 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1731, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 364 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 113\n",
      "[Noun Extractor] postprocessing ignore_features : 113 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.51 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6882 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=13766, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1976 words\n",
      "[Noun Extractor] checked compounds. discovered 213 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 748 -> 731\n",
      "[Noun Extractor] postprocessing ignore_features : 731 -> 709\n",
      "[Noun Extractor] postprocessing ignore_NJ : 709 -> 708\n",
      "[Noun Extractor] 708 nouns (213 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 9858 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=20851, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 3335 words\n",
      "[Noun Extractor] checked compounds. discovered 567 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1530 -> 1345\n",
      "[Noun Extractor] postprocessing ignore_features : 1345 -> 1311\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1311 -> 1306\n",
      "[Noun Extractor] 1306 nouns (567 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 58.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2250 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3868, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 681 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 223 -> 223\n",
      "[Noun Extractor] postprocessing ignore_features : 223 -> 216\n",
      "[Noun Extractor] postprocessing ignore_NJ : 216 -> 216\n",
      "[Noun Extractor] 216 nouns (30 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1802, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 77\n",
      "[Noun Extractor] 77 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1342 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2310, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 392 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2715, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 416 words\n",
      "[Noun Extractor] checked compounds. discovered 25 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 146 -> 139\n",
      "[Noun Extractor] postprocessing ignore_features : 139 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (25 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 861 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1740, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 81\n",
      "[Noun Extractor] postprocessing ignore_NJ : 81 -> 81\n",
      "[Noun Extractor] 81 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 52.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2911 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4942, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1289 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 302 -> 301\n",
      "[Noun Extractor] postprocessing ignore_features : 301 -> 288\n",
      "[Noun Extractor] postprocessing ignore_NJ : 288 -> 288\n",
      "[Noun Extractor] 288 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1008 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1602, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 332 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 90\n",
      "[Noun Extractor] postprocessing ignore_NJ : 90 -> 90\n",
      "[Noun Extractor] 90 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1362 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2450, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 425 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_features : 129 -> 117\n",
      "[Noun Extractor] postprocessing ignore_NJ : 117 -> 117\n",
      "[Noun Extractor] 117 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6695 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12515, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2134 words\n",
      "[Noun Extractor] checked compounds. discovered 260 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 779 -> 691\n",
      "[Noun Extractor] postprocessing ignore_features : 691 -> 666\n",
      "[Noun Extractor] postprocessing ignore_NJ : 666 -> 666\n",
      "[Noun Extractor] 666 nouns (260 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2058 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3747, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 695 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 226 -> 222\n",
      "[Noun Extractor] postprocessing ignore_features : 222 -> 215\n",
      "[Noun Extractor] postprocessing ignore_NJ : 215 -> 215\n",
      "[Noun Extractor] 215 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1143 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2132, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 324 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (9 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 741 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1221, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 236 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 86 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 73\n",
      "[Noun Extractor] 73 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1312 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2387, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 401 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 138 -> 138\n",
      "[Noun Extractor] postprocessing ignore_features : 138 -> 130\n",
      "[Noun Extractor] postprocessing ignore_NJ : 130 -> 130\n",
      "[Noun Extractor] 130 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2648, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 289 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 78 -> 78\n",
      "[Noun Extractor] postprocessing ignore_features : 78 -> 73\n",
      "[Noun Extractor] postprocessing ignore_NJ : 73 -> 73\n",
      "[Noun Extractor] 73 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1564 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2776, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 526 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 171 -> 169\n",
      "[Noun Extractor] postprocessing ignore_features : 169 -> 160\n",
      "[Noun Extractor] postprocessing ignore_NJ : 160 -> 160\n",
      "[Noun Extractor] 160 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1745 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3449, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 655 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 166 -> 166\n",
      "[Noun Extractor] postprocessing ignore_features : 166 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 22.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4415 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8218, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1286 words\n",
      "[Noun Extractor] checked compounds. discovered 32 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 370 -> 366\n",
      "[Noun Extractor] postprocessing ignore_features : 366 -> 356\n",
      "[Noun Extractor] postprocessing ignore_NJ : 356 -> 356\n",
      "[Noun Extractor] 356 nouns (32 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 23.46 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1927 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3650, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 561 words\n",
      "[Noun Extractor] checked compounds. discovered 20 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 181 -> 179\n",
      "[Noun Extractor] postprocessing ignore_features : 179 -> 167\n",
      "[Noun Extractor] postprocessing ignore_NJ : 167 -> 167\n",
      "[Noun Extractor] 167 nouns (20 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6659 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11505, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2168 words\n",
      "[Noun Extractor] checked compounds. discovered 198 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 707 -> 686\n",
      "[Noun Extractor] postprocessing ignore_features : 686 -> 663\n",
      "[Noun Extractor] postprocessing ignore_NJ : 663 -> 662\n",
      "[Noun Extractor] 662 nouns (198 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1388 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2888, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 75\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75 -> 75\n",
      "[Noun Extractor] 75 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1043 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1937, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 344 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 133 -> 133\n",
      "[Noun Extractor] postprocessing ignore_features : 133 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5038 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10506, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1403 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 485 -> 482\n",
      "[Noun Extractor] postprocessing ignore_features : 482 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 470\n",
      "[Noun Extractor] 470 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.92 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2316 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3770, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 750 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 231 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 214\n",
      "[Noun Extractor] postprocessing ignore_NJ : 214 -> 214\n",
      "[Noun Extractor] 214 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1399 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2424, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 512 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 152 -> 152\n",
      "[Noun Extractor] postprocessing ignore_features : 152 -> 143\n",
      "[Noun Extractor] postprocessing ignore_NJ : 143 -> 143\n",
      "[Noun Extractor] 143 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 737 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1236, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 73 -> 73\n",
      "[Noun Extractor] postprocessing ignore_features : 73 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 65\n",
      "[Noun Extractor] 65 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.26 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 14814 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=35354, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 4414 words\n",
      "[Noun Extractor] checked compounds. discovered 803 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2243 -> 2008\n",
      "[Noun Extractor] postprocessing ignore_features : 2008 -> 1967\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1967 -> 1964\n",
      "[Noun Extractor] 1964 nouns (803 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 59.63 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 407 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=815, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 123 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 29 -> 28\n",
      "[Noun Extractor] postprocessing ignore_features : 28 -> 25\n",
      "[Noun Extractor] postprocessing ignore_NJ : 25 -> 25\n",
      "[Noun Extractor] 25 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1379 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2841, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 386 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 82 -> 82\n",
      "[Noun Extractor] postprocessing ignore_features : 82 -> 79\n",
      "[Noun Extractor] postprocessing ignore_NJ : 79 -> 79\n",
      "[Noun Extractor] 79 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 35.09 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 585 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=993, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 170 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=995, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 43\n",
      "[Noun Extractor] postprocessing ignore_NJ : 43 -> 43\n",
      "[Noun Extractor] 43 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1686 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2514, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 90 -> 88\n",
      "[Noun Extractor] postprocessing ignore_features : 88 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 31.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1357 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2475, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 442 words\n",
      "[Noun Extractor] checked compounds. discovered 26 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 159 -> 159\n",
      "[Noun Extractor] postprocessing ignore_features : 159 -> 151\n",
      "[Noun Extractor] postprocessing ignore_NJ : 151 -> 151\n",
      "[Noun Extractor] 151 nouns (26 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.19 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1014 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1661, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 358 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 104 -> 104\n",
      "[Noun Extractor] postprocessing ignore_features : 104 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 921 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1671, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 298 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 87 -> 87\n",
      "[Noun Extractor] postprocessing ignore_features : 87 -> 82\n",
      "[Noun Extractor] postprocessing ignore_NJ : 82 -> 82\n",
      "[Noun Extractor] 82 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.70 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 900 from 1 sents. mem=0.923 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1378, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1534 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2756, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 461 words\n",
      "[Noun Extractor] checked compounds. discovered 41 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 132\n",
      "[Noun Extractor] postprocessing ignore_features : 132 -> 129\n",
      "[Noun Extractor] postprocessing ignore_NJ : 129 -> 129\n",
      "[Noun Extractor] 129 nouns (41 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 580 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1106, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 157 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 31 -> 30\n",
      "[Noun Extractor] postprocessing ignore_features : 30 -> 26\n",
      "[Noun Extractor] postprocessing ignore_NJ : 26 -> 26\n",
      "[Noun Extractor] 26 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 840 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1198, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.39 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1139 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1942, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 308 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 462 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1365, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 82 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 17 -> 17\n",
      "[Noun Extractor] postprocessing ignore_features : 17 -> 16\n",
      "[Noun Extractor] postprocessing ignore_NJ : 16 -> 16\n",
      "[Noun Extractor] 16 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 54.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1048 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1771, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 323 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 106 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 98\n",
      "[Noun Extractor] postprocessing ignore_NJ : 98 -> 98\n",
      "[Noun Extractor] 98 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.62 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1157 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1788, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 356 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 95 -> 95\n",
      "[Noun Extractor] postprocessing ignore_features : 95 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 34.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2581 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4731, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 649 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 178\n",
      "[Noun Extractor] postprocessing ignore_features : 178 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 174\n",
      "[Noun Extractor] 174 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1865 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3259, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 583 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 151 -> 151\n",
      "[Noun Extractor] postprocessing ignore_features : 151 -> 144\n",
      "[Noun Extractor] postprocessing ignore_NJ : 144 -> 144\n",
      "[Noun Extractor] 144 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 659 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1024, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 199 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.91 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 775 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1285, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 54 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (3 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.03 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 731 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1143, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 240 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1367 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2681, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 395 words\n",
      "[Noun Extractor] checked compounds. discovered 29 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 155 -> 154\n",
      "[Noun Extractor] postprocessing ignore_features : 154 -> 145\n",
      "[Noun Extractor] postprocessing ignore_NJ : 145 -> 145\n",
      "[Noun Extractor] 145 nouns (29 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 57.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2281, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 427 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 142 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 134\n",
      "[Noun Extractor] 134 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.17 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 952 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1457, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 54\n",
      "[Noun Extractor] postprocessing ignore_features : 54 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 33.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1216 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1977, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 388 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 126 -> 126\n",
      "[Noun Extractor] postprocessing ignore_features : 126 -> 119\n",
      "[Noun Extractor] postprocessing ignore_NJ : 119 -> 119\n",
      "[Noun Extractor] 119 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 918 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1687, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 302 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 91 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 88\n",
      "[Noun Extractor] postprocessing ignore_NJ : 88 -> 88\n",
      "[Noun Extractor] 88 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4988 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9140, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1496 words\n",
      "[Noun Extractor] checked compounds. discovered 44 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 412 -> 403\n",
      "[Noun Extractor] postprocessing ignore_features : 403 -> 384\n",
      "[Noun Extractor] postprocessing ignore_NJ : 384 -> 384\n",
      "[Noun Extractor] 384 nouns (44 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 23.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1186 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1824, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 387 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 116\n",
      "[Noun Extractor] postprocessing ignore_features : 116 -> 113\n",
      "[Noun Extractor] postprocessing ignore_NJ : 113 -> 113\n",
      "[Noun Extractor] 113 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1508, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 121 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_features : 24 -> 24\n",
      "[Noun Extractor] postprocessing ignore_NJ : 24 -> 24\n",
      "[Noun Extractor] 24 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 6448 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12676, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2077 words\n",
      "[Noun Extractor] checked compounds. discovered 81 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 637 -> 628\n",
      "[Noun Extractor] postprocessing ignore_features : 628 -> 603\n",
      "[Noun Extractor] postprocessing ignore_NJ : 603 -> 602\n",
      "[Noun Extractor] 602 nouns (81 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 540 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=897, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 175 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 414 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=706, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 137 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 38 -> 38\n",
      "[Noun Extractor] postprocessing ignore_features : 38 -> 34\n",
      "[Noun Extractor] postprocessing ignore_NJ : 34 -> 34\n",
      "[Noun Extractor] 34 nouns (2 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1158 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2085, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 348 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 117 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 111\n",
      "[Noun Extractor] postprocessing ignore_NJ : 111 -> 110\n",
      "[Noun Extractor] 110 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 747 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1168, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 74 -> 74\n",
      "[Noun Extractor] postprocessing ignore_features : 74 -> 70\n",
      "[Noun Extractor] postprocessing ignore_NJ : 70 -> 70\n",
      "[Noun Extractor] 70 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1282 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2048, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 128 -> 128\n",
      "[Noun Extractor] postprocessing ignore_features : 128 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 901 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1383, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 280 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 63\n",
      "[Noun Extractor] postprocessing ignore_NJ : 63 -> 63\n",
      "[Noun Extractor] 63 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 35.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1340 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2322, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 447 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 825 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1292, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 35.76 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1853 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3651, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 631 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 170 -> 170\n",
      "[Noun Extractor] postprocessing ignore_features : 170 -> 163\n",
      "[Noun Extractor] postprocessing ignore_NJ : 163 -> 162\n",
      "[Noun Extractor] 162 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5640 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=10126, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2107 words\n",
      "[Noun Extractor] checked compounds. discovered 111 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 553 -> 545\n",
      "[Noun Extractor] postprocessing ignore_features : 545 -> 525\n",
      "[Noun Extractor] postprocessing ignore_NJ : 525 -> 525\n",
      "[Noun Extractor] 525 nouns (111 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 998 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1518, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.42 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2599 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4578, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 802 words\n",
      "[Noun Extractor] checked compounds. discovered 39 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 303 -> 296\n",
      "[Noun Extractor] postprocessing ignore_features : 296 -> 282\n",
      "[Noun Extractor] postprocessing ignore_NJ : 282 -> 282\n",
      "[Noun Extractor] 282 nouns (39 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2605 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4864, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 810 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 242\n",
      "[Noun Extractor] postprocessing ignore_features : 242 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1316 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2414, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 116 -> 114\n",
      "[Noun Extractor] postprocessing ignore_features : 114 -> 107\n",
      "[Noun Extractor] postprocessing ignore_NJ : 107 -> 107\n",
      "[Noun Extractor] 107 nouns (8 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2830 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5334, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 890 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 271 -> 269\n",
      "[Noun Extractor] postprocessing ignore_features : 269 -> 253\n",
      "[Noun Extractor] postprocessing ignore_NJ : 253 -> 253\n",
      "[Noun Extractor] 253 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1679 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2674, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 552 words\n",
      "[Noun Extractor] checked compounds. discovered 21 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 145 -> 145\n",
      "[Noun Extractor] postprocessing ignore_features : 145 -> 138\n",
      "[Noun Extractor] postprocessing ignore_NJ : 138 -> 138\n",
      "[Noun Extractor] 138 nouns (21 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1092 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2000, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 406 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 76\n",
      "[Noun Extractor] 76 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.85 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 789 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1214, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 235 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 66\n",
      "[Noun Extractor] postprocessing ignore_features : 66 -> 62\n",
      "[Noun Extractor] postprocessing ignore_NJ : 62 -> 62\n",
      "[Noun Extractor] 62 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1277 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2145, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 408 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 121 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 116\n",
      "[Noun Extractor] postprocessing ignore_NJ : 116 -> 116\n",
      "[Noun Extractor] 116 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.15 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1083, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 196 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 39\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39 -> 39\n",
      "[Noun Extractor] 39 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 647 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=949, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 56\n",
      "[Noun Extractor] postprocessing ignore_features : 56 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.78 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1133, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 205 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 639 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1076, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 211 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 65 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.23 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2619 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5074, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 759 words\n",
      "[Noun Extractor] checked compounds. discovered 27 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 236 -> 234\n",
      "[Noun Extractor] postprocessing ignore_features : 234 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (27 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.34 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 828 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1368, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 267 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 83 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 80\n",
      "[Noun Extractor] postprocessing ignore_NJ : 80 -> 80\n",
      "[Noun Extractor] 80 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.08 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 831 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1393, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 253 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 72\n",
      "[Noun Extractor] postprocessing ignore_NJ : 72 -> 72\n",
      "[Noun Extractor] 72 nouns (10 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.49 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1875 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3130, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 593 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 188 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 180\n",
      "[Noun Extractor] postprocessing ignore_NJ : 180 -> 180\n",
      "[Noun Extractor] 180 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.36 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 895 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1409, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 321 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 67\n",
      "[Noun Extractor] postprocessing ignore_features : 67 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.40 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 869 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1322, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 260 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 79\n",
      "[Noun Extractor] postprocessing ignore_features : 79 -> 74\n",
      "[Noun Extractor] postprocessing ignore_NJ : 74 -> 74\n",
      "[Noun Extractor] 74 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2116 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3757, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 702 words\n",
      "[Noun Extractor] checked compounds. discovered 24 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 219 -> 219\n",
      "[Noun Extractor] postprocessing ignore_features : 219 -> 212\n",
      "[Noun Extractor] postprocessing ignore_NJ : 212 -> 211\n",
      "[Noun Extractor] 211 nouns (24 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2822 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5142, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1042 words\n",
      "[Noun Extractor] checked compounds. discovered 57 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 332 -> 311\n",
      "[Noun Extractor] postprocessing ignore_features : 311 -> 293\n",
      "[Noun Extractor] postprocessing ignore_NJ : 293 -> 292\n",
      "[Noun Extractor] 292 nouns (57 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 891 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1467, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 263 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 81 -> 81\n",
      "[Noun Extractor] postprocessing ignore_features : 81 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.99 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1033 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1537, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 322 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 89\n",
      "[Noun Extractor] postprocessing ignore_features : 89 -> 85\n",
      "[Noun Extractor] postprocessing ignore_NJ : 85 -> 85\n",
      "[Noun Extractor] 85 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 719 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1029, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 228 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 72 -> 72\n",
      "[Noun Extractor] postprocessing ignore_features : 72 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 69\n",
      "[Noun Extractor] 69 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.35 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 927 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2730, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 341 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 92 -> 91\n",
      "[Noun Extractor] postprocessing ignore_features : 91 -> 87\n",
      "[Noun Extractor] postprocessing ignore_NJ : 87 -> 87\n",
      "[Noun Extractor] 87 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 35.79 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3471 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7088, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1125 words\n",
      "[Noun Extractor] checked compounds. discovered 49 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 317 -> 313\n",
      "[Noun Extractor] postprocessing ignore_features : 313 -> 303\n",
      "[Noun Extractor] postprocessing ignore_NJ : 303 -> 303\n",
      "[Noun Extractor] 303 nouns (49 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.74 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1287 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2327, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 318 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 68\n",
      "[Noun Extractor] postprocessing ignore_features : 68 -> 65\n",
      "[Noun Extractor] postprocessing ignore_NJ : 65 -> 65\n",
      "[Noun Extractor] 65 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.80 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2435 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4039, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 789 words\n",
      "[Noun Extractor] checked compounds. discovered 30 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 244 -> 240\n",
      "[Noun Extractor] postprocessing ignore_features : 240 -> 228\n",
      "[Noun Extractor] postprocessing ignore_NJ : 228 -> 227\n",
      "[Noun Extractor] 227 nouns (30 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 815 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1499, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 265 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 79 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 67\n",
      "[Noun Extractor] postprocessing ignore_NJ : 67 -> 67\n",
      "[Noun Extractor] 67 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1886 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3428, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 587 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 211 -> 207\n",
      "[Noun Extractor] postprocessing ignore_features : 207 -> 193\n",
      "[Noun Extractor] postprocessing ignore_NJ : 193 -> 193\n",
      "[Noun Extractor] 193 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.07 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4991 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8667, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1572 words\n",
      "[Noun Extractor] checked compounds. discovered 38 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 417 -> 412\n",
      "[Noun Extractor] postprocessing ignore_features : 412 -> 394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 394 -> 394\n",
      "[Noun Extractor] 394 nouns (38 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 22.56 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1608 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2632, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 448 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 136 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 124\n",
      "[Noun Extractor] postprocessing ignore_NJ : 124 -> 124\n",
      "[Noun Extractor] 124 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1200 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2123, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 353 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 111 -> 108\n",
      "[Noun Extractor] postprocessing ignore_features : 108 -> 104\n",
      "[Noun Extractor] postprocessing ignore_NJ : 104 -> 104\n",
      "[Noun Extractor] 104 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.32 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 404 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=733, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 100 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 18 -> 18\n",
      "[Noun Extractor] postprocessing ignore_features : 18 -> 17\n",
      "[Noun Extractor] postprocessing ignore_NJ : 17 -> 17\n",
      "[Noun Extractor] 17 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 33.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 681 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1161, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 212 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 67 -> 65\n",
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.22 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 680 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1073, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 210 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 51 -> 51\n",
      "[Noun Extractor] postprocessing ignore_features : 51 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1098 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1746, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 371 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 77 -> 77\n",
      "[Noun Extractor] postprocessing ignore_features : 77 -> 69\n",
      "[Noun Extractor] postprocessing ignore_NJ : 69 -> 68\n",
      "[Noun Extractor] 68 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1170 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1796, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 349 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 93\n",
      "[Noun Extractor] postprocessing ignore_features : 93 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 619 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=941, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 59 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.38 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 853 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1327, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 273 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 66 -> 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 65 -> 58\n",
      "[Noun Extractor] postprocessing ignore_NJ : 58 -> 58\n",
      "[Noun Extractor] 58 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1569 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2518, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 488 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 140 -> 140\n",
      "[Noun Extractor] postprocessing ignore_features : 140 -> 133\n",
      "[Noun Extractor] postprocessing ignore_NJ : 133 -> 133\n",
      "[Noun Extractor] 133 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.95 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2739 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5114, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 849 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 268 -> 263\n",
      "[Noun Extractor] postprocessing ignore_features : 263 -> 252\n",
      "[Noun Extractor] postprocessing ignore_NJ : 252 -> 252\n",
      "[Noun Extractor] 252 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.50 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1870 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3267, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 474 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 134 -> 134\n",
      "[Noun Extractor] postprocessing ignore_features : 134 -> 125\n",
      "[Noun Extractor] postprocessing ignore_NJ : 125 -> 125\n",
      "[Noun Extractor] 125 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 778 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1243, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 255 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 64 -> 64\n",
      "[Noun Extractor] postprocessing ignore_features : 64 -> 61\n",
      "[Noun Extractor] postprocessing ignore_NJ : 61 -> 61\n",
      "[Noun Extractor] 61 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 610 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=953, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 153 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 49 -> 49\n",
      "[Noun Extractor] postprocessing ignore_features : 49 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4321 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8200, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1695 words\n",
      "[Noun Extractor] checked compounds. discovered 68 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 454 -> 454\n",
      "[Noun Extractor] postprocessing ignore_features : 454 -> 440\n",
      "[Noun Extractor] postprocessing ignore_NJ : 440 -> 440\n",
      "[Noun Extractor] 440 nouns (68 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1987 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3336, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 762 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 185 -> 184\n",
      "[Noun Extractor] postprocessing ignore_features : 184 -> 174\n",
      "[Noun Extractor] postprocessing ignore_NJ : 174 -> 173\n",
      "[Noun Extractor] 173 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1453 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2367, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 515 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 122 -> 121\n",
      "[Noun Extractor] postprocessing ignore_features : 121 -> 115\n",
      "[Noun Extractor] postprocessing ignore_NJ : 115 -> 115\n",
      "[Noun Extractor] 115 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.61 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2616 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4803, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 776 words\n",
      "[Noun Extractor] checked compounds. discovered 31 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 238 -> 236\n",
      "[Noun Extractor] postprocessing ignore_features : 236 -> 224\n",
      "[Noun Extractor] postprocessing ignore_NJ : 224 -> 224\n",
      "[Noun Extractor] 224 nouns (31 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 873 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1460, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 248 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 69 -> 69\n",
      "[Noun Extractor] postprocessing ignore_features : 69 -> 64\n",
      "[Noun Extractor] postprocessing ignore_NJ : 64 -> 64\n",
      "[Noun Extractor] 64 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7420 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12810, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2466 words\n",
      "[Noun Extractor] checked compounds. discovered 138 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 792 -> 747\n",
      "[Noun Extractor] postprocessing ignore_features : 747 -> 722\n",
      "[Noun Extractor] postprocessing ignore_NJ : 722 -> 722\n",
      "[Noun Extractor] 722 nouns (138 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 35.28 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1291 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2270, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 125 -> 125\n",
      "[Noun Extractor] postprocessing ignore_features : 125 -> 120\n",
      "[Noun Extractor] postprocessing ignore_NJ : 120 -> 120\n",
      "[Noun Extractor] 120 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 940 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1372, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 347 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 89 -> 83\n",
      "[Noun Extractor] postprocessing ignore_features : 83 -> 76\n",
      "[Noun Extractor] postprocessing ignore_NJ : 76 -> 76\n",
      "[Noun Extractor] 76 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 33.97 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1273 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2410, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 716 words\n",
      "[Noun Extractor] checked compounds. discovered 11 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 94 -> 94\n",
      "[Noun Extractor] postprocessing ignore_features : 94 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (11 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1257 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2300, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 417 words\n",
      "[Noun Extractor] checked compounds. discovered 36 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 119 -> 117\n",
      "[Noun Extractor] postprocessing ignore_features : 117 -> 112\n",
      "[Noun Extractor] postprocessing ignore_NJ : 112 -> 111\n",
      "[Noun Extractor] 111 nouns (36 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 51.83 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 720 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1267, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 180 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 41\n",
      "[Noun Extractor] postprocessing ignore_NJ : 41 -> 41\n",
      "[Noun Extractor] 41 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 34.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 654 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1097, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 187 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.29 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 569 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_features : 37 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1030, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 51\n",
      "[Noun Extractor] postprocessing ignore_NJ : 51 -> 51\n",
      "[Noun Extractor] 51 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1951 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3177, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 630 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 178 -> 176\n",
      "[Noun Extractor] postprocessing ignore_features : 176 -> 171\n",
      "[Noun Extractor] postprocessing ignore_NJ : 171 -> 171\n",
      "[Noun Extractor] 171 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.94 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 7179 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=14143, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 2397 words\n",
      "[Noun Extractor] checked compounds. discovered 397 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 1056 -> 1031\n",
      "[Noun Extractor] postprocessing ignore_features : 1031 -> 1008\n",
      "[Noun Extractor] postprocessing ignore_NJ : 1008 -> 1000\n",
      "[Noun Extractor] 1000 nouns (397 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 58.18 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4323 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8442, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1552 words\n",
      "[Noun Extractor] checked compounds. discovered 86 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 499 -> 486\n",
      "[Noun Extractor] postprocessing ignore_features : 486 -> 470\n",
      "[Noun Extractor] postprocessing ignore_NJ : 470 -> 469\n",
      "[Noun Extractor] 469 nouns (86 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1986 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3945, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 645 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 230 -> 229\n",
      "[Noun Extractor] postprocessing ignore_features : 229 -> 222\n",
      "[Noun Extractor] postprocessing ignore_NJ : 222 -> 220\n",
      "[Noun Extractor] 220 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.10 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 812 from 1 sents. mem=0.923 Gb                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1278, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 238 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52 -> 52\n",
      "[Noun Extractor] postprocessing ignore_features : 52 -> 50\n",
      "[Noun Extractor] postprocessing ignore_NJ : 50 -> 50\n",
      "[Noun Extractor] 50 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 34.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1328 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2112, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 438 words\n",
      "[Noun Extractor] checked compounds. discovered 18 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 127 -> 127\n",
      "[Noun Extractor] postprocessing ignore_features : 127 -> 121\n",
      "[Noun Extractor] postprocessing ignore_NJ : 121 -> 121\n",
      "[Noun Extractor] 121 nouns (18 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 16366 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=37636, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 5169 words\n",
      "[Noun Extractor] checked compounds. discovered 1401 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 2799 -> 2443\n",
      "[Noun Extractor] postprocessing ignore_features : 2443 -> 2394\n",
      "[Noun Extractor] postprocessing ignore_NJ : 2394 -> 2391\n",
      "[Noun Extractor] 2391 nouns (1401 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 56.64 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 560 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=836, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 145 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_features : 44 -> 44\n",
      "[Noun Extractor] postprocessing ignore_NJ : 44 -> 44\n",
      "[Noun Extractor] 44 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.66 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1658 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2790, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 489 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 143 -> 142\n",
      "[Noun Extractor] postprocessing ignore_features : 142 -> 134\n",
      "[Noun Extractor] postprocessing ignore_NJ : 134 -> 133\n",
      "[Noun Extractor] 133 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.25 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5238 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=12604, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1419 words\n",
      "[Noun Extractor] checked compounds. discovered 132 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 575 -> 563\n",
      "[Noun Extractor] postprocessing ignore_features : 563 -> 542\n",
      "[Noun Extractor] postprocessing ignore_NJ : 542 -> 542\n",
      "[Noun Extractor] 542 nouns (132 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 50.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1083 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1695, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 330 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99 -> 99\n",
      "[Noun Extractor] postprocessing ignore_features : 99 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 570 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=840, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 9 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 47 -> 47\n",
      "[Noun Extractor] postprocessing ignore_features : 47 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (9 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 42.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2523 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4566, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 770 words\n",
      "[Noun Extractor] checked compounds. discovered 60 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 255 -> 252\n",
      "[Noun Extractor] postprocessing ignore_features : 252 -> 239\n",
      "[Noun Extractor] postprocessing ignore_NJ : 239 -> 239\n",
      "[Noun Extractor] 239 nouns (60 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 605 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1051, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 200 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 48 -> 48\n",
      "[Noun Extractor] postprocessing ignore_features : 48 -> 46\n",
      "[Noun Extractor] postprocessing ignore_NJ : 46 -> 46\n",
      "[Noun Extractor] 46 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 364 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=572, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 105 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 19 -> 19\n",
      "[Noun Extractor] postprocessing ignore_features : 19 -> 18\n",
      "[Noun Extractor] postprocessing ignore_NJ : 18 -> 18\n",
      "[Noun Extractor] 18 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.69 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2779 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5223, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 883 words\n",
      "[Noun Extractor] checked compounds. discovered 50 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 257 -> 256\n",
      "[Noun Extractor] postprocessing ignore_features : 256 -> 248\n",
      "[Noun Extractor] postprocessing ignore_NJ : 248 -> 247\n",
      "[Noun Extractor] 247 nouns (50 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.02 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 501 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=727, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 174 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 34 -> 34\n",
      "[Noun Extractor] postprocessing ignore_features : 34 -> 31\n",
      "[Noun Extractor] postprocessing ignore_NJ : 31 -> 31\n",
      "[Noun Extractor] 31 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.87 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1442 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2514, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 436 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 103 -> 103\n",
      "[Noun Extractor] postprocessing ignore_features : 103 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.73 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 682 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1107, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 14 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 70 -> 70\n",
      "[Noun Extractor] postprocessing ignore_features : 70 -> 66\n",
      "[Noun Extractor] postprocessing ignore_NJ : 66 -> 66\n",
      "[Noun Extractor] 66 nouns (14 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.98 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1016 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1653, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 307 words\n",
      "[Noun Extractor] checked compounds. discovered 6 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 58 -> 58\n",
      "[Noun Extractor] postprocessing ignore_features : 58 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (6 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.72 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 4938 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9814, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1722 words\n",
      "[Noun Extractor] checked compounds. discovered 161 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 651 -> 582\n",
      "[Noun Extractor] postprocessing ignore_features : 582 -> 566\n",
      "[Noun Extractor] postprocessing ignore_NJ : 566 -> 565\n",
      "[Noun Extractor] 565 nouns (161 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 53.37 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 784 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1121, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 218 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 57\n",
      "[Noun Extractor] postprocessing ignore_NJ : 57 -> 57\n",
      "[Noun Extractor] 57 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 31.67 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1947 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3436, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 682 words\n",
      "[Noun Extractor] checked compounds. discovered 23 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 198 -> 196\n",
      "[Noun Extractor] postprocessing ignore_features : 196 -> 185\n",
      "[Noun Extractor] postprocessing ignore_NJ : 185 -> 185\n",
      "[Noun Extractor] 185 nouns (23 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.14 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2979 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5350, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1163 words\n",
      "[Noun Extractor] checked compounds. discovered 12 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 265 -> 264\n",
      "[Noun Extractor] postprocessing ignore_features : 264 -> 257\n",
      "[Noun Extractor] postprocessing ignore_NJ : 257 -> 257\n",
      "[Noun Extractor] 257 nouns (12 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.81 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1062 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1632, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 331 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.05 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1138 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1907, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 379 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 113 -> 110\n",
      "[Noun Extractor] postprocessing ignore_features : 110 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 45.41 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 597 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=852, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 222 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 56 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 52\n",
      "[Noun Extractor] postprocessing ignore_NJ : 52 -> 52\n",
      "[Noun Extractor] 52 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.20 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5494 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=11507, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1688 words\n",
      "[Noun Extractor] checked compounds. discovered 158 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 700 -> 675\n",
      "[Noun Extractor] postprocessing ignore_features : 675 -> 655\n",
      "[Noun Extractor] postprocessing ignore_NJ : 655 -> 655\n",
      "[Noun Extractor] 655 nouns (158 compounds) with min frequency=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 53.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1762 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3517, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 531 words\n",
      "[Noun Extractor] checked compounds. discovered 54 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 173 -> 163\n",
      "[Noun Extractor] postprocessing ignore_features : 163 -> 153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 153 -> 153\n",
      "[Noun Extractor] 153 nouns (54 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 49.45 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 739 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1259, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 220 words\n",
      "[Noun Extractor] checked compounds. discovered 10 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 76 -> 76\n",
      "[Noun Extractor] postprocessing ignore_features : 76 -> 71\n",
      "[Noun Extractor] postprocessing ignore_NJ : 71 -> 71\n",
      "[Noun Extractor] 71 nouns (10 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.93 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1041 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1829, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 310 words\n",
      "[Noun Extractor] checked compounds. discovered 13 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 96 -> 96\n",
      "[Noun Extractor] postprocessing ignore_features : 96 -> 91\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91 -> 91\n",
      "[Noun Extractor] 91 nouns (13 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3650 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5819, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1200 words\n",
      "[Noun Extractor] checked compounds. discovered 64 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 360 -> 356\n",
      "[Noun Extractor] postprocessing ignore_features : 356 -> 341\n",
      "[Noun Extractor] postprocessing ignore_NJ : 341 -> 341\n",
      "[Noun Extractor] 341 nouns (64 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.60 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3611 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5885, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1085 words\n",
      "[Noun Extractor] checked compounds. discovered 19 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 295 -> 292\n",
      "[Noun Extractor] postprocessing ignore_features : 292 -> 277\n",
      "[Noun Extractor] postprocessing ignore_NJ : 277 -> 277\n",
      "[Noun Extractor] 277 nouns (19 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 22.24 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2399 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3832, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 778 words\n",
      "[Noun Extractor] checked compounds. discovered 42 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 247 -> 246\n",
      "[Noun Extractor] postprocessing ignore_features : 246 -> 238\n",
      "[Noun Extractor] postprocessing ignore_NJ : 238 -> 238\n",
      "[Noun Extractor] 238 nouns (42 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 46.01 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1610 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2739, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 544 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 193 -> 191\n",
      "[Noun Extractor] postprocessing ignore_features : 191 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 47.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 641 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1092, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 206 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 42 -> 42\n",
      "[Noun Extractor] postprocessing ignore_features : 42 -> 38\n",
      "[Noun Extractor] postprocessing ignore_NJ : 38 -> 38\n",
      "[Noun Extractor] 38 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 32.88 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1354 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2282, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 503 words\n",
      "[Noun Extractor] checked compounds. discovered 16 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 105 -> 105\n",
      "[Noun Extractor] postprocessing ignore_features : 105 -> 102\n",
      "[Noun Extractor] postprocessing ignore_NJ : 102 -> 102\n",
      "[Noun Extractor] 102 nouns (16 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.59 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1931 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=3512, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 632 words\n",
      "[Noun Extractor] checked compounds. discovered 34 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 190 -> 186\n",
      "[Noun Extractor] postprocessing ignore_features : 186 -> 177\n",
      "[Noun Extractor] postprocessing ignore_NJ : 177 -> 177\n",
      "[Noun Extractor] 177 nouns (34 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.53 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 847 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1414, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 237 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 78\n",
      "[Noun Extractor] postprocessing ignore_NJ : 78 -> 78\n",
      "[Noun Extractor] 78 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 38.68 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 466 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=667, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 155 words\n",
      "[Noun Extractor] checked compounds. discovered 3 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 37 -> 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] postprocessing ignore_features : 37 -> 35\n",
      "[Noun Extractor] postprocessing ignore_NJ : 35 -> 35\n",
      "[Noun Extractor] 35 nouns (3 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 40.33 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 974 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1647, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 287 words\n",
      "[Noun Extractor] checked compounds. discovered 7 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 93 -> 90\n",
      "[Noun Extractor] postprocessing ignore_features : 90 -> 86\n",
      "[Noun Extractor] postprocessing ignore_NJ : 86 -> 86\n",
      "[Noun Extractor] 86 nouns (7 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 43.90 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 5164 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=8252, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1807 words\n",
      "[Noun Extractor] checked compounds. discovered 35 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 474 -> 473\n",
      "[Noun Extractor] postprocessing ignore_features : 473 -> 458\n",
      "[Noun Extractor] postprocessing ignore_NJ : 458 -> 458\n",
      "[Noun Extractor] 458 nouns (35 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 24.44 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 494 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=855, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 184 words\n",
      "[Noun Extractor] checked compounds. discovered 2 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53 -> 53\n",
      "[Noun Extractor] postprocessing ignore_features : 53 -> 48\n",
      "[Noun Extractor] postprocessing ignore_NJ : 48 -> 48\n",
      "[Noun Extractor] 48 nouns (2 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.21 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 965 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1620, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 281 words\n",
      "[Noun Extractor] checked compounds. discovered 17 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 95\n",
      "[Noun Extractor] postprocessing ignore_NJ : 95 -> 95\n",
      "[Noun Extractor] 95 nouns (17 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.75 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1147 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2039, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 354 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 112 -> 112\n",
      "[Noun Extractor] postprocessing ignore_features : 112 -> 109\n",
      "[Noun Extractor] postprocessing ignore_NJ : 109 -> 109\n",
      "[Noun Extractor] 109 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 48.11 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 3815 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=7830, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1295 words\n",
      "[Noun Extractor] checked compounds. discovered 133 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 488 -> 447\n",
      "[Noun Extractor] postprocessing ignore_features : 447 -> 429\n",
      "[Noun Extractor] postprocessing ignore_NJ : 429 -> 425\n",
      "[Noun Extractor] 425 nouns (133 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 52.57 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 594 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=909, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 201 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 55 -> 55\n",
      "[Noun Extractor] postprocessing ignore_features : 55 -> 49\n",
      "[Noun Extractor] postprocessing ignore_NJ : 49 -> 49\n",
      "[Noun Extractor] 49 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 39.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 2864 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=5158, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 1065 words\n",
      "[Noun Extractor] checked compounds. discovered 37 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 293 -> 285\n",
      "[Noun Extractor] postprocessing ignore_features : 285 -> 270\n",
      "[Noun Extractor] postprocessing ignore_NJ : 270 -> 270\n",
      "[Noun Extractor] 270 nouns (37 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.86 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1161 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1889, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 370 words\n",
      "[Noun Extractor] checked compounds. discovered 5 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 97 -> 97\n",
      "[Noun Extractor] postprocessing ignore_features : 97 -> 92\n",
      "[Noun Extractor] postprocessing ignore_NJ : 92 -> 92\n",
      "[Noun Extractor] 92 nouns (5 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.82 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 620 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=900, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 185 words\n",
      "[Noun Extractor] checked compounds. discovered 4 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 57 -> 57\n",
      "[Noun Extractor] postprocessing ignore_features : 57 -> 55\n",
      "[Noun Extractor] postprocessing ignore_NJ : 55 -> 55\n",
      "[Noun Extractor] 55 nouns (4 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 37.89 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 621 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=961, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 171 words\n",
      "[Noun Extractor] checked compounds. discovered 1 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 41 -> 41\n",
      "[Noun Extractor] postprocessing ignore_features : 41 -> 37\n",
      "[Noun Extractor] postprocessing ignore_NJ : 37 -> 37\n",
      "[Noun Extractor] 37 nouns (1 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 36.84 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 664 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1206, mem=0.923 Gb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] batch prediction was completed for 192 words\n",
      "[Noun Extractor] checked compounds. discovered 8 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 61 -> 61\n",
      "[Noun Extractor] postprocessing ignore_features : 61 -> 60\n",
      "[Noun Extractor] postprocessing ignore_NJ : 60 -> 60\n",
      "[Noun Extractor] 60 nouns (8 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 41.71 % eojeols are covered\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1133 from 1 sents. mem=0.923 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=1965, mem=0.923 Gb\n",
      "[Noun Extractor] batch prediction was completed for 357 words\n",
      "[Noun Extractor] checked compounds. discovered 15 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 98 -> 98\n",
      "[Noun Extractor] postprocessing ignore_features : 98 -> 96\n",
      "[Noun Extractor] postprocessing ignore_NJ : 96 -> 96\n",
      "[Noun Extractor] 96 nouns (15 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.923 Gb                    \n",
      "[Noun Extractor] 44.94 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "import Extractor\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Humor.db')\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql('SELECT head FROM head',conn)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "ext = Extractor.Ext(df)\n",
    "df = ext.cleaning()\n",
    "\n",
    "new_words = ext.search_dict(sorted(ext.extract_nouns().items(),key=lambda _:_[1], reverse=True))\n",
    "sent = ext.extract_sent(new_words)\n",
    "\n",
    "# 변수 생성\n",
    "statistic = ext.extract_statistic_value(sent)\n",
    "r_rat = ext.extract_r_rat(sent,statistic)\n",
    "statistic = ext.combine_var(statistic, r_rat)\n",
    "conn = sqlite3.connect('Humor_var.db')\n",
    "statistic.to_sql('var', conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
